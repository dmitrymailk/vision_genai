{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "https://pytorch.org/tutorials/recipes/recipes/profiler_recipe.html"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "6j0WbNTinv2D"
      },
      "outputs": [],
      "source": [
        "# For tips on running notebooks in Google Colab, see\n",
        "# https://pytorch.org/tutorials/beginner/colab\n",
        "%matplotlib inline"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lMIQzZpRnv2E"
      },
      "source": [
        "\n",
        "# PyTorch Profiler\n",
        "This recipe explains how to use PyTorch profiler and measure the time and\n",
        "memory consumption of the model's operators.\n",
        "\n",
        "## Introduction\n",
        "PyTorch includes a simple profiler API that is useful when user needs\n",
        "to determine the most expensive operators in the model.\n",
        "\n",
        "In this recipe, we will use a simple Resnet model to demonstrate how to\n",
        "use profiler to analyze model performance.\n",
        "\n",
        "## Setup\n",
        "To install ``torch`` and ``torchvision`` use the following command:\n",
        "\n",
        "```sh\n",
        "pip install torch torchvision\n",
        "```\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LC6R9htjnv2F"
      },
      "source": [
        "## Steps\n",
        "\n",
        "1. Import all necessary libraries\n",
        "2. Instantiate a simple Resnet model\n",
        "3. Using profiler to analyze execution time\n",
        "4. Using profiler to analyze memory consumption\n",
        "5. Using tracing functionality\n",
        "6. Examining stack traces\n",
        "7. Visualizing data as a flame graph\n",
        "8. Using profiler to analyze long-running jobs\n",
        "\n",
        "### 1. Import all necessary libraries\n",
        "\n",
        "In this recipe we will use ``torch``, ``torchvision.models``\n",
        "and ``profiler`` modules:\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "cjs1bXeEnv2F"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/opt/conda/lib/python3.10/site-packages/torchvision/io/image.py:13: UserWarning: Failed to load image Python extension: '/opt/conda/lib/python3.10/site-packages/torchvision/image.so: undefined symbol: _ZN3c1017RegisterOperatorsD1Ev'If you don't plan on using image functionality from `torchvision.io`, you can ignore this warning. Otherwise, there might be something wrong with your environment. Did you have `libjpeg` or `libpng` installed before building `torchvision` from source?\n",
            "  warn(\n"
          ]
        }
      ],
      "source": [
        "import torch\n",
        "import torchvision.models as models\n",
        "from torch.profiler import profile, record_function, ProfilerActivity"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SrKgrK9dnv2G"
      },
      "source": [
        "### 2. Instantiate a simple Resnet model\n",
        "\n",
        "Let's create an instance of a Resnet model and prepare an input\n",
        "for it:\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "DzyRINoJnv2G"
      },
      "outputs": [],
      "source": [
        "model = models.resnet18()\n",
        "inputs = torch.randn(5, 3, 224, 224)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HzRqm1l5nv2G"
      },
      "source": [
        "### 3. Using profiler to analyze execution time\n",
        "\n",
        "PyTorch profiler is enabled through the context manager and accepts\n",
        "a number of parameters, some of the most useful are:\n",
        "\n",
        "- ``activities`` - a list of activities to profile:\n",
        "   - ``ProfilerActivity.CPU`` - PyTorch operators, TorchScript functions and\n",
        "     user-defined code labels (see ``record_function`` below);\n",
        "   - ``ProfilerActivity.CUDA`` - on-device CUDA kernels;\n",
        "- ``record_shapes`` - whether to record shapes of the operator inputs;\n",
        "- ``profile_memory`` - whether to report amount of memory consumed by\n",
        "  model's Tensors;\n",
        "- ``use_cuda`` - whether to measure execution time of CUDA kernels.\n",
        "\n",
        "Note: when using CUDA, profiler also shows the runtime CUDA events\n",
        "occurring on the host.\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wukbQ_AAnv2H"
      },
      "source": [
        "Let's see how we can use profiler to analyze the execution time:\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "GAsM8tkYnv2H"
      },
      "outputs": [],
      "source": [
        "with profile(activities=[ProfilerActivity.CPU], record_shapes=True) as prof:\n",
        "    with record_function(\"model_inference\"):\n",
        "        model(inputs)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "12kuhODmnv2H"
      },
      "source": [
        "Note that we can use ``record_function`` context manager to label\n",
        "arbitrary code ranges with user provided names\n",
        "(``model_inference`` is used as a label in the example above).\n",
        "\n",
        "Profiler allows one to check which operators were called during the\n",
        "execution of a code range wrapped with a profiler context manager.\n",
        "If multiple profiler ranges are active at the same time (e.g. in\n",
        "parallel PyTorch threads), each profiling context manager tracks only\n",
        "the operators of its corresponding range.\n",
        "Profiler also automatically profiles the asynchronous tasks launched\n",
        "with ``torch.jit._fork`` and (in case of a backward pass)\n",
        "the backward pass operators launched with ``backward()`` call.\n",
        "\n",
        "Let's print out the stats for the execution above:\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "Ldv8H4sgnv2H"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "---------------------------------  ------------  ------------  ------------  ------------  ------------  ------------  \n",
            "                             Name    Self CPU %      Self CPU   CPU total %     CPU total  CPU time avg    # of Calls  \n",
            "---------------------------------  ------------  ------------  ------------  ------------  ------------  ------------  \n",
            "                  model_inference         4.60%       4.221ms       100.00%      91.835ms      91.835ms             1  \n",
            "                     aten::conv2d         0.40%     366.856us        73.66%      67.650ms       3.382ms            20  \n",
            "                aten::convolution         0.35%     318.362us        73.27%      67.283ms       3.364ms            20  \n",
            "               aten::_convolution         0.24%     219.702us        72.92%      66.964ms       3.348ms            20  \n",
            "         aten::mkldnn_convolution        72.34%      66.429ms        72.68%      66.745ms       3.337ms            20  \n",
            "                 aten::max_pool2d         0.03%      30.879us         8.40%       7.718ms       7.718ms             1  \n",
            "                 aten::batch_norm         0.08%      69.529us         8.38%       7.695ms     384.755us            20  \n",
            "    aten::max_pool2d_with_indices         8.37%       7.687ms         8.37%       7.687ms       7.687ms             1  \n",
            "     aten::_batch_norm_impl_index         0.30%     274.037us         8.30%       7.626ms     381.278us            20  \n",
            "          aten::native_batch_norm         7.76%       7.122ms         7.98%       7.330ms     366.515us            20  \n",
            "---------------------------------  ------------  ------------  ------------  ------------  ------------  ------------  \n",
            "Self CPU time total: 91.835ms\n",
            "\n"
          ]
        }
      ],
      "source": [
        "print(prof.key_averages().table(sort_by=\"cpu_time_total\", row_limit=10))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Of3a1Gpsnv2I"
      },
      "source": [
        "Here we see that, as expected, most of the time is spent in convolution (and specifically in ``mkldnn_convolution``\n",
        "for PyTorch compiled with ``MKL-DNN`` support).\n",
        "Note the difference between self cpu time and cpu time - operators can call other operators, self cpu time excludes time\n",
        "spent in children operator calls, while total cpu time includes it. You can choose to sort by the self cpu time by passing\n",
        "``sort_by=\"self_cpu_time_total\"`` into the ``table`` call.\n",
        "\n",
        "To get a finer granularity of results and include operator input shapes, pass ``group_by_input_shape=True``\n",
        "(note: this requires running the profiler with ``record_shapes=True``):\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "bbytXYPfnv2I"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "---------------------------------  ------------  ------------  ------------  ------------  ------------  ------------  --------------------------------------------------------------------------------  \n",
            "                             Name    Self CPU %      Self CPU   CPU total %     CPU total  CPU time avg    # of Calls                                                                      Input Shapes  \n",
            "---------------------------------  ------------  ------------  ------------  ------------  ------------  ------------  --------------------------------------------------------------------------------  \n",
            "                  model_inference         4.60%       4.221ms       100.00%      91.835ms      91.835ms             1                                                                                []  \n",
            "                     aten::conv2d         0.03%      25.682us        16.07%      14.756ms       3.689ms             4                             [[5, 64, 56, 56], [64, 64, 3, 3], [], [], [], [], []]  \n",
            "                aten::convolution         0.09%      79.216us        16.04%      14.730ms       3.683ms             4                     [[5, 64, 56, 56], [64, 64, 3, 3], [], [], [], [], [], [], []]  \n",
            "               aten::_convolution         0.06%      54.754us        15.95%      14.651ms       3.663ms             4     [[5, 64, 56, 56], [64, 64, 3, 3], [], [], [], [], [], [], [], [], [], [], []]  \n",
            "         aten::mkldnn_convolution        15.81%      14.520ms        15.89%      14.596ms       3.649ms             4                             [[5, 64, 56, 56], [64, 64, 3, 3], [], [], [], [], []]  \n",
            "                     aten::conv2d         0.29%     265.331us        14.96%      13.741ms      13.741ms             1                             [[5, 3, 224, 224], [64, 3, 7, 7], [], [], [], [], []]  \n",
            "                aten::convolution         0.04%      37.886us        14.67%      13.476ms      13.476ms             1                     [[5, 3, 224, 224], [64, 3, 7, 7], [], [], [], [], [], [], []]  \n",
            "               aten::_convolution         0.02%      17.118us        14.63%      13.438ms      13.438ms             1     [[5, 3, 224, 224], [64, 3, 7, 7], [], [], [], [], [], [], [], [], [], [], []]  \n",
            "         aten::mkldnn_convolution        14.57%      13.382ms        14.61%      13.421ms      13.421ms             1                             [[5, 3, 224, 224], [64, 3, 7, 7], [], [], [], [], []]  \n",
            "                     aten::conv2d         0.02%      15.253us        11.22%      10.308ms       3.436ms             3                            [[5, 512, 7, 7], [512, 512, 3, 3], [], [], [], [], []]  \n",
            "---------------------------------  ------------  ------------  ------------  ------------  ------------  ------------  --------------------------------------------------------------------------------  \n",
            "Self CPU time total: 91.835ms\n",
            "\n"
          ]
        }
      ],
      "source": [
        "print(prof.key_averages(group_by_input_shape=True).table(sort_by=\"cpu_time_total\", row_limit=10))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AFWYWTZtnv2I"
      },
      "source": [
        "Note the occurrence of ``aten::convolution`` twice with different input shapes.\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dzsU6e3nnv2I"
      },
      "source": [
        "Profiler can also be used to analyze performance of models executed on GPUs:\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "id": "x7GkZbGUnv2I"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "-------------------------------------------------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  \n",
            "                                                   Name    Self CPU %      Self CPU   CPU total %     CPU total  CPU time avg     Self CUDA   Self CUDA %    CUDA total  CUDA time avg    # of Calls  \n",
            "-------------------------------------------------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  \n",
            "                                        model_inference         0.00%       0.000us         0.00%       0.000us       0.000us      77.541ms        94.99%      77.541ms      38.771ms             2  \n",
            "                                        model_inference         1.01%       1.806ms       100.00%     178.994ms     178.994ms       0.000us         0.00%       4.093ms       4.093ms             1  \n",
            "                                           aten::conv2d         0.03%      55.404us        82.63%     147.904ms       7.395ms       0.000us         0.00%       3.059ms     152.935us            20  \n",
            "                                      aten::convolution         0.09%     166.011us        82.60%     147.849ms       7.392ms       0.000us         0.00%       3.059ms     152.935us            20  \n",
            "                                     aten::_convolution         0.27%     484.576us        82.50%     147.683ms       7.384ms       0.000us         0.00%       3.059ms     152.935us            20  \n",
            "                                aten::cudnn_convolution        51.13%      91.522ms        82.23%     147.198ms       7.360ms       3.059ms         3.75%       3.059ms     152.935us            20  \n",
            "void cudnn::engines_precompiled::nchwToNhwcKernel<fl...         0.00%       0.000us         0.00%       0.000us       0.000us     558.111us         0.68%     558.111us      16.415us            34  \n",
            "                                       aten::batch_norm         0.02%      33.021us         5.36%       9.587ms     479.374us       0.000us         0.00%     525.824us      26.291us            20  \n",
            "                           aten::_batch_norm_impl_index         0.04%      73.458us         5.34%       9.554ms     477.723us       0.000us         0.00%     525.824us      26.291us            20  \n",
            "                                 aten::cudnn_batch_norm         2.32%       4.145ms         5.30%       9.481ms     474.050us     525.824us         0.64%     525.824us      26.291us            20  \n",
            "-------------------------------------------------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  \n",
            "Self CPU time total: 178.999ms\n",
            "Self CUDA time total: 81.634ms\n",
            "\n"
          ]
        }
      ],
      "source": [
        "model = models.resnet18().cuda()\n",
        "inputs = torch.randn(5, 3, 224, 224).cuda()\n",
        "\n",
        "with profile(activities=[\n",
        "        ProfilerActivity.CPU, ProfilerActivity.CUDA], record_shapes=True) as prof:\n",
        "    with record_function(\"model_inference\"):\n",
        "        model(inputs)\n",
        "\n",
        "print(prof.key_averages().table(sort_by=\"cuda_time_total\", row_limit=10))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "F0P6wQbHnv2I"
      },
      "source": [
        "(Note: the first use of CUDA profiling may bring an extra overhead.)\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Zn2eQFuvnv2J"
      },
      "source": [
        "Note the occurrence of on-device kernels in the output (e.g. ``sgemm_32x32x32_NN``).\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "am1Uovgjnv2J"
      },
      "source": [
        "### 4. Using profiler to analyze memory consumption\n",
        "\n",
        "PyTorch profiler can also show the amount of memory (used by the model's tensors)\n",
        "that was allocated (or released) during the execution of the model's operators.\n",
        "In the output below, 'self' memory corresponds to the memory allocated (released)\n",
        "by the operator, excluding the children calls to the other operators.\n",
        "To enable memory profiling functionality pass ``profile_memory=True``.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "id": "nY6D1jVpnv2J"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "---------------------------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  \n",
            "                             Name    Self CPU %      Self CPU   CPU total %     CPU total  CPU time avg       CPU Mem  Self CPU Mem    # of Calls  \n",
            "---------------------------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  \n",
            "                      aten::empty         0.47%     330.637us         0.47%     330.637us       1.653us      94.86 Mb      94.86 Mb           200  \n",
            "    aten::max_pool2d_with_indices         9.30%       6.598ms         9.30%       6.598ms       6.598ms      11.48 Mb      11.48 Mb             1  \n",
            "                      aten::addmm         0.17%     118.586us         0.18%     127.930us     127.930us      19.53 Kb      19.53 Kb             1  \n",
            "                       aten::mean         0.02%      15.597us         0.11%      74.759us      74.759us      10.00 Kb       9.99 Kb             1  \n",
            "                       aten::div_         0.02%      14.066us         0.05%      34.149us      34.149us           8 b           4 b             1  \n",
            "              aten::empty_strided         0.01%       3.863us         0.01%       3.863us       3.863us           4 b           4 b             1  \n",
            "                     aten::conv2d         0.15%     107.619us        77.52%      54.983ms       2.749ms      47.37 Mb           0 b            20  \n",
            "                aten::convolution         0.40%     286.505us        77.37%      54.875ms       2.744ms      47.37 Mb           0 b            20  \n",
            "               aten::_convolution         0.27%     190.048us        76.97%      54.588ms       2.729ms      47.37 Mb           0 b            20  \n",
            "         aten::mkldnn_convolution        76.29%      54.110ms        76.70%      54.398ms       2.720ms      47.37 Mb           0 b            20  \n",
            "---------------------------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  \n",
            "Self CPU time total: 70.923ms\n",
            "\n",
            "---------------------------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  \n",
            "                             Name    Self CPU %      Self CPU   CPU total %     CPU total  CPU time avg       CPU Mem  Self CPU Mem    # of Calls  \n",
            "---------------------------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  \n",
            "                      aten::empty         0.47%     330.637us         0.47%     330.637us       1.653us      94.86 Mb      94.86 Mb           200  \n",
            "                 aten::batch_norm         0.10%      68.390us         9.85%       6.986ms     349.313us      47.41 Mb           0 b            20  \n",
            "     aten::_batch_norm_impl_index         0.17%     117.576us         9.75%       6.918ms     345.894us      47.41 Mb           0 b            20  \n",
            "          aten::native_batch_norm         9.22%       6.542ms         9.56%       6.780ms     339.001us      47.41 Mb     -75.00 Kb            20  \n",
            "                     aten::conv2d         0.15%     107.619us        77.52%      54.983ms       2.749ms      47.37 Mb           0 b            20  \n",
            "                aten::convolution         0.40%     286.505us        77.37%      54.875ms       2.744ms      47.37 Mb           0 b            20  \n",
            "               aten::_convolution         0.27%     190.048us        76.97%      54.588ms       2.729ms      47.37 Mb           0 b            20  \n",
            "         aten::mkldnn_convolution        76.29%      54.110ms        76.70%      54.398ms       2.720ms      47.37 Mb           0 b            20  \n",
            "                 aten::empty_like         0.11%      74.954us         0.15%     104.738us       5.237us      47.37 Mb           0 b            20  \n",
            "                 aten::max_pool2d         0.01%       9.675us         9.32%       6.608ms       6.608ms      11.48 Mb           0 b             1  \n",
            "---------------------------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  \n",
            "Self CPU time total: 70.923ms\n",
            "\n"
          ]
        }
      ],
      "source": [
        "model = models.resnet18()\n",
        "inputs = torch.randn(5, 3, 224, 224)\n",
        "\n",
        "with profile(activities=[ProfilerActivity.CPU],\n",
        "        profile_memory=True, record_shapes=True) as prof:\n",
        "    model(inputs)\n",
        "\n",
        "print(prof.key_averages().table(sort_by=\"self_cpu_memory_usage\", row_limit=10))\n",
        "\n",
        "print(prof.key_averages().table(sort_by=\"cpu_memory_usage\", row_limit=10))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8mF4yO8znv2J"
      },
      "source": [
        "### 5. Using tracing functionality\n",
        "\n",
        "Profiling results can be outputted as a ``.json`` trace file:\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "id": "uaNHEFAGnv2J"
      },
      "outputs": [],
      "source": [
        "model = models.resnet18().cuda()\n",
        "inputs = torch.randn(5, 3, 224, 224).cuda()\n",
        "\n",
        "with profile(activities=[ProfilerActivity.CPU, ProfilerActivity.CUDA]) as prof:\n",
        "    model(inputs)\n",
        "\n",
        "prof.export_chrome_trace(\"spellchecker/kosenko/sandbox/profiling/2_pytorch_profiler_recipe/trace.json\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "M48t7HQNnv2J"
      },
      "source": [
        "You can examine the sequence of profiled operators and CUDA kernels\n",
        "in Chrome trace viewer (``chrome://tracing``):\n",
        "\n",
        "<img src=\"file://../../_static/img/trace_img.png\" scale=\"25 %\">\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {},
      "outputs": [],
      "source": [
        "# chrome://tracing"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LHWqKj51nv2K"
      },
      "source": [
        "### 6. Examining stack traces\n",
        "\n",
        "Profiler can be used to analyze Python and TorchScript stack traces:\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "id": "S8MoCorCnv2K"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "-------------------------------------------------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  \n",
            "                                                   Name    Self CPU %      Self CPU   CPU total %     CPU total  CPU time avg     Self CUDA   Self CUDA %    CUDA total  CUDA time avg    # of Calls  \n",
            "-------------------------------------------------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  \n",
            "                                aten::cudnn_convolution        23.71%     924.088us        35.44%       1.381ms      69.063us       3.055ms        74.69%       3.055ms     152.770us            20  \n",
            "void cudnn::engines_precompiled::nchwToNhwcKernel<fl...         0.00%       0.000us         0.00%       0.000us       0.000us     558.300us        13.65%     558.300us      16.421us            34  \n",
            "-------------------------------------------------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  \n",
            "Self CPU time total: 3.897ms\n",
            "Self CUDA time total: 4.091ms\n",
            "\n"
          ]
        }
      ],
      "source": [
        "with profile(\n",
        "    activities=[ProfilerActivity.CPU, ProfilerActivity.CUDA],\n",
        "    with_stack=True,\n",
        ") as prof:\n",
        "    model(inputs)\n",
        "\n",
        "# Print aggregated stats\n",
        "print(prof.key_averages(group_by_stack_n=5).table(sort_by=\"self_cuda_time_total\", row_limit=2))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5_j9WE9knv2K"
      },
      "source": [
        "Note the two convolutions and the two call sites in ``torchvision/models/resnet.py`` script.\n",
        "\n",
        "(Warning: stack tracing adds an extra profiling overhead.)\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "09BEaW6Inv2K"
      },
      "source": [
        "### 7. Visualizing data as a flame graph\n",
        "\n",
        "Execution time (``self_cpu_time_total`` and ``self_cuda_time_total`` metrics) and stack traces\n",
        "can also be visualized as a flame graph. To do this, first export the raw data using ``export_stacks`` (requires ``with_stack=True``):\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "id": "AROYur-Lnv2K"
      },
      "outputs": [],
      "source": [
        "prof.export_stacks(\"/tmp/profiler_stacks.txt\", \"self_cuda_time_total\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NexZAz17nv2L"
      },
      "source": [
        "We recommend using [Flamegraph tool](https://github.com/brendangregg/FlameGraph) to generate an\n",
        "interactive ``.svg`` file:\n",
        "\n",
        "```sh\n",
        "git clone https://github.com/brendangregg/FlameGraph\n",
        "cd FlameGraph\n",
        "./flamegraph.pl --title \"CUDA time\" --countname \"us.\" /tmp/profiler_stacks.txt > perf_viz.svg\n",
        "```\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "j7q7a3p8nv2L"
      },
      "source": [
        "<img src=\"file://../../_static/img/perf_viz.png\" scale=\"25 %\">\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fodPQquqnv2L"
      },
      "source": [
        "### 8. Using profiler to analyze long-running jobs\n",
        "\n",
        "PyTorch profiler offers an additional API to handle long-running jobs\n",
        "(such as training loops). Tracing all of the execution can be\n",
        "slow and result in very large trace files. To avoid this, use optional\n",
        "arguments:\n",
        "\n",
        "- ``schedule`` - specifies a function that takes an integer argument (step number)\n",
        "  as an input and returns an action for the profiler, the best way to use this parameter\n",
        "  is to use ``torch.profiler.schedule`` helper function that can generate a schedule for you;\n",
        "- ``on_trace_ready`` - specifies a function that takes a reference to the profiler as\n",
        "  an input and is called by the profiler each time the new trace is ready.\n",
        "\n",
        "To illustrate how the API works, let's first consider the following example with\n",
        "``torch.profiler.schedule`` helper function:\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "id": "6ulKcs_4nv2L"
      },
      "outputs": [],
      "source": [
        "from torch.profiler import schedule\n",
        "\n",
        "my_schedule = schedule(\n",
        "    skip_first=10,\n",
        "    wait=5,\n",
        "    warmup=1,\n",
        "    active=3,\n",
        "    repeat=2)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "w4Y2flJxnv2L"
      },
      "source": [
        "Profiler assumes that the long-running job is composed of steps, numbered\n",
        "starting from zero. The example above defines the following sequence of actions\n",
        "for the profiler:\n",
        "\n",
        "1. Parameter ``skip_first`` tells profiler that it should ignore the first 10 steps\n",
        "   (default value of ``skip_first`` is zero);\n",
        "2. After the first ``skip_first`` steps, profiler starts executing profiler cycles;\n",
        "3. Each cycle consists of three phases:\n",
        "\n",
        "   - idling (``wait=5`` steps), during this phase profiler is not active;\n",
        "   - warming up (``warmup=1`` steps), during this phase profiler starts tracing, but\n",
        "     the results are discarded; this phase is used to discard the samples obtained by\n",
        "     the profiler at the beginning of the trace since they are usually skewed by an extra\n",
        "     overhead;\n",
        "   - active tracing (``active=3`` steps), during this phase profiler traces and records data;\n",
        "4. An optional ``repeat`` parameter specifies an upper bound on the number of cycles.\n",
        "   By default (zero value), profiler will execute cycles as long as the job runs.\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QVKZh-qenv2O"
      },
      "source": [
        "Thus, in the example above, profiler will skip the first 15 steps, spend the next step on the warm up,\n",
        "actively record the next 3 steps, skip another 5 steps, spend the next step on the warm up, actively\n",
        "record another 3 steps. Since the ``repeat=2`` parameter value is specified, the profiler will stop\n",
        "the recording after the first two cycles.\n",
        "\n",
        "At the end of each cycle profiler calls the specified ``on_trace_ready`` function and passes itself as\n",
        "an argument. This function is used to process the new trace - either by obtaining the table output or\n",
        "by saving the output on disk as a trace file.\n",
        "\n",
        "To send the signal to the profiler that the next step has started, call ``prof.step()`` function.\n",
        "The current profiler step is stored in ``prof.step_num``.\n",
        "\n",
        "The following example shows how to use all of the concepts above:\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {
        "id": "_hoaKfK7nv2O"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "-------------------------------------------------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  \n",
            "                                                   Name    Self CPU %      Self CPU   CPU total %     CPU total  CPU time avg     Self CUDA   Self CUDA %    CUDA total  CUDA time avg    # of Calls  \n",
            "-------------------------------------------------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  \n",
            "                                          ProfilerStep*         0.00%       0.000us         0.00%       0.000us       0.000us       8.384ms        47.18%       8.384ms       4.192ms             2  \n",
            "                                aten::cudnn_convolution        10.13%     965.568us        16.67%       1.588ms      39.709us       6.114ms        34.41%       6.114ms     152.861us            40  \n",
            "sm86_xmma_fprop_implicit_gemm_indexed_tf32f32_tf32f3...         0.00%       0.000us         0.00%       0.000us       0.000us       1.568ms         8.83%       1.568ms     174.243us             9  \n",
            "void cudnn::engines_precompiled::nchwToNhwcKernel<fl...         0.00%       0.000us         0.00%       0.000us       0.000us       1.393ms         7.84%       1.393ms      17.858us            78  \n",
            "void cutlass__5x_cudnn::Kernel<cutlass_tensorop_s168...         0.00%       0.000us         0.00%       0.000us       0.000us       1.058ms         5.96%       1.058ms     117.599us             9  \n",
            "                                 aten::cudnn_batch_norm         8.60%     820.050us        20.42%       1.946ms      48.645us       1.048ms         5.90%       1.048ms      26.193us            40  \n",
            "sm80_xmma_fprop_implicit_gemm_tf32f32_tf32f32_f32_nh...         0.00%       0.000us         0.00%       0.000us       0.000us       1.047ms         5.89%       1.047ms     174.484us             6  \n",
            "sm86_xmma_fprop_implicit_gemm_tf32f32_tf32f32_f32_nh...         0.00%       0.000us         0.00%       0.000us       0.000us       1.030ms         5.79%       1.030ms     128.703us             8  \n",
            "void cudnn::bn_fw_tr_1C11_singleread<float, 512, tru...         0.00%       0.000us         0.00%       0.000us       0.000us     546.204us         3.07%     546.204us      15.172us            36  \n",
            "void at::native::vectorized_elementwise_kernel<4, at...         0.00%       0.000us         0.00%       0.000us       0.000us     504.161us         2.84%     504.161us      12.927us            39  \n",
            "-------------------------------------------------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  \n",
            "Self CPU time total: 9.530ms\n",
            "Self CUDA time total: 17.769ms\n",
            "\n",
            "-------------------------------------------------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  \n",
            "                                                   Name    Self CPU %      Self CPU   CPU total %     CPU total  CPU time avg     Self CUDA   Self CUDA %    CUDA total  CUDA time avg    # of Calls  \n",
            "-------------------------------------------------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  \n",
            "                                          ProfilerStep*         0.00%       0.000us         0.00%       0.000us       0.000us      16.773ms        46.67%      16.773ms       4.193ms             4  \n",
            "                                aten::cudnn_convolution         9.83%       1.915ms        16.15%       3.146ms      39.321us      12.227ms        34.02%      12.227ms     152.836us            80  \n",
            "sm86_xmma_fprop_implicit_gemm_indexed_tf32f32_tf32f3...         0.00%       0.000us         0.00%       0.000us       0.000us       3.135ms         8.72%       3.135ms     174.177us            18  \n",
            "void cudnn::engines_precompiled::nchwToNhwcKernel<fl...         0.00%       0.000us         0.00%       0.000us       0.000us       2.828ms         7.87%       2.828ms      17.677us           160  \n",
            "void cutlass__5x_cudnn::Kernel<cutlass_tensorop_s168...         0.00%       0.000us         0.00%       0.000us       0.000us       2.380ms         6.62%       2.380ms     119.018us            20  \n",
            "                                 aten::cudnn_batch_norm         8.41%       1.638ms        19.92%       3.879ms      48.486us       2.098ms         5.84%       2.098ms      26.229us            80  \n",
            "sm80_xmma_fprop_implicit_gemm_tf32f32_tf32f32_f32_nh...         0.00%       0.000us         0.00%       0.000us       0.000us       2.093ms         5.82%       2.093ms     174.442us            12  \n",
            "sm86_xmma_fprop_implicit_gemm_tf32f32_tf32f32_f32_nh...         0.00%       0.000us         0.00%       0.000us       0.000us       2.058ms         5.73%       2.058ms     128.637us            16  \n",
            "void cudnn::bn_fw_tr_1C11_singleread<float, 512, tru...         0.00%       0.000us         0.00%       0.000us       0.000us       1.130ms         3.14%       1.130ms      15.068us            75  \n",
            "void at::native::vectorized_elementwise_kernel<4, at...         0.00%       0.000us         0.00%       0.000us       0.000us       1.012ms         2.82%       1.012ms      12.648us            80  \n",
            "-------------------------------------------------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  \n",
            "Self CPU time total: 19.474ms\n",
            "Self CUDA time total: 35.937ms\n",
            "\n"
          ]
        }
      ],
      "source": [
        "def trace_handler(p):\n",
        "    output = p.key_averages().table(sort_by=\"self_cuda_time_total\", row_limit=10)\n",
        "    print(output)\n",
        "    temp = 'spellchecker/kosenko/sandbox/profiling/2_pytorch_profiler_recipe'\n",
        "    p.export_chrome_trace(\n",
        "        f\"{temp}/trace_\" + str(p.step_num) + \".json\"\n",
        "    )\n",
        "\n",
        "with profile(\n",
        "    activities=[ProfilerActivity.CPU, ProfilerActivity.CUDA],\n",
        "    schedule=torch.profiler.schedule(\n",
        "        wait=1,\n",
        "        warmup=1,\n",
        "        active=2),\n",
        "    on_trace_ready=trace_handler\n",
        ") as p:\n",
        "    for idx in range(8):\n",
        "        model(inputs)\n",
        "        p.step()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6aI1wsr9nv2O"
      },
      "source": [
        "## Learn More\n",
        "\n",
        "Take a look at the following recipes/tutorials to continue your learning:\n",
        "\n",
        "-  [PyTorch Benchmark](https://pytorch.org/tutorials/recipes/recipes/benchmark.html)\n",
        "-  [PyTorch Profiler with TensorBoard](https://pytorch.org/tutorials/intermediate/tensorboard_profiler_tutorial.html) tutorial\n",
        "-  [Visualizing models, data, and training with TensorBoard](https://pytorch.org/tutorials/intermediate/tensorboard_tutorial.html) tutorial\n",
        "\n",
        "\n"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.13"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
