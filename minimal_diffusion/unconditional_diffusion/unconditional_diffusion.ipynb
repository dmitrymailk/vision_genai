{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "d9ee4c81",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.11/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "***** Running training *****\n",
      "  Num examples = 8189\n",
      "  Num Epochs = 1\n",
      "  Instantaneous batch size per device = 16\n",
      "  Total train batch size (w. parallel, distributed & accumulation) = 16\n",
      "  Gradient Accumulation steps = 1\n",
      "  Total optimization steps = 512\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 0:  98%|█████████▊| 502/512 [00:49<00:02,  4.25it/s, ema_decay=0.991, loss=0.0339, lr=9.33e-5, step=502]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved state to minimal_diffusion/unconditional_diffusion/ddpm-model-64/checkpoint-500\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 0: 100%|██████████| 512/512 [00:50<00:00, 10.16it/s, ema_decay=0.991, loss=0.0461, lr=0, step=512]      \n",
      "100%|██████████| 1000/1000 [00:17<00:00, 55.94it/s]\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "\n",
    "os.environ[\"WANDB_PROJECT\"] = \"unconditional_image_generation\"\n",
    "\n",
    "import argparse\n",
    "import inspect\n",
    "import logging\n",
    "import math\n",
    "import shutil\n",
    "from datetime import timedelta\n",
    "from pathlib import Path\n",
    "\n",
    "import accelerate\n",
    "import datasets\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "from accelerate import Accelerator, InitProcessGroupKwargs\n",
    "from accelerate.logging import get_logger\n",
    "from accelerate.utils import ProjectConfiguration\n",
    "from datasets import load_dataset\n",
    "from huggingface_hub import create_repo, upload_folder\n",
    "from packaging import version\n",
    "from torchvision import transforms\n",
    "from tqdm.auto import tqdm\n",
    "\n",
    "import diffusers\n",
    "from diffusers import DDPMPipeline, DDPMScheduler, UNet2DModel\n",
    "from diffusers.optimization import get_scheduler\n",
    "from diffusers.training_utils import EMAModel\n",
    "from diffusers.utils import (\n",
    "    check_min_version,\n",
    "    is_accelerate_version,\n",
    "    is_tensorboard_available,\n",
    "    is_wandb_available,\n",
    ")\n",
    "from diffusers.utils.import_utils import is_xformers_available\n",
    "\n",
    "from dataclasses import dataclass\n",
    "\n",
    "\n",
    "def _extract_into_tensor(arr, timesteps, broadcast_shape):\n",
    "    \"\"\"\n",
    "    Extract values from a 1-D numpy array for a batch of indices.\n",
    "\n",
    "    :param arr: the 1-D numpy array.\n",
    "    :param timesteps: a tensor of indices into the array to extract.\n",
    "    :param broadcast_shape: a larger shape of K dimensions with the batch\n",
    "                            dimension equal to the length of timesteps.\n",
    "    :return: a tensor of shape [batch_size, 1, ...] where the shape has K dims.\n",
    "    \"\"\"\n",
    "    if not isinstance(arr, torch.Tensor):\n",
    "        arr = torch.from_numpy(arr)\n",
    "    res = arr[timesteps].float().to(timesteps.device)\n",
    "    while len(res.shape) < len(broadcast_shape):\n",
    "        res = res[..., None]\n",
    "    return res.expand(broadcast_shape)\n",
    "\n",
    "\n",
    "@dataclass\n",
    "class Args:\n",
    "    resolution: int = 64\n",
    "    ddpm_num_steps: int = 1000\n",
    "    ddpm_beta_schedule: str = \"linear\"\n",
    "    learning_rate: float = 1e-4\n",
    "    adam_beta1: float = 0.95\n",
    "    adam_beta2: float = 0.999\n",
    "    adam_weight_decay: float = 1e-6\n",
    "    adam_epsilon: float = 1e-08\n",
    "    dataset_name: str = \"huggan/flowers-102-categories\"\n",
    "    dataset_config_name: str = None\n",
    "    cache_dir: str = None\n",
    "    ema_max_decay: float = 0.9999\n",
    "    ema_inv_gamma: float = 1.0\n",
    "    ema_power: float = 3 / 4\n",
    "    train_batch_size: int = 16\n",
    "    eval_batch_size: int = 16\n",
    "    gradient_accumulation_steps: int = 1\n",
    "    lr_scheduler: str = \"cosine\"\n",
    "    lr_warmup_steps: int = 500\n",
    "    num_epochs: int = 1\n",
    "    center_crop: bool = False\n",
    "    random_flip: bool = False\n",
    "    dataloader_num_workers: int = 0\n",
    "    use_ema: bool = True\n",
    "    checkpointing_steps: int = 500\n",
    "    checkpoints_total_limit: int = None\n",
    "    output_dir: str = \"minimal_diffusion/unconditional_diffusion/ddpm-model-64\"\n",
    "    save_images_epochs: int = 10\n",
    "    save_model_epochs: int = 1\n",
    "    prediction_type: str = \"epsilon\"\n",
    "    logging_dir: str = \"logs\"\n",
    "    mixed_precision: str = \"no\"\n",
    "    logger: str = None\n",
    "    ddpm_num_inference_steps: int = 1000\n",
    "\n",
    "\n",
    "args = Args()\n",
    "\n",
    "model = UNet2DModel(\n",
    "    sample_size=args.resolution,\n",
    "    in_channels=3,\n",
    "    out_channels=3,\n",
    "    layers_per_block=2,\n",
    "    block_out_channels=(128, 128, 256, 256, 512, 512),\n",
    "    down_block_types=(\n",
    "        \"DownBlock2D\",\n",
    "        \"DownBlock2D\",\n",
    "        \"DownBlock2D\",\n",
    "        \"DownBlock2D\",\n",
    "        \"AttnDownBlock2D\",\n",
    "        \"DownBlock2D\",\n",
    "    ),\n",
    "    up_block_types=(\n",
    "        \"UpBlock2D\",\n",
    "        \"AttnUpBlock2D\",\n",
    "        \"UpBlock2D\",\n",
    "        \"UpBlock2D\",\n",
    "        \"UpBlock2D\",\n",
    "        \"UpBlock2D\",\n",
    "    ),\n",
    ")\n",
    "model = model.cuda()\n",
    "ema_model = EMAModel(\n",
    "    model.parameters(),\n",
    "    decay=args.ema_max_decay,\n",
    "    use_ema_warmup=True,\n",
    "    inv_gamma=args.ema_inv_gamma,\n",
    "    power=args.ema_power,\n",
    "    model_cls=UNet2DModel,\n",
    "    model_config=model.config,\n",
    ")\n",
    "\n",
    "noise_scheduler = DDPMScheduler(\n",
    "    num_train_timesteps=args.ddpm_num_steps,\n",
    "    beta_schedule=args.ddpm_beta_schedule,\n",
    ")\n",
    "\n",
    "optimizer = torch.optim.AdamW(\n",
    "    model.parameters(),\n",
    "    lr=args.learning_rate,\n",
    "    betas=(args.adam_beta1, args.adam_beta2),\n",
    "    weight_decay=args.adam_weight_decay,\n",
    "    eps=args.adam_epsilon,\n",
    ")\n",
    "\n",
    "dataset = load_dataset(\n",
    "    args.dataset_name,\n",
    "    args.dataset_config_name,\n",
    "    cache_dir=args.cache_dir,\n",
    "    split=\"train\",\n",
    ")\n",
    "\n",
    "augmentations = transforms.Compose(\n",
    "    [\n",
    "        transforms.Resize(\n",
    "            args.resolution, interpolation=transforms.InterpolationMode.BILINEAR\n",
    "        ),\n",
    "        (\n",
    "            transforms.CenterCrop(args.resolution)\n",
    "            if args.center_crop\n",
    "            else transforms.RandomCrop(args.resolution)\n",
    "        ),\n",
    "        (\n",
    "            transforms.RandomHorizontalFlip()\n",
    "            if args.random_flip\n",
    "            else transforms.Lambda(lambda x: x)\n",
    "        ),\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize(\n",
    "            [0.5],\n",
    "            [0.5],\n",
    "        ),\n",
    "    ]\n",
    ")\n",
    "\n",
    "\n",
    "def transform_images(examples):\n",
    "    images = [augmentations(image.convert(\"RGB\")) for image in examples[\"image\"]]\n",
    "    return {\"input\": images}\n",
    "\n",
    "\n",
    "dataset.set_transform(transform_images)\n",
    "train_dataloader = torch.utils.data.DataLoader(\n",
    "    dataset,\n",
    "    batch_size=args.train_batch_size,\n",
    "    shuffle=True,\n",
    "    num_workers=args.dataloader_num_workers,\n",
    ")\n",
    "\n",
    "# Initialize the learning rate scheduler\n",
    "lr_scheduler = get_scheduler(\n",
    "    args.lr_scheduler,\n",
    "    optimizer=optimizer,\n",
    "    num_warmup_steps=args.lr_warmup_steps * args.gradient_accumulation_steps,\n",
    "    num_training_steps=(len(train_dataloader) * args.num_epochs),\n",
    ")\n",
    "\n",
    "# # Prepare everything with our `accelerator`.\n",
    "# model, optimizer, train_dataloader, lr_scheduler = accelerator.prepare(\n",
    "#     model, optimizer, train_dataloader, lr_scheduler\n",
    "# )\n",
    "\n",
    "\n",
    "total_batch_size = args.train_batch_size * args.gradient_accumulation_steps\n",
    "num_update_steps_per_epoch = math.ceil(\n",
    "    len(train_dataloader) / args.gradient_accumulation_steps\n",
    ")\n",
    "max_train_steps = args.num_epochs * num_update_steps_per_epoch\n",
    "\n",
    "\n",
    "print(\"***** Running training *****\")\n",
    "print(f\"  Num examples = {len(dataset)}\")\n",
    "print(f\"  Num Epochs = {args.num_epochs}\")\n",
    "print(f\"  Instantaneous batch size per device = {args.train_batch_size}\")\n",
    "print(\n",
    "    f\"  Total train batch size (w. parallel, distributed & accumulation) = {total_batch_size}\"\n",
    ")\n",
    "print(f\"  Gradient Accumulation steps = {args.gradient_accumulation_steps}\")\n",
    "print(f\"  Total optimization steps = {max_train_steps}\")\n",
    "\n",
    "global_step = 0\n",
    "first_epoch = 0\n",
    "\n",
    "weight_dtype = torch.float32\n",
    "# model.enable_xformers_memory_efficient_attention()\n",
    "logging_dir = os.path.join(args.output_dir, args.logging_dir)\n",
    "# accelerator_project_config = ProjectConfiguration(\n",
    "#     project_dir=args.output_dir, logging_dir=logging_dir\n",
    "# )\n",
    "# accelerator = Accelerator(\n",
    "#     gradient_accumulation_steps=args.gradient_accumulation_steps,\n",
    "#     mixed_precision=args.mixed_precision,\n",
    "#     log_with=args.logger,\n",
    "#     project_config=accelerator_project_config,\n",
    "#     # kwargs_handlers=[kwargs],\n",
    "# )\n",
    "# model, optimizer, train_dataloader, lr_scheduler = accelerator.prepare(\n",
    "#     model, optimizer, train_dataloader, lr_scheduler\n",
    "# )\n",
    "\n",
    "# ema_model = ema_model.cuda()\n",
    "# Train!\n",
    "device = \"cuda\"\n",
    "for epoch in range(first_epoch, args.num_epochs):\n",
    "    model.train()\n",
    "    progress_bar = tqdm(total=num_update_steps_per_epoch)\n",
    "    progress_bar.set_description(f\"Epoch {epoch}\")\n",
    "    for step, batch in enumerate(train_dataloader):\n",
    "        # Skip steps until we reach the resumed step\n",
    "\n",
    "        clean_images = batch[\"input\"].to(weight_dtype).to(device)\n",
    "        # Sample noise that we'll add to the images\n",
    "        noise = torch.randn(\n",
    "            clean_images.shape,\n",
    "            dtype=weight_dtype,\n",
    "            device=clean_images.device,\n",
    "        )\n",
    "        bsz = clean_images.shape[0]\n",
    "        # Sample a random timestep for each image\n",
    "        timesteps = torch.randint(\n",
    "            0,\n",
    "            noise_scheduler.config.num_train_timesteps,\n",
    "            (bsz,),\n",
    "            device=clean_images.device,\n",
    "        ).long()\n",
    "\n",
    "        # Add noise to the clean images according to the noise magnitude at each timestep\n",
    "        # (this is the forward diffusion process)\n",
    "        noisy_images = noise_scheduler.add_noise(clean_images, noise, timesteps)\n",
    "\n",
    "        # with accelerator.accumulate(model):\n",
    "        # Predict the noise residual\n",
    "        model_output = model(noisy_images, timesteps).sample\n",
    "\n",
    "        if args.prediction_type == \"epsilon\":\n",
    "            loss = F.mse_loss(\n",
    "                model_output.float(),\n",
    "                noise.float(),\n",
    "            )  # this could have different weights!\n",
    "        elif args.prediction_type == \"sample\":\n",
    "            alpha_t = _extract_into_tensor(\n",
    "                noise_scheduler.alphas_cumprod,\n",
    "                timesteps,\n",
    "                (clean_images.shape[0], 1, 1, 1),\n",
    "            )\n",
    "            snr_weights = alpha_t / (1 - alpha_t)\n",
    "            # use SNR weighting from distillation paper\n",
    "            loss = snr_weights * F.mse_loss(\n",
    "                model_output.float(), clean_images.float(), reduction=\"none\"\n",
    "            )\n",
    "            loss = loss.mean()\n",
    "            # else:\n",
    "            #     raise ValueError(f\"Unsupported prediction type: {args.prediction_type}\")\n",
    "\n",
    "            # optimizer.backward(loss)\n",
    "        loss.backward()\n",
    "\n",
    "        torch.nn.utils.clip_grad_norm_(model.parameters(), 1.0)\n",
    "        optimizer.step()\n",
    "        lr_scheduler.step()\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        # Checks if the accelerator has performed an optimization step behind the scenes\n",
    "        # if accelerator.sync_gradients:\n",
    "        if args.use_ema:\n",
    "            ema_model.step(model.parameters())\n",
    "        progress_bar.update(1)\n",
    "        global_step += 1\n",
    "\n",
    "        # if accelerator.is_main_process:\n",
    "        if global_step % args.checkpointing_steps == 0:\n",
    "            # _before_ saving state, check if this save would set us over the `checkpoints_total_limit`\n",
    "\n",
    "            save_path = os.path.join(args.output_dir, f\"checkpoint-{global_step}\")\n",
    "            # accelerator.save_state(save_path)\n",
    "            checkpoint = {\n",
    "                \"epoch\": epoch,\n",
    "                \"model\": model,\n",
    "                \"optimizer\": optimizer,\n",
    "                \"lr_sched\": lr_scheduler.state_dict(),\n",
    "                \"ema_model\": ema_model,\n",
    "            }\n",
    "            torch.save(checkpoint, f\"{save_path}.pth\")\n",
    "            print(f\"Saved state to {save_path}\")\n",
    "\n",
    "        logs = {\n",
    "            \"loss\": loss.detach().item(),\n",
    "            \"lr\": lr_scheduler.get_last_lr()[0],\n",
    "            \"step\": global_step,\n",
    "        }\n",
    "        if args.use_ema:\n",
    "            logs[\"ema_decay\"] = ema_model.cur_decay_value\n",
    "        progress_bar.set_postfix(**logs)\n",
    "        # accelerator.log(logs, step=global_step)\n",
    "    progress_bar.close()\n",
    "\n",
    "    # accelerator.wait_for_everyone()\n",
    "\n",
    "    # Generate sample images for visual inspection\n",
    "    if epoch % args.save_images_epochs == 0 or epoch == args.num_epochs - 1:\n",
    "        # unet = accelerator.unwrap_model(model)\n",
    "        unet = model\n",
    "\n",
    "        if args.use_ema:\n",
    "            ema_model.store(unet.parameters())\n",
    "            ema_model.copy_to(unet.parameters())\n",
    "\n",
    "        pipeline = DDPMPipeline(\n",
    "            unet=unet,\n",
    "            scheduler=noise_scheduler,\n",
    "        )\n",
    "\n",
    "        generator = torch.Generator(device=pipeline.device).manual_seed(0)\n",
    "        # run pipeline in inference (sample random noise and denoise)\n",
    "        images = pipeline(\n",
    "            generator=generator,\n",
    "            batch_size=args.eval_batch_size,\n",
    "            num_inference_steps=args.ddpm_num_inference_steps,\n",
    "            output_type=\"np\",\n",
    "        ).images\n",
    "\n",
    "        if args.use_ema:\n",
    "            ema_model.restore(unet.parameters())\n",
    "\n",
    "        # denormalize the images and save to tensorboard\n",
    "        images_processed = (images * 255).round().astype(\"uint8\")\n",
    "\n",
    "    if epoch % args.save_model_epochs == 0 or epoch == args.num_epochs - 1:\n",
    "        # save the model\n",
    "        # unet = accelerator.unwrap_model(model)\n",
    "        unet = model\n",
    "\n",
    "        if args.use_ema:\n",
    "            ema_model.store(unet.parameters())\n",
    "            ema_model.copy_to(unet.parameters())\n",
    "\n",
    "        pipeline = DDPMPipeline(\n",
    "            unet=unet,\n",
    "            scheduler=noise_scheduler,\n",
    "        )\n",
    "\n",
    "        pipeline.save_pretrained(args.output_dir)\n",
    "\n",
    "        if args.use_ema:\n",
    "            ema_model.restore(unet.parameters())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "04a58b23",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dockerfile\t\tdocker-compose.yml  requirements.txt\n",
      "diffusion_models_class\tminimal_diffusion   wandb\n"
     ]
    }
   ],
   "source": [
    "!ls"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dd8caf3e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 23 sec torch"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
