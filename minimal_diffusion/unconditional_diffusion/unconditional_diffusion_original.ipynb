{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "d9ee4c81",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.11/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "***** Running training *****\n",
      "  Num examples = 8189\n",
      "  Num Epochs = 1\n",
      "  Instantaneous batch size per device = 16\n",
      "  Total train batch size (w. parallel, distributed & accumulation) = 16\n",
      "  Gradient Accumulation steps = 1\n",
      "  Total optimization steps = 512\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mdimweb\u001b[0m to \u001b[32mhttps://api.wandb.ai\u001b[0m. Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.19.11"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/code/wandb/run-20250531_171102-4v44suus</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/dimweb/train_unconditional/runs/4v44suus' target=\"_blank\">clear-lion-6</a></strong> to <a href='https://wandb.ai/dimweb/train_unconditional' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/developer-guide' target=\"_blank\">docs</a>)<br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/dimweb/train_unconditional' target=\"_blank\">https://wandb.ai/dimweb/train_unconditional</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/dimweb/train_unconditional/runs/4v44suus' target=\"_blank\">https://wandb.ai/dimweb/train_unconditional/runs/4v44suus</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 0:  98%|█████████▊| 502/512 [00:49<00:02,  3.85it/s, ema_decay=0.991, loss=0.0785, lr=9.33e-5, step=502]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved state to minimal_diffusion/unconditional_diffusion/ddpm-model-64/checkpoint-500\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 0: 100%|██████████| 512/512 [00:50<00:00, 10.10it/s, ema_decay=0.991, loss=0.0549, lr=0, step=512]      \n",
      "100%|██████████| 1000/1000 [00:17<00:00, 55.95it/s]\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m The nbformat package was not found. It is required to save notebook history.\n"
     ]
    },
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<br>    <style><br>        .wandb-row {<br>            display: flex;<br>            flex-direction: row;<br>            flex-wrap: wrap;<br>            justify-content: flex-start;<br>            width: 100%;<br>        }<br>        .wandb-col {<br>            display: flex;<br>            flex-direction: column;<br>            flex-basis: 100%;<br>            flex: 1;<br>            padding: 10px;<br>        }<br>    </style><br><div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>ema_decay</td><td>▁▆▇▇▇███████████████████████████████████</td></tr><tr><td>epoch</td><td>▁</td></tr><tr><td>loss</td><td>█▆▅▄▄▁▂▂▂▁▂▁▂▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▂</td></tr><tr><td>lr</td><td>▁▂▂▂▂▃▃▃▃▃▃▃▃▄▄▄▄▄▄▄▅▅▅▅▅▅▅▅▆▆▆▆▆▇▇▇▇██▂</td></tr><tr><td>step</td><td>▁▁▁▁▂▂▂▂▂▂▃▃▃▃▄▄▄▄▄▄▄▄▄▄▅▅▆▆▆▆▆▆▆▆▆▇▇▇██</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>ema_decay</td><td>0.99071</td></tr><tr><td>epoch</td><td>0</td></tr><tr><td>loss</td><td>0.05492</td></tr><tr><td>lr</td><td>0</td></tr><tr><td>step</td><td>512</td></tr></table><br/></div></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run <strong style=\"color:#cdcd00\">clear-lion-6</strong> at: <a href='https://wandb.ai/dimweb/train_unconditional/runs/4v44suus' target=\"_blank\">https://wandb.ai/dimweb/train_unconditional/runs/4v44suus</a><br> View project at: <a href='https://wandb.ai/dimweb/train_unconditional' target=\"_blank\">https://wandb.ai/dimweb/train_unconditional</a><br>Synced 5 W&B file(s), 16 media file(s), 0 artifact file(s) and 0 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code>./wandb/run-20250531_171102-4v44suus/logs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import os\n",
    "\n",
    "os.environ[\"WANDB_PROJECT\"] = \"unconditional_image_generation\"\n",
    "\n",
    "import argparse\n",
    "import inspect\n",
    "import logging\n",
    "import math\n",
    "import shutil\n",
    "from datetime import timedelta\n",
    "from pathlib import Path\n",
    "\n",
    "import accelerate\n",
    "import datasets\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "from accelerate import Accelerator, InitProcessGroupKwargs\n",
    "from accelerate.logging import get_logger\n",
    "from accelerate.utils import ProjectConfiguration\n",
    "from datasets import load_dataset\n",
    "from huggingface_hub import create_repo, upload_folder\n",
    "from packaging import version\n",
    "from torchvision import transforms\n",
    "from tqdm.auto import tqdm\n",
    "\n",
    "import diffusers\n",
    "from diffusers import DDPMPipeline, DDPMScheduler, UNet2DModel\n",
    "from diffusers.optimization import get_scheduler\n",
    "from diffusers.training_utils import EMAModel\n",
    "from diffusers.utils import (\n",
    "    check_min_version,\n",
    "    is_accelerate_version,\n",
    "    is_tensorboard_available,\n",
    "    is_wandb_available,\n",
    ")\n",
    "from diffusers.utils.import_utils import is_xformers_available\n",
    "\n",
    "from dataclasses import dataclass\n",
    "import wandb\n",
    "\n",
    "\n",
    "def _extract_into_tensor(arr, timesteps, broadcast_shape):\n",
    "    \"\"\"\n",
    "    Extract values from a 1-D numpy array for a batch of indices.\n",
    "\n",
    "    :param arr: the 1-D numpy array.\n",
    "    :param timesteps: a tensor of indices into the array to extract.\n",
    "    :param broadcast_shape: a larger shape of K dimensions with the batch\n",
    "                            dimension equal to the length of timesteps.\n",
    "    :return: a tensor of shape [batch_size, 1, ...] where the shape has K dims.\n",
    "    \"\"\"\n",
    "    if not isinstance(arr, torch.Tensor):\n",
    "        arr = torch.from_numpy(arr)\n",
    "    res = arr[timesteps].float().to(timesteps.device)\n",
    "    while len(res.shape) < len(broadcast_shape):\n",
    "        res = res[..., None]\n",
    "    return res.expand(broadcast_shape)\n",
    "\n",
    "\n",
    "@dataclass\n",
    "class Args:\n",
    "    resolution: int = 64\n",
    "    ddpm_num_steps: int = 1000\n",
    "    ddpm_beta_schedule: str = \"linear\"\n",
    "    learning_rate: float = 1e-4\n",
    "    adam_beta1: float = 0.95\n",
    "    adam_beta2: float = 0.999\n",
    "    adam_weight_decay: float = 1e-6\n",
    "    adam_epsilon: float = 1e-08\n",
    "    dataset_name: str = \"huggan/flowers-102-categories\"\n",
    "    dataset_config_name: str = None\n",
    "    cache_dir: str = None\n",
    "    ema_max_decay: float = 0.9999\n",
    "    ema_inv_gamma: float = 1.0\n",
    "    ema_power: float = 3 / 4\n",
    "    train_batch_size: int = 16\n",
    "    eval_batch_size: int = 16\n",
    "    gradient_accumulation_steps: int = 1\n",
    "    lr_scheduler: str = \"cosine\"\n",
    "    lr_warmup_steps: int = 500\n",
    "    num_epochs: int = 1\n",
    "    center_crop: bool = False\n",
    "    random_flip: bool = False\n",
    "    dataloader_num_workers: int = 0\n",
    "    use_ema: bool = True\n",
    "    checkpointing_steps: int = 500\n",
    "    checkpoints_total_limit: int = None\n",
    "    output_dir: str = \"minimal_diffusion/unconditional_diffusion/ddpm-model-64\"\n",
    "    save_images_epochs: int = 10\n",
    "    save_model_epochs: int = 1\n",
    "    prediction_type: str = \"epsilon\"\n",
    "    logging_dir: str = \"logs\"\n",
    "    mixed_precision: str = \"no\"\n",
    "    logger: str = \"wandb\"\n",
    "    ddpm_num_inference_steps: int = 1000\n",
    "    project_name: str = \"train_unconditional\"\n",
    "\n",
    "\n",
    "args = Args()\n",
    "\n",
    "model = UNet2DModel(\n",
    "    sample_size=args.resolution,\n",
    "    in_channels=3,\n",
    "    out_channels=3,\n",
    "    layers_per_block=2,\n",
    "    block_out_channels=(128, 128, 256, 256, 512, 512),\n",
    "    down_block_types=(\n",
    "        \"DownBlock2D\",\n",
    "        \"DownBlock2D\",\n",
    "        \"DownBlock2D\",\n",
    "        \"DownBlock2D\",\n",
    "        \"AttnDownBlock2D\",\n",
    "        \"DownBlock2D\",\n",
    "    ),\n",
    "    up_block_types=(\n",
    "        \"UpBlock2D\",\n",
    "        \"AttnUpBlock2D\",\n",
    "        \"UpBlock2D\",\n",
    "        \"UpBlock2D\",\n",
    "        \"UpBlock2D\",\n",
    "        \"UpBlock2D\",\n",
    "    ),\n",
    ")\n",
    "model = model.cuda()\n",
    "ema_model = EMAModel(\n",
    "    model.parameters(),\n",
    "    decay=args.ema_max_decay,\n",
    "    use_ema_warmup=True,\n",
    "    inv_gamma=args.ema_inv_gamma,\n",
    "    power=args.ema_power,\n",
    "    model_cls=UNet2DModel,\n",
    "    model_config=model.config,\n",
    ")\n",
    "\n",
    "noise_scheduler = DDPMScheduler(\n",
    "    num_train_timesteps=args.ddpm_num_steps,\n",
    "    beta_schedule=args.ddpm_beta_schedule,\n",
    ")\n",
    "\n",
    "optimizer = torch.optim.AdamW(\n",
    "    model.parameters(),\n",
    "    lr=args.learning_rate,\n",
    "    betas=(args.adam_beta1, args.adam_beta2),\n",
    "    weight_decay=args.adam_weight_decay,\n",
    "    eps=args.adam_epsilon,\n",
    ")\n",
    "\n",
    "dataset = load_dataset(\n",
    "    args.dataset_name,\n",
    "    args.dataset_config_name,\n",
    "    cache_dir=args.cache_dir,\n",
    "    split=\"train\",\n",
    ")\n",
    "\n",
    "augmentations = transforms.Compose(\n",
    "    [\n",
    "        transforms.Resize(\n",
    "            args.resolution, interpolation=transforms.InterpolationMode.BILINEAR\n",
    "        ),\n",
    "        (\n",
    "            transforms.CenterCrop(args.resolution)\n",
    "            if args.center_crop\n",
    "            else transforms.RandomCrop(args.resolution)\n",
    "        ),\n",
    "        (\n",
    "            transforms.RandomHorizontalFlip()\n",
    "            if args.random_flip\n",
    "            else transforms.Lambda(lambda x: x)\n",
    "        ),\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize(\n",
    "            [0.5],\n",
    "            [0.5],\n",
    "        ),\n",
    "    ]\n",
    ")\n",
    "\n",
    "\n",
    "def transform_images(examples):\n",
    "    images = [augmentations(image.convert(\"RGB\")) for image in examples[\"image\"]]\n",
    "    return {\"input\": images}\n",
    "\n",
    "\n",
    "dataset.set_transform(transform_images)\n",
    "train_dataloader = torch.utils.data.DataLoader(\n",
    "    dataset,\n",
    "    batch_size=args.train_batch_size,\n",
    "    shuffle=True,\n",
    "    num_workers=args.dataloader_num_workers,\n",
    ")\n",
    "\n",
    "# Initialize the learning rate scheduler\n",
    "lr_scheduler = get_scheduler(\n",
    "    args.lr_scheduler,\n",
    "    optimizer=optimizer,\n",
    "    num_warmup_steps=args.lr_warmup_steps * args.gradient_accumulation_steps,\n",
    "    num_training_steps=(len(train_dataloader) * args.num_epochs),\n",
    ")\n",
    "\n",
    "\n",
    "total_batch_size = args.train_batch_size * args.gradient_accumulation_steps\n",
    "num_update_steps_per_epoch = math.ceil(\n",
    "    len(train_dataloader) / args.gradient_accumulation_steps\n",
    ")\n",
    "max_train_steps = args.num_epochs * num_update_steps_per_epoch\n",
    "\n",
    "\n",
    "print(\"***** Running training *****\")\n",
    "print(f\"  Num examples = {len(dataset)}\")\n",
    "print(f\"  Num Epochs = {args.num_epochs}\")\n",
    "print(f\"  Instantaneous batch size per device = {args.train_batch_size}\")\n",
    "print(\n",
    "    f\"  Total train batch size (w. parallel, distributed & accumulation) = {total_batch_size}\"\n",
    ")\n",
    "print(f\"  Gradient Accumulation steps = {args.gradient_accumulation_steps}\")\n",
    "print(f\"  Total optimization steps = {max_train_steps}\")\n",
    "\n",
    "global_step = 0\n",
    "first_epoch = 0\n",
    "\n",
    "weight_dtype = torch.float32\n",
    "# model.enable_xformers_memory_efficient_attention()\n",
    "logging_dir = os.path.join(args.output_dir, args.logging_dir)\n",
    "accelerator_project_config = ProjectConfiguration(\n",
    "    project_dir=args.output_dir, logging_dir=logging_dir\n",
    ")\n",
    "accelerator = Accelerator(\n",
    "    gradient_accumulation_steps=args.gradient_accumulation_steps,\n",
    "    mixed_precision=args.mixed_precision,\n",
    "    log_with=args.logger,\n",
    "    project_config=accelerator_project_config,\n",
    "    # kwargs_handlers=[kwargs],\n",
    ")\n",
    "model, optimizer, train_dataloader, lr_scheduler = accelerator.prepare(\n",
    "    model, optimizer, train_dataloader, lr_scheduler\n",
    ")\n",
    "\n",
    "if accelerator.is_main_process:\n",
    "    run = os.path.split(args.project_name)[-1].split(\".\")[0]\n",
    "    accelerator.init_trackers(run)\n",
    "\n",
    "# Train!\n",
    "for epoch in range(first_epoch, args.num_epochs):\n",
    "    model.train()\n",
    "    progress_bar = tqdm(\n",
    "        total=num_update_steps_per_epoch,\n",
    "        disable=not accelerator.is_local_main_process,\n",
    "    )\n",
    "    progress_bar.set_description(f\"Epoch {epoch}\")\n",
    "    for step, batch in enumerate(train_dataloader):\n",
    "        # Skip steps until we reach the resumed step\n",
    "\n",
    "        clean_images = batch[\"input\"].to(weight_dtype)\n",
    "        # Sample noise that we'll add to the images\n",
    "        noise = torch.randn(\n",
    "            clean_images.shape, dtype=weight_dtype, device=clean_images.device\n",
    "        )\n",
    "        bsz = clean_images.shape[0]\n",
    "        # Sample a random timestep for each image\n",
    "        timesteps = torch.randint(\n",
    "            0,\n",
    "            noise_scheduler.config.num_train_timesteps,\n",
    "            (bsz,),\n",
    "            device=clean_images.device,\n",
    "        ).long()\n",
    "\n",
    "        # Add noise to the clean images according to the noise magnitude at each timestep\n",
    "        # (this is the forward diffusion process)\n",
    "        noisy_images = noise_scheduler.add_noise(clean_images, noise, timesteps)\n",
    "\n",
    "        with accelerator.accumulate(model):\n",
    "            # Predict the noise residual\n",
    "            model_output = model(noisy_images, timesteps).sample\n",
    "\n",
    "            if args.prediction_type == \"epsilon\":\n",
    "                loss = F.mse_loss(\n",
    "                    model_output.float(),\n",
    "                    noise.float(),\n",
    "                )  # this could have different weights!\n",
    "            elif args.prediction_type == \"sample\":\n",
    "                alpha_t = _extract_into_tensor(\n",
    "                    noise_scheduler.alphas_cumprod,\n",
    "                    timesteps,\n",
    "                    (clean_images.shape[0], 1, 1, 1),\n",
    "                )\n",
    "                snr_weights = alpha_t / (1 - alpha_t)\n",
    "                # use SNR weighting from distillation paper\n",
    "                loss = snr_weights * F.mse_loss(\n",
    "                    model_output.float(), clean_images.float(), reduction=\"none\"\n",
    "                )\n",
    "                loss = loss.mean()\n",
    "            else:\n",
    "                raise ValueError(f\"Unsupported prediction type: {args.prediction_type}\")\n",
    "\n",
    "            accelerator.backward(loss)\n",
    "\n",
    "            if accelerator.sync_gradients:\n",
    "                accelerator.clip_grad_norm_(model.parameters(), 1.0)\n",
    "            optimizer.step()\n",
    "            lr_scheduler.step()\n",
    "            optimizer.zero_grad()\n",
    "\n",
    "        # Checks if the accelerator has performed an optimization step behind the scenes\n",
    "        if accelerator.sync_gradients:\n",
    "            if args.use_ema:\n",
    "                ema_model.step(model.parameters())\n",
    "            progress_bar.update(1)\n",
    "            global_step += 1\n",
    "\n",
    "            if accelerator.is_main_process:\n",
    "                if global_step % args.checkpointing_steps == 0:\n",
    "                    # _before_ saving state, check if this save would set us over the `checkpoints_total_limit`\n",
    "\n",
    "                    save_path = os.path.join(\n",
    "                        args.output_dir, f\"checkpoint-{global_step}\"\n",
    "                    )\n",
    "                    accelerator.save_state(save_path)\n",
    "                    print(f\"Saved state to {save_path}\")\n",
    "\n",
    "        logs = {\n",
    "            \"loss\": loss.detach().item(),\n",
    "            \"lr\": lr_scheduler.get_last_lr()[0],\n",
    "            \"step\": global_step,\n",
    "        }\n",
    "        if args.use_ema:\n",
    "            logs[\"ema_decay\"] = ema_model.cur_decay_value\n",
    "        progress_bar.set_postfix(**logs)\n",
    "        accelerator.log(logs, step=global_step)\n",
    "    progress_bar.close()\n",
    "\n",
    "    accelerator.wait_for_everyone()\n",
    "\n",
    "    # Generate sample images for visual inspection\n",
    "    if accelerator.is_main_process:\n",
    "        if epoch % args.save_images_epochs == 0 or epoch == args.num_epochs - 1:\n",
    "            unet = accelerator.unwrap_model(model)\n",
    "\n",
    "            if args.use_ema:\n",
    "                ema_model.store(unet.parameters())\n",
    "                ema_model.copy_to(unet.parameters())\n",
    "\n",
    "            pipeline = DDPMPipeline(\n",
    "                unet=unet,\n",
    "                scheduler=noise_scheduler,\n",
    "            )\n",
    "\n",
    "            generator = torch.Generator(device=pipeline.device).manual_seed(0)\n",
    "            # run pipeline in inference (sample random noise and denoise)\n",
    "            images = pipeline(\n",
    "                generator=generator,\n",
    "                batch_size=args.eval_batch_size,\n",
    "                num_inference_steps=args.ddpm_num_inference_steps,\n",
    "                output_type=\"np\",\n",
    "            ).images\n",
    "\n",
    "            if args.use_ema:\n",
    "                ema_model.restore(unet.parameters())\n",
    "\n",
    "            # denormalize the images and save to tensorboard\n",
    "            images_processed = (images * 255).round().astype(\"uint8\")\n",
    "\n",
    "            if args.logger == \"wandb\":\n",
    "                # Upcoming `log_images` helper coming in https://github.com/huggingface/accelerate/pull/962/files\n",
    "                accelerator.get_tracker(\"wandb\").log(\n",
    "                    {\n",
    "                        \"test_samples\": [wandb.Image(img) for img in images_processed],\n",
    "                        \"epoch\": epoch,\n",
    "                    },\n",
    "                    step=global_step,\n",
    "                )\n",
    "\n",
    "        if epoch % args.save_model_epochs == 0 or epoch == args.num_epochs - 1:\n",
    "            # save the model\n",
    "            unet = accelerator.unwrap_model(model)\n",
    "\n",
    "            if args.use_ema:\n",
    "                ema_model.store(unet.parameters())\n",
    "                ema_model.copy_to(unet.parameters())\n",
    "\n",
    "            pipeline = DDPMPipeline(\n",
    "                unet=unet,\n",
    "                scheduler=noise_scheduler,\n",
    "            )\n",
    "\n",
    "            pipeline.save_pretrained(args.output_dir)\n",
    "\n",
    "            if args.use_ema:\n",
    "                ema_model.restore(unet.parameters())\n",
    "\n",
    "\n",
    "accelerator.end_training()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
