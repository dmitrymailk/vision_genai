{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "d9ee4c81",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "***** Running training *****\n",
      "  Num examples = 8189\n",
      "  Num Epochs = 1\n",
      "  Instantaneous batch size per device = 16\n",
      "  Total train batch size (w. parallel, distributed & accumulation) = 16\n",
      "  Gradient Accumulation steps = 1\n",
      "  Total optimization steps = 512\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.19.11"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/code/wandb/run-20250531_211520-qyn2dlkk</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/dimweb/train_unconditional/runs/qyn2dlkk' target=\"_blank\">scarlet-dust-14</a></strong> to <a href='https://wandb.ai/dimweb/train_unconditional' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/developer-guide' target=\"_blank\">docs</a>)<br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/dimweb/train_unconditional' target=\"_blank\">https://wandb.ai/dimweb/train_unconditional</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/dimweb/train_unconditional/runs/qyn2dlkk' target=\"_blank\">https://wandb.ai/dimweb/train_unconditional/runs/qyn2dlkk</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 0:  98%|█████████▊| 502/512 [00:49<00:02,  3.85it/s, ema_decay=0.991, loss=0.0448, lr=9.33e-5, step=502]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved state to minimal_diffusion/unconditional_diffusion/ddpm-model-64/checkpoint-500\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 0: 100%|██████████| 512/512 [00:50<00:00, 10.20it/s, ema_decay=0.991, loss=0.121, lr=0, step=512]       \n",
      "100%|██████████| 1000/1000 [00:17<00:00, 55.99it/s]\n"
     ]
    },
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<br>    <style><br>        .wandb-row {<br>            display: flex;<br>            flex-direction: row;<br>            flex-wrap: wrap;<br>            justify-content: flex-start;<br>            width: 100%;<br>        }<br>        .wandb-col {<br>            display: flex;<br>            flex-direction: column;<br>            flex-basis: 100%;<br>            flex: 1;<br>            padding: 10px;<br>        }<br>    </style><br><div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>ema_decay</td><td>▁▄▄▅▆▇▇▇▇▇▇▇▇▇▇▇████████████████████████</td></tr><tr><td>epoch</td><td>▁</td></tr><tr><td>loss</td><td>█▇▆▆▆▄▄▄▃▂▂▂▂▁▂▂▁▂▁▂▂▁▁▁▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▂</td></tr><tr><td>lr</td><td>▁▁▁▁▂▂▂▂▂▂▃▃▃▃▃▄▄▄▄▄▅▅▅▅▅▆▆▆▇▇▇▇▇▇▇████▆</td></tr><tr><td>step</td><td>▁▁▁▁▁▂▂▂▂▃▃▃▃▃▃▃▃▃▄▄▄▄▄▄▅▅▅▅▅▅▆▆▆▆▆▇▇▇▇█</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>ema_decay</td><td>0.99071</td></tr><tr><td>epoch</td><td>0</td></tr><tr><td>loss</td><td>0.12067</td></tr><tr><td>lr</td><td>0</td></tr><tr><td>step</td><td>512</td></tr></table><br/></div></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run <strong style=\"color:#cdcd00\">scarlet-dust-14</strong> at: <a href='https://wandb.ai/dimweb/train_unconditional/runs/qyn2dlkk' target=\"_blank\">https://wandb.ai/dimweb/train_unconditional/runs/qyn2dlkk</a><br> View project at: <a href='https://wandb.ai/dimweb/train_unconditional' target=\"_blank\">https://wandb.ai/dimweb/train_unconditional</a><br>Synced 5 W&B file(s), 16 media file(s), 0 artifact file(s) and 0 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code>./wandb/run-20250531_211520-qyn2dlkk/logs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import os\n",
    "\n",
    "\n",
    "import argparse\n",
    "import inspect\n",
    "import logging\n",
    "import math\n",
    "import shutil\n",
    "from datetime import timedelta\n",
    "from pathlib import Path\n",
    "\n",
    "import accelerate\n",
    "import datasets\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "from accelerate import Accelerator, InitProcessGroupKwargs\n",
    "from accelerate.logging import get_logger\n",
    "from accelerate.utils import ProjectConfiguration\n",
    "from datasets import load_dataset\n",
    "from huggingface_hub import create_repo, upload_folder\n",
    "from packaging import version\n",
    "from torchvision import transforms\n",
    "from tqdm.auto import tqdm\n",
    "\n",
    "import diffusers\n",
    "\n",
    "# from diffusers import DDPMPipeline, DDPMScheduler, UNet2DModel\n",
    "# from diffusers import DDPMPipeline, UNet2DModel\n",
    "from diffusers import UNet2DModel\n",
    "from diffusers.optimization import get_scheduler\n",
    "from diffusers.training_utils import EMAModel\n",
    "from diffusers.utils import (\n",
    "    check_min_version,\n",
    "    is_accelerate_version,\n",
    "    is_tensorboard_available,\n",
    "    is_wandb_available,\n",
    ")\n",
    "from diffusers.utils.import_utils import is_xformers_available\n",
    "\n",
    "from dataclasses import dataclass\n",
    "import wandb\n",
    "import math\n",
    "from dataclasses import dataclass\n",
    "from typing import List, Optional, Tuple, Union\n",
    "\n",
    "import numpy as np\n",
    "import torch\n",
    "\n",
    "from diffusers.configuration_utils import ConfigMixin, register_to_config\n",
    "from diffusers.utils import BaseOutput\n",
    "from diffusers.utils.torch_utils import randn_tensor\n",
    "from diffusers.schedulers.scheduling_utils import (\n",
    "    KarrasDiffusionSchedulers,\n",
    "    SchedulerMixin,\n",
    ")\n",
    "from diffusers.models import UNet2DModel\n",
    "from diffusers.utils.torch_utils import randn_tensor\n",
    "from diffusers.pipelines.pipeline_utils import DiffusionPipeline, ImagePipelineOutput\n",
    "\n",
    "from diffusers.configuration_utils import ConfigMixin, register_to_config\n",
    "from diffusers.utils import BaseOutput\n",
    "from diffusers.models.embeddings import (\n",
    "    GaussianFourierProjection,\n",
    "    TimestepEmbedding,\n",
    "    Timesteps,\n",
    ")\n",
    "from diffusers.models.modeling_utils import ModelMixin\n",
    "from diffusers.models.unets.unet_2d_blocks import (\n",
    "    # UNetMidBlock2D,\n",
    "    get_down_block,\n",
    "    get_up_block,\n",
    ")\n",
    "from diffusers.models.unets.unet_2d import UNet2DOutput\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "from diffusers.schedulers.scheduling_ddpm import DDPMSchedulerOutput\n",
    "from diffusers.utils.torch_utils import is_torch_version, maybe_allow_in_graph\n",
    "from diffusers.models.attention_processor import (\n",
    "    Attention,\n",
    "    AttnAddedKVProcessor,\n",
    "    AttnAddedKVProcessor2_0,\n",
    "    SpatialNorm,\n",
    ")\n",
    "from diffusers.models.normalization import AdaGroupNorm\n",
    "from diffusers.models.resnet import (\n",
    "    Downsample2D,\n",
    "    FirDownsample2D,\n",
    "    FirUpsample2D,\n",
    "    KDownsample2D,\n",
    "    KUpsample2D,\n",
    ")\n",
    "from diffusers.models.activations import get_activation\n",
    "from diffusers.models.downsampling import (  # noqa\n",
    "    Downsample1D,\n",
    "    Downsample2D,\n",
    "    FirDownsample2D,\n",
    "    KDownsample2D,\n",
    "    downsample_2d,\n",
    ")\n",
    "from diffusers.models.upsampling import (  # noqa\n",
    "    FirUpsample2D,\n",
    "    KUpsample2D,\n",
    "    Upsample1D,\n",
    "    Upsample2D,\n",
    "    upfirdn2d_native,\n",
    "    upsample_2d,\n",
    ")\n",
    "\n",
    "\n",
    "def betas_for_alpha_bar(\n",
    "    num_diffusion_timesteps,\n",
    "    max_beta=0.999,\n",
    "    alpha_transform_type=\"cosine\",\n",
    "):\n",
    "    \"\"\"\n",
    "    Create a beta schedule that discretizes the given alpha_t_bar function, which defines the cumulative product of\n",
    "    (1-beta) over time from t = [0,1].\n",
    "\n",
    "    Contains a function alpha_bar that takes an argument t and transforms it to the cumulative product of (1-beta) up\n",
    "    to that part of the diffusion process.\n",
    "\n",
    "\n",
    "    Args:\n",
    "        num_diffusion_timesteps (`int`): the number of betas to produce.\n",
    "        max_beta (`float`): the maximum beta to use; use values lower than 1 to\n",
    "                     prevent singularities.\n",
    "        alpha_transform_type (`str`, *optional*, default to `cosine`): the type of noise schedule for alpha_bar.\n",
    "                     Choose from `cosine` or `exp`\n",
    "\n",
    "    Returns:\n",
    "        betas (`np.ndarray`): the betas used by the scheduler to step the model outputs\n",
    "    \"\"\"\n",
    "    if alpha_transform_type == \"cosine\":\n",
    "\n",
    "        def alpha_bar_fn(t):\n",
    "            return math.cos((t + 0.008) / 1.008 * math.pi / 2) ** 2\n",
    "\n",
    "    elif alpha_transform_type == \"exp\":\n",
    "\n",
    "        def alpha_bar_fn(t):\n",
    "            return math.exp(t * -12.0)\n",
    "\n",
    "    else:\n",
    "        raise ValueError(f\"Unsupported alpha_transform_type: {alpha_transform_type}\")\n",
    "\n",
    "    betas = []\n",
    "    for i in range(num_diffusion_timesteps):\n",
    "        t1 = i / num_diffusion_timesteps\n",
    "        t2 = (i + 1) / num_diffusion_timesteps\n",
    "        betas.append(min(1 - alpha_bar_fn(t2) / alpha_bar_fn(t1), max_beta))\n",
    "    return torch.tensor(betas, dtype=torch.float32)\n",
    "\n",
    "\n",
    "# Copied from diffusers.schedulers.scheduling_ddim.rescale_zero_terminal_snr\n",
    "def rescale_zero_terminal_snr(betas):\n",
    "    \"\"\"\n",
    "    Rescales betas to have zero terminal SNR Based on https://arxiv.org/pdf/2305.08891.pdf (Algorithm 1)\n",
    "\n",
    "\n",
    "    Args:\n",
    "        betas (`torch.Tensor`):\n",
    "            the betas that the scheduler is being initialized with.\n",
    "\n",
    "    Returns:\n",
    "        `torch.Tensor`: rescaled betas with zero terminal SNR\n",
    "    \"\"\"\n",
    "    # Convert betas to alphas_bar_sqrt\n",
    "    alphas = 1.0 - betas\n",
    "    alphas_cumprod = torch.cumprod(alphas, dim=0)\n",
    "    alphas_bar_sqrt = alphas_cumprod.sqrt()\n",
    "\n",
    "    # Store old values.\n",
    "    alphas_bar_sqrt_0 = alphas_bar_sqrt[0].clone()\n",
    "    alphas_bar_sqrt_T = alphas_bar_sqrt[-1].clone()\n",
    "\n",
    "    # Shift so the last timestep is zero.\n",
    "    alphas_bar_sqrt -= alphas_bar_sqrt_T\n",
    "\n",
    "    # Scale so the first timestep is back to the old value.\n",
    "    alphas_bar_sqrt *= alphas_bar_sqrt_0 / (alphas_bar_sqrt_0 - alphas_bar_sqrt_T)\n",
    "\n",
    "    # Convert alphas_bar_sqrt to betas\n",
    "    alphas_bar = alphas_bar_sqrt**2  # Revert sqrt\n",
    "    alphas = alphas_bar[1:] / alphas_bar[:-1]  # Revert cumprod\n",
    "    alphas = torch.cat([alphas_bar[0:1], alphas])\n",
    "    betas = 1 - alphas\n",
    "\n",
    "    return betas\n",
    "\n",
    "\n",
    "class DDPMScheduler(SchedulerMixin, ConfigMixin):\n",
    "    \"\"\"\n",
    "    `DDPMScheduler` explores the connections between denoising score matching and Langevin dynamics sampling.\n",
    "\n",
    "    This model inherits from [`SchedulerMixin`] and [`ConfigMixin`]. Check the superclass documentation for the generic\n",
    "    methods the library implements for all schedulers such as loading and saving.\n",
    "\n",
    "    Args:\n",
    "        num_train_timesteps (`int`, defaults to 1000):\n",
    "            The number of diffusion steps to train the model.\n",
    "        beta_start (`float`, defaults to 0.0001):\n",
    "            The starting `beta` value of inference.\n",
    "        beta_end (`float`, defaults to 0.02):\n",
    "            The final `beta` value.\n",
    "        beta_schedule (`str`, defaults to `\"linear\"`):\n",
    "            The beta schedule, a mapping from a beta range to a sequence of betas for stepping the model. Choose from\n",
    "            `linear`, `scaled_linear`, `squaredcos_cap_v2`, or `sigmoid`.\n",
    "        trained_betas (`np.ndarray`, *optional*):\n",
    "            An array of betas to pass directly to the constructor without using `beta_start` and `beta_end`.\n",
    "        variance_type (`str`, defaults to `\"fixed_small\"`):\n",
    "            Clip the variance when adding noise to the denoised sample. Choose from `fixed_small`, `fixed_small_log`,\n",
    "            `fixed_large`, `fixed_large_log`, `learned` or `learned_range`.\n",
    "        clip_sample (`bool`, defaults to `True`):\n",
    "            Clip the predicted sample for numerical stability.\n",
    "        clip_sample_range (`float`, defaults to 1.0):\n",
    "            The maximum magnitude for sample clipping. Valid only when `clip_sample=True`.\n",
    "        prediction_type (`str`, defaults to `epsilon`, *optional*):\n",
    "            Prediction type of the scheduler function; can be `epsilon` (predicts the noise of the diffusion process),\n",
    "            `sample` (directly predicts the noisy sample`) or `v_prediction` (see section 2.4 of [Imagen\n",
    "            Video](https://imagen.research.google/video/paper.pdf) paper).\n",
    "        thresholding (`bool`, defaults to `False`):\n",
    "            Whether to use the \"dynamic thresholding\" method. This is unsuitable for latent-space diffusion models such\n",
    "            as Stable Diffusion.\n",
    "        dynamic_thresholding_ratio (`float`, defaults to 0.995):\n",
    "            The ratio for the dynamic thresholding method. Valid only when `thresholding=True`.\n",
    "        sample_max_value (`float`, defaults to 1.0):\n",
    "            The threshold value for dynamic thresholding. Valid only when `thresholding=True`.\n",
    "        timestep_spacing (`str`, defaults to `\"leading\"`):\n",
    "            The way the timesteps should be scaled. Refer to Table 2 of the [Common Diffusion Noise Schedules and\n",
    "            Sample Steps are Flawed](https://huggingface.co/papers/2305.08891) for more information.\n",
    "        steps_offset (`int`, defaults to 0):\n",
    "            An offset added to the inference steps, as required by some model families.\n",
    "        rescale_betas_zero_snr (`bool`, defaults to `False`):\n",
    "            Whether to rescale the betas to have zero terminal SNR. This enables the model to generate very bright and\n",
    "            dark samples instead of limiting it to samples with medium brightness. Loosely related to\n",
    "            [`--offset_noise`](https://github.com/huggingface/diffusers/blob/74fd735eb073eb1d774b1ab4154a0876eb82f055/examples/dreambooth/train_dreambooth.py#L506).\n",
    "    \"\"\"\n",
    "\n",
    "    _compatibles = [e.name for e in KarrasDiffusionSchedulers]\n",
    "    order = 1\n",
    "\n",
    "    @register_to_config\n",
    "    def __init__(\n",
    "        self,\n",
    "        num_train_timesteps: int = 1000,\n",
    "        beta_start: float = 0.0001,\n",
    "        beta_end: float = 0.02,\n",
    "        beta_schedule: str = \"linear\",\n",
    "        trained_betas: Optional[Union[np.ndarray, List[float]]] = None,\n",
    "        variance_type: str = \"fixed_small\",\n",
    "        clip_sample: bool = True,\n",
    "        prediction_type: str = \"epsilon\",\n",
    "        thresholding: bool = False,\n",
    "        dynamic_thresholding_ratio: float = 0.995,\n",
    "        clip_sample_range: float = 1.0,\n",
    "        sample_max_value: float = 1.0,\n",
    "        timestep_spacing: str = \"leading\",\n",
    "        steps_offset: int = 0,\n",
    "        rescale_betas_zero_snr: bool = False,\n",
    "    ):\n",
    "        if trained_betas is not None:\n",
    "            self.betas = torch.tensor(trained_betas, dtype=torch.float32)\n",
    "        elif beta_schedule == \"linear\":\n",
    "            self.betas = torch.linspace(\n",
    "                beta_start, beta_end, num_train_timesteps, dtype=torch.float32\n",
    "            )\n",
    "        elif beta_schedule == \"scaled_linear\":\n",
    "            # this schedule is very specific to the latent diffusion model.\n",
    "            self.betas = (\n",
    "                torch.linspace(\n",
    "                    beta_start**0.5,\n",
    "                    beta_end**0.5,\n",
    "                    num_train_timesteps,\n",
    "                    dtype=torch.float32,\n",
    "                )\n",
    "                ** 2\n",
    "            )\n",
    "        elif beta_schedule == \"squaredcos_cap_v2\":\n",
    "            # Glide cosine schedule\n",
    "            self.betas = betas_for_alpha_bar(num_train_timesteps)\n",
    "        elif beta_schedule == \"sigmoid\":\n",
    "            # GeoDiff sigmoid schedule\n",
    "            betas = torch.linspace(-6, 6, num_train_timesteps)\n",
    "            self.betas = torch.sigmoid(betas) * (beta_end - beta_start) + beta_start\n",
    "        else:\n",
    "            raise NotImplementedError(\n",
    "                f\"{beta_schedule} is not implemented for {self.__class__}\"\n",
    "            )\n",
    "\n",
    "        # Rescale for zero SNR\n",
    "        if rescale_betas_zero_snr:\n",
    "            self.betas = rescale_zero_terminal_snr(self.betas)\n",
    "\n",
    "        self.alphas = 1.0 - self.betas\n",
    "        self.alphas_cumprod = torch.cumprod(self.alphas, dim=0)\n",
    "        self.one = torch.tensor(1.0)\n",
    "\n",
    "        # standard deviation of the initial noise distribution\n",
    "        self.init_noise_sigma = 1.0\n",
    "\n",
    "        # setable values\n",
    "        self.custom_timesteps = False\n",
    "        self.num_inference_steps = None\n",
    "        self.timesteps = torch.from_numpy(\n",
    "            np.arange(0, num_train_timesteps)[::-1].copy()\n",
    "        )\n",
    "\n",
    "        self.variance_type = variance_type\n",
    "\n",
    "    def scale_model_input(\n",
    "        self, sample: torch.Tensor, timestep: Optional[int] = None\n",
    "    ) -> torch.Tensor:\n",
    "        \"\"\"\n",
    "        Ensures interchangeability with schedulers that need to scale the denoising model input depending on the\n",
    "        current timestep.\n",
    "\n",
    "        Args:\n",
    "            sample (`torch.Tensor`):\n",
    "                The input sample.\n",
    "            timestep (`int`, *optional*):\n",
    "                The current timestep in the diffusion chain.\n",
    "\n",
    "        Returns:\n",
    "            `torch.Tensor`:\n",
    "                A scaled input sample.\n",
    "        \"\"\"\n",
    "        return sample\n",
    "\n",
    "    def set_timesteps(\n",
    "        self,\n",
    "        num_inference_steps: Optional[int] = None,\n",
    "        device: Union[str, torch.device] = None,\n",
    "        timesteps: Optional[List[int]] = None,\n",
    "    ):\n",
    "        \"\"\"\n",
    "        Sets the discrete timesteps used for the diffusion chain (to be run before inference).\n",
    "\n",
    "        Args:\n",
    "            num_inference_steps (`int`):\n",
    "                The number of diffusion steps used when generating samples with a pre-trained model. If used,\n",
    "                `timesteps` must be `None`.\n",
    "            device (`str` or `torch.device`, *optional*):\n",
    "                The device to which the timesteps should be moved to. If `None`, the timesteps are not moved.\n",
    "            timesteps (`List[int]`, *optional*):\n",
    "                Custom timesteps used to support arbitrary spacing between timesteps. If `None`, then the default\n",
    "                timestep spacing strategy of equal spacing between timesteps is used. If `timesteps` is passed,\n",
    "                `num_inference_steps` must be `None`.\n",
    "\n",
    "        \"\"\"\n",
    "        if num_inference_steps is not None and timesteps is not None:\n",
    "            raise ValueError(\n",
    "                \"Can only pass one of `num_inference_steps` or `custom_timesteps`.\"\n",
    "            )\n",
    "\n",
    "        if timesteps is not None:\n",
    "            for i in range(1, len(timesteps)):\n",
    "                if timesteps[i] >= timesteps[i - 1]:\n",
    "                    raise ValueError(\"`custom_timesteps` must be in descending order.\")\n",
    "\n",
    "            if timesteps[0] >= self.config.num_train_timesteps:\n",
    "                raise ValueError(\n",
    "                    f\"`timesteps` must start before `self.config.train_timesteps`: {self.config.num_train_timesteps}.\"\n",
    "                )\n",
    "\n",
    "            timesteps = np.array(timesteps, dtype=np.int64)\n",
    "            self.custom_timesteps = True\n",
    "        else:\n",
    "            if num_inference_steps > self.config.num_train_timesteps:\n",
    "                raise ValueError(\n",
    "                    f\"`num_inference_steps`: {num_inference_steps} cannot be larger than `self.config.train_timesteps`:\"\n",
    "                    f\" {self.config.num_train_timesteps} as the unet model trained with this scheduler can only handle\"\n",
    "                    f\" maximal {self.config.num_train_timesteps} timesteps.\"\n",
    "                )\n",
    "\n",
    "            self.num_inference_steps = num_inference_steps\n",
    "            self.custom_timesteps = False\n",
    "\n",
    "            # \"linspace\", \"leading\", \"trailing\" corresponds to annotation of Table 2. of https://arxiv.org/abs/2305.08891\n",
    "            if self.config.timestep_spacing == \"linspace\":\n",
    "                timesteps = (\n",
    "                    np.linspace(\n",
    "                        0, self.config.num_train_timesteps - 1, num_inference_steps\n",
    "                    )\n",
    "                    .round()[::-1]\n",
    "                    .copy()\n",
    "                    .astype(np.int64)\n",
    "                )\n",
    "            elif self.config.timestep_spacing == \"leading\":\n",
    "                step_ratio = self.config.num_train_timesteps // self.num_inference_steps\n",
    "                # creates integer timesteps by multiplying by ratio\n",
    "                # casting to int to avoid issues when num_inference_step is power of 3\n",
    "                timesteps = (\n",
    "                    (np.arange(0, num_inference_steps) * step_ratio)\n",
    "                    .round()[::-1]\n",
    "                    .copy()\n",
    "                    .astype(np.int64)\n",
    "                )\n",
    "                timesteps += self.config.steps_offset\n",
    "            elif self.config.timestep_spacing == \"trailing\":\n",
    "                step_ratio = self.config.num_train_timesteps / self.num_inference_steps\n",
    "                # creates integer timesteps by multiplying by ratio\n",
    "                # casting to int to avoid issues when num_inference_step is power of 3\n",
    "                timesteps = np.round(\n",
    "                    np.arange(self.config.num_train_timesteps, 0, -step_ratio)\n",
    "                ).astype(np.int64)\n",
    "                timesteps -= 1\n",
    "            else:\n",
    "                raise ValueError(\n",
    "                    f\"{self.config.timestep_spacing} is not supported. Please make sure to choose one of 'linspace', 'leading' or 'trailing'.\"\n",
    "                )\n",
    "\n",
    "        self.timesteps = torch.from_numpy(timesteps).to(device)\n",
    "\n",
    "    def _get_variance(self, t, predicted_variance=None, variance_type=None):\n",
    "        prev_t = self.previous_timestep(t)\n",
    "\n",
    "        alpha_prod_t = self.alphas_cumprod[t]\n",
    "        alpha_prod_t_prev = self.alphas_cumprod[prev_t] if prev_t >= 0 else self.one\n",
    "        current_beta_t = 1 - alpha_prod_t / alpha_prod_t_prev\n",
    "\n",
    "        # For t > 0, compute predicted variance βt (see formula (6) and (7) from https://arxiv.org/pdf/2006.11239.pdf)\n",
    "        # and sample from it to get previous sample\n",
    "        # x_{t-1} ~ N(pred_prev_sample, variance) == add variance to pred_sample\n",
    "        variance = (1 - alpha_prod_t_prev) / (1 - alpha_prod_t) * current_beta_t\n",
    "\n",
    "        # we always take the log of variance, so clamp it to ensure it's not 0\n",
    "        variance = torch.clamp(variance, min=1e-20)\n",
    "\n",
    "        if variance_type is None:\n",
    "            variance_type = self.config.variance_type\n",
    "\n",
    "        # hacks - were probably added for training stability\n",
    "        if variance_type == \"fixed_small\":\n",
    "            variance = variance\n",
    "        # for rl-diffuser https://arxiv.org/abs/2205.09991\n",
    "        elif variance_type == \"fixed_small_log\":\n",
    "            variance = torch.log(variance)\n",
    "            variance = torch.exp(0.5 * variance)\n",
    "        elif variance_type == \"fixed_large\":\n",
    "            variance = current_beta_t\n",
    "        elif variance_type == \"fixed_large_log\":\n",
    "            # Glide max_log\n",
    "            variance = torch.log(current_beta_t)\n",
    "        elif variance_type == \"learned\":\n",
    "            return predicted_variance\n",
    "        elif variance_type == \"learned_range\":\n",
    "            min_log = torch.log(variance)\n",
    "            max_log = torch.log(current_beta_t)\n",
    "            frac = (predicted_variance + 1) / 2\n",
    "            variance = frac * max_log + (1 - frac) * min_log\n",
    "\n",
    "        return variance\n",
    "\n",
    "    def _threshold_sample(self, sample: torch.Tensor) -> torch.Tensor:\n",
    "        \"\"\"\n",
    "        \"Dynamic thresholding: At each sampling step we set s to a certain percentile absolute pixel value in xt0 (the\n",
    "        prediction of x_0 at timestep t), and if s > 1, then we threshold xt0 to the range [-s, s] and then divide by\n",
    "        s. Dynamic thresholding pushes saturated pixels (those near -1 and 1) inwards, thereby actively preventing\n",
    "        pixels from saturation at each step. We find that dynamic thresholding results in significantly better\n",
    "        photorealism as well as better image-text alignment, especially when using very large guidance weights.\"\n",
    "\n",
    "        https://arxiv.org/abs/2205.11487\n",
    "        \"\"\"\n",
    "        dtype = sample.dtype\n",
    "        batch_size, channels, *remaining_dims = sample.shape\n",
    "\n",
    "        if dtype not in (torch.float32, torch.float64):\n",
    "            sample = (\n",
    "                sample.float()\n",
    "            )  # upcast for quantile calculation, and clamp not implemented for cpu half\n",
    "\n",
    "        # Flatten sample for doing quantile calculation along each image\n",
    "        sample = sample.reshape(batch_size, channels * np.prod(remaining_dims))\n",
    "\n",
    "        abs_sample = sample.abs()  # \"a certain percentile absolute pixel value\"\n",
    "\n",
    "        s = torch.quantile(abs_sample, self.config.dynamic_thresholding_ratio, dim=1)\n",
    "        s = torch.clamp(\n",
    "            s, min=1, max=self.config.sample_max_value\n",
    "        )  # When clamped to min=1, equivalent to standard clipping to [-1, 1]\n",
    "        s = s.unsqueeze(1)  # (batch_size, 1) because clamp will broadcast along dim=0\n",
    "        sample = (\n",
    "            torch.clamp(sample, -s, s) / s\n",
    "        )  # \"we threshold xt0 to the range [-s, s] and then divide by s\"\n",
    "\n",
    "        sample = sample.reshape(batch_size, channels, *remaining_dims)\n",
    "        sample = sample.to(dtype)\n",
    "\n",
    "        return sample\n",
    "\n",
    "    def step(\n",
    "        self,\n",
    "        model_output: torch.Tensor,\n",
    "        timestep: int,\n",
    "        sample: torch.Tensor,\n",
    "        generator=None,\n",
    "        return_dict: bool = True,\n",
    "    ) -> Union[DDPMSchedulerOutput, Tuple]:\n",
    "        \"\"\"\n",
    "        Predict the sample from the previous timestep by reversing the SDE. This function propagates the diffusion\n",
    "        process from the learned model outputs (most often the predicted noise).\n",
    "\n",
    "        Args:\n",
    "            model_output (`torch.Tensor`):\n",
    "                The direct output from learned diffusion model.\n",
    "            timestep (`float`):\n",
    "                The current discrete timestep in the diffusion chain.\n",
    "            sample (`torch.Tensor`):\n",
    "                A current instance of a sample created by the diffusion process.\n",
    "            generator (`torch.Generator`, *optional*):\n",
    "                A random number generator.\n",
    "            return_dict (`bool`, *optional*, defaults to `True`):\n",
    "                Whether or not to return a [`~schedulers.scheduling_ddpm.DDPMSchedulerOutput`] or `tuple`.\n",
    "\n",
    "        Returns:\n",
    "            [`~schedulers.scheduling_ddpm.DDPMSchedulerOutput`] or `tuple`:\n",
    "                If return_dict is `True`, [`~schedulers.scheduling_ddpm.DDPMSchedulerOutput`] is returned, otherwise a\n",
    "                tuple is returned where the first element is the sample tensor.\n",
    "\n",
    "        \"\"\"\n",
    "        t = timestep\n",
    "\n",
    "        prev_t = self.previous_timestep(t)\n",
    "\n",
    "        if model_output.shape[1] == sample.shape[1] * 2 and self.variance_type in [\n",
    "            \"learned\",\n",
    "            \"learned_range\",\n",
    "        ]:\n",
    "            model_output, predicted_variance = torch.split(\n",
    "                model_output, sample.shape[1], dim=1\n",
    "            )\n",
    "        else:\n",
    "            predicted_variance = None\n",
    "\n",
    "        # 1. compute alphas, betas\n",
    "        alpha_prod_t = self.alphas_cumprod[t]\n",
    "        alpha_prod_t_prev = self.alphas_cumprod[prev_t] if prev_t >= 0 else self.one\n",
    "        beta_prod_t = 1 - alpha_prod_t\n",
    "        beta_prod_t_prev = 1 - alpha_prod_t_prev\n",
    "        current_alpha_t = alpha_prod_t / alpha_prod_t_prev\n",
    "        current_beta_t = 1 - current_alpha_t\n",
    "\n",
    "        # 2. compute predicted original sample from predicted noise also called\n",
    "        # \"predicted x_0\" of formula (15) from https://arxiv.org/pdf/2006.11239.pdf\n",
    "        if self.config.prediction_type == \"epsilon\":\n",
    "            pred_original_sample = (\n",
    "                sample - beta_prod_t ** (0.5) * model_output\n",
    "            ) / alpha_prod_t ** (0.5)\n",
    "        elif self.config.prediction_type == \"sample\":\n",
    "            pred_original_sample = model_output\n",
    "        elif self.config.prediction_type == \"v_prediction\":\n",
    "            pred_original_sample = (alpha_prod_t**0.5) * sample - (\n",
    "                beta_prod_t**0.5\n",
    "            ) * model_output\n",
    "        else:\n",
    "            raise ValueError(\n",
    "                f\"prediction_type given as {self.config.prediction_type} must be one of `epsilon`, `sample` or\"\n",
    "                \" `v_prediction`  for the DDPMScheduler.\"\n",
    "            )\n",
    "\n",
    "        # 3. Clip or threshold \"predicted x_0\"\n",
    "        if self.config.thresholding:\n",
    "            pred_original_sample = self._threshold_sample(pred_original_sample)\n",
    "        elif self.config.clip_sample:\n",
    "            pred_original_sample = pred_original_sample.clamp(\n",
    "                -self.config.clip_sample_range, self.config.clip_sample_range\n",
    "            )\n",
    "\n",
    "        # 4. Compute coefficients for pred_original_sample x_0 and current sample x_t\n",
    "        # See formula (7) from https://arxiv.org/pdf/2006.11239.pdf\n",
    "        pred_original_sample_coeff = (\n",
    "            alpha_prod_t_prev ** (0.5) * current_beta_t\n",
    "        ) / beta_prod_t\n",
    "        current_sample_coeff = current_alpha_t ** (0.5) * beta_prod_t_prev / beta_prod_t\n",
    "\n",
    "        # 5. Compute predicted previous sample µ_t\n",
    "        # See formula (7) from https://arxiv.org/pdf/2006.11239.pdf\n",
    "        pred_prev_sample = (\n",
    "            pred_original_sample_coeff * pred_original_sample\n",
    "            + current_sample_coeff * sample\n",
    "        )\n",
    "\n",
    "        # 6. Add noise\n",
    "        variance = 0\n",
    "        if t > 0:\n",
    "            device = model_output.device\n",
    "            variance_noise = randn_tensor(\n",
    "                model_output.shape,\n",
    "                generator=generator,\n",
    "                device=device,\n",
    "                dtype=model_output.dtype,\n",
    "            )\n",
    "            if self.variance_type == \"fixed_small_log\":\n",
    "                variance = (\n",
    "                    self._get_variance(t, predicted_variance=predicted_variance)\n",
    "                    * variance_noise\n",
    "                )\n",
    "            elif self.variance_type == \"learned_range\":\n",
    "                variance = self._get_variance(t, predicted_variance=predicted_variance)\n",
    "                variance = torch.exp(0.5 * variance) * variance_noise\n",
    "            else:\n",
    "                variance = (\n",
    "                    self._get_variance(t, predicted_variance=predicted_variance) ** 0.5\n",
    "                ) * variance_noise\n",
    "\n",
    "        pred_prev_sample = pred_prev_sample + variance\n",
    "\n",
    "        if not return_dict:\n",
    "            return (\n",
    "                pred_prev_sample,\n",
    "                pred_original_sample,\n",
    "            )\n",
    "\n",
    "        return DDPMSchedulerOutput(\n",
    "            prev_sample=pred_prev_sample, pred_original_sample=pred_original_sample\n",
    "        )\n",
    "\n",
    "    def add_noise(\n",
    "        self,\n",
    "        original_samples: torch.Tensor,\n",
    "        noise: torch.Tensor,\n",
    "        timesteps: torch.IntTensor,\n",
    "    ) -> torch.Tensor:\n",
    "        # Make sure alphas_cumprod and timestep have same device and dtype as original_samples\n",
    "        # Move the self.alphas_cumprod to device to avoid redundant CPU to GPU data movement\n",
    "        # for the subsequent add_noise calls\n",
    "        self.alphas_cumprod = self.alphas_cumprod.to(device=original_samples.device)\n",
    "        alphas_cumprod = self.alphas_cumprod.to(dtype=original_samples.dtype)\n",
    "        timesteps = timesteps.to(original_samples.device)\n",
    "\n",
    "        sqrt_alpha_prod = alphas_cumprod[timesteps] ** 0.5\n",
    "        sqrt_alpha_prod = sqrt_alpha_prod.flatten()\n",
    "        while len(sqrt_alpha_prod.shape) < len(original_samples.shape):\n",
    "            sqrt_alpha_prod = sqrt_alpha_prod.unsqueeze(-1)\n",
    "\n",
    "        sqrt_one_minus_alpha_prod = (1 - alphas_cumprod[timesteps]) ** 0.5\n",
    "        sqrt_one_minus_alpha_prod = sqrt_one_minus_alpha_prod.flatten()\n",
    "        while len(sqrt_one_minus_alpha_prod.shape) < len(original_samples.shape):\n",
    "            sqrt_one_minus_alpha_prod = sqrt_one_minus_alpha_prod.unsqueeze(-1)\n",
    "\n",
    "        noisy_samples = (\n",
    "            sqrt_alpha_prod * original_samples + sqrt_one_minus_alpha_prod * noise\n",
    "        )\n",
    "        return noisy_samples\n",
    "\n",
    "    def get_velocity(\n",
    "        self, sample: torch.Tensor, noise: torch.Tensor, timesteps: torch.IntTensor\n",
    "    ) -> torch.Tensor:\n",
    "        # Make sure alphas_cumprod and timestep have same device and dtype as sample\n",
    "        self.alphas_cumprod = self.alphas_cumprod.to(device=sample.device)\n",
    "        alphas_cumprod = self.alphas_cumprod.to(dtype=sample.dtype)\n",
    "        timesteps = timesteps.to(sample.device)\n",
    "\n",
    "        sqrt_alpha_prod = alphas_cumprod[timesteps] ** 0.5\n",
    "        sqrt_alpha_prod = sqrt_alpha_prod.flatten()\n",
    "        while len(sqrt_alpha_prod.shape) < len(sample.shape):\n",
    "            sqrt_alpha_prod = sqrt_alpha_prod.unsqueeze(-1)\n",
    "\n",
    "        sqrt_one_minus_alpha_prod = (1 - alphas_cumprod[timesteps]) ** 0.5\n",
    "        sqrt_one_minus_alpha_prod = sqrt_one_minus_alpha_prod.flatten()\n",
    "        while len(sqrt_one_minus_alpha_prod.shape) < len(sample.shape):\n",
    "            sqrt_one_minus_alpha_prod = sqrt_one_minus_alpha_prod.unsqueeze(-1)\n",
    "\n",
    "        velocity = sqrt_alpha_prod * noise - sqrt_one_minus_alpha_prod * sample\n",
    "        return velocity\n",
    "\n",
    "    def __len__(self):\n",
    "        return self.config.num_train_timesteps\n",
    "\n",
    "    def previous_timestep(self, timestep):\n",
    "        if self.custom_timesteps or self.num_inference_steps:\n",
    "            index = (self.timesteps == timestep).nonzero(as_tuple=True)[0][0]\n",
    "            if index == self.timesteps.shape[0] - 1:\n",
    "                prev_t = torch.tensor(-1)\n",
    "            else:\n",
    "                prev_t = self.timesteps[index + 1]\n",
    "        else:\n",
    "            prev_t = timestep - 1\n",
    "        return prev_t\n",
    "\n",
    "\n",
    "class DDPMPipeline(DiffusionPipeline):\n",
    "    r\"\"\"\n",
    "    Pipeline for image generation.\n",
    "\n",
    "    This model inherits from [`DiffusionPipeline`]. Check the superclass documentation for the generic methods\n",
    "    implemented for all pipelines (downloading, saving, running on a particular device, etc.).\n",
    "\n",
    "    Parameters:\n",
    "        unet ([`UNet2DModel`]):\n",
    "            A `UNet2DModel` to denoise the encoded image latents.\n",
    "        scheduler ([`SchedulerMixin`]):\n",
    "            A scheduler to be used in combination with `unet` to denoise the encoded image. Can be one of\n",
    "            [`DDPMScheduler`], or [`DDIMScheduler`].\n",
    "    \"\"\"\n",
    "\n",
    "    model_cpu_offload_seq = \"unet\"\n",
    "\n",
    "    def __init__(self, unet: UNet2DModel, scheduler: DDPMScheduler):\n",
    "        super().__init__()\n",
    "        self.register_modules(unet=unet, scheduler=scheduler)\n",
    "\n",
    "    @torch.no_grad()\n",
    "    def __call__(\n",
    "        self,\n",
    "        batch_size: int = 1,\n",
    "        generator: Optional[Union[torch.Generator, List[torch.Generator]]] = None,\n",
    "        num_inference_steps: int = 1000,\n",
    "        output_type: Optional[str] = \"pil\",\n",
    "        return_dict: bool = True,\n",
    "    ) -> Union[ImagePipelineOutput, Tuple]:\n",
    "        r\"\"\"\n",
    "        The call function to the pipeline for generation.\n",
    "\n",
    "        Args:\n",
    "            batch_size (`int`, *optional*, defaults to 1):\n",
    "                The number of images to generate.\n",
    "            generator (`torch.Generator`, *optional*):\n",
    "                A [`torch.Generator`](https://pytorch.org/docs/stable/generated/torch.Generator.html) to make\n",
    "                generation deterministic.\n",
    "            num_inference_steps (`int`, *optional*, defaults to 1000):\n",
    "                The number of denoising steps. More denoising steps usually lead to a higher quality image at the\n",
    "                expense of slower inference.\n",
    "            output_type (`str`, *optional*, defaults to `\"pil\"`):\n",
    "                The output format of the generated image. Choose between `PIL.Image` or `np.array`.\n",
    "            return_dict (`bool`, *optional*, defaults to `True`):\n",
    "                Whether or not to return a [`~pipelines.ImagePipelineOutput`] instead of a plain tuple.\n",
    "\n",
    "        Example:\n",
    "\n",
    "        ```py\n",
    "        >>> from diffusers import DDPMPipeline\n",
    "\n",
    "        >>> # load model and scheduler\n",
    "        >>> pipe = DDPMPipeline.from_pretrained(\"google/ddpm-cat-256\")\n",
    "\n",
    "        >>> # run pipeline in inference (sample random noise and denoise)\n",
    "        >>> image = pipe().images[0]\n",
    "\n",
    "        >>> # save image\n",
    "        >>> image.save(\"ddpm_generated_image.png\")\n",
    "        ```\n",
    "\n",
    "        Returns:\n",
    "            [`~pipelines.ImagePipelineOutput`] or `tuple`:\n",
    "                If `return_dict` is `True`, [`~pipelines.ImagePipelineOutput`] is returned, otherwise a `tuple` is\n",
    "                returned where the first element is a list with the generated images\n",
    "        \"\"\"\n",
    "        # Sample gaussian noise to begin loop\n",
    "        if isinstance(self.unet.config.sample_size, int):\n",
    "            image_shape = (\n",
    "                batch_size,\n",
    "                self.unet.config.in_channels,\n",
    "                self.unet.config.sample_size,\n",
    "                self.unet.config.sample_size,\n",
    "            )\n",
    "        else:\n",
    "            image_shape = (\n",
    "                batch_size,\n",
    "                self.unet.config.in_channels,\n",
    "                *self.unet.config.sample_size,\n",
    "            )\n",
    "\n",
    "        if self.device.type == \"mps\":\n",
    "            # randn does not work reproducibly on mps\n",
    "            image = randn_tensor(\n",
    "                image_shape, generator=generator, dtype=self.unet.dtype\n",
    "            )\n",
    "            image = image.to(self.device)\n",
    "        else:\n",
    "            image = randn_tensor(\n",
    "                image_shape,\n",
    "                generator=generator,\n",
    "                device=self.device,\n",
    "                dtype=self.unet.dtype,\n",
    "            )\n",
    "\n",
    "        # set step values\n",
    "        self.scheduler.set_timesteps(num_inference_steps)\n",
    "\n",
    "        for t in self.progress_bar(self.scheduler.timesteps):\n",
    "            # 1. predict noise model_output\n",
    "            model_output = self.unet(image, t).sample\n",
    "\n",
    "            # 2. compute previous image: x_t -> x_t-1\n",
    "            image = self.scheduler.step(\n",
    "                model_output, t, image, generator=generator\n",
    "            ).prev_sample\n",
    "\n",
    "        image = (image / 2 + 0.5).clamp(0, 1)\n",
    "        image = image.cpu().permute(0, 2, 3, 1).numpy()\n",
    "        if output_type == \"pil\":\n",
    "            image = self.numpy_to_pil(image)\n",
    "\n",
    "        if not return_dict:\n",
    "            return (image,)\n",
    "\n",
    "        return ImagePipelineOutput(images=image)\n",
    "\n",
    "\n",
    "class ResnetBlockCondNorm2D(nn.Module):\n",
    "    r\"\"\"\n",
    "    A Resnet block that use normalization layer that incorporate conditioning information.\n",
    "\n",
    "    Parameters:\n",
    "        in_channels (`int`): The number of channels in the input.\n",
    "        out_channels (`int`, *optional*, default to be `None`):\n",
    "            The number of output channels for the first conv2d layer. If None, same as `in_channels`.\n",
    "        dropout (`float`, *optional*, defaults to `0.0`): The dropout probability to use.\n",
    "        temb_channels (`int`, *optional*, default to `512`): the number of channels in timestep embedding.\n",
    "        groups (`int`, *optional*, default to `32`): The number of groups to use for the first normalization layer.\n",
    "        groups_out (`int`, *optional*, default to None):\n",
    "            The number of groups to use for the second normalization layer. if set to None, same as `groups`.\n",
    "        eps (`float`, *optional*, defaults to `1e-6`): The epsilon to use for the normalization.\n",
    "        non_linearity (`str`, *optional*, default to `\"swish\"`): the activation function to use.\n",
    "        time_embedding_norm (`str`, *optional*, default to `\"ada_group\"` ):\n",
    "            The normalization layer for time embedding `temb`. Currently only support \"ada_group\" or \"spatial\".\n",
    "        kernel (`torch.Tensor`, optional, default to None): FIR filter, see\n",
    "            [`~models.resnet.FirUpsample2D`] and [`~models.resnet.FirDownsample2D`].\n",
    "        output_scale_factor (`float`, *optional*, default to be `1.0`): the scale factor to use for the output.\n",
    "        use_in_shortcut (`bool`, *optional*, default to `True`):\n",
    "            If `True`, add a 1x1 nn.conv2d layer for skip-connection.\n",
    "        up (`bool`, *optional*, default to `False`): If `True`, add an upsample layer.\n",
    "        down (`bool`, *optional*, default to `False`): If `True`, add a downsample layer.\n",
    "        conv_shortcut_bias (`bool`, *optional*, default to `True`):  If `True`, adds a learnable bias to the\n",
    "            `conv_shortcut` output.\n",
    "        conv_2d_out_channels (`int`, *optional*, default to `None`): the number of channels in the output.\n",
    "            If None, same as `out_channels`.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(\n",
    "        self,\n",
    "        *,\n",
    "        in_channels: int,\n",
    "        out_channels: Optional[int] = None,\n",
    "        conv_shortcut: bool = False,\n",
    "        dropout: float = 0.0,\n",
    "        temb_channels: int = 512,\n",
    "        groups: int = 32,\n",
    "        groups_out: Optional[int] = None,\n",
    "        eps: float = 1e-6,\n",
    "        non_linearity: str = \"swish\",\n",
    "        time_embedding_norm: str = \"ada_group\",  # ada_group, spatial\n",
    "        output_scale_factor: float = 1.0,\n",
    "        use_in_shortcut: Optional[bool] = None,\n",
    "        up: bool = False,\n",
    "        down: bool = False,\n",
    "        conv_shortcut_bias: bool = True,\n",
    "        conv_2d_out_channels: Optional[int] = None,\n",
    "    ):\n",
    "        super().__init__()\n",
    "        self.in_channels = in_channels\n",
    "        out_channels = in_channels if out_channels is None else out_channels\n",
    "        self.out_channels = out_channels\n",
    "        self.use_conv_shortcut = conv_shortcut\n",
    "        self.up = up\n",
    "        self.down = down\n",
    "        self.output_scale_factor = output_scale_factor\n",
    "        self.time_embedding_norm = time_embedding_norm\n",
    "\n",
    "        if groups_out is None:\n",
    "            groups_out = groups\n",
    "\n",
    "        if self.time_embedding_norm == \"ada_group\":  # ada_group\n",
    "            self.norm1 = AdaGroupNorm(temb_channels, in_channels, groups, eps=eps)\n",
    "        elif self.time_embedding_norm == \"spatial\":\n",
    "            self.norm1 = SpatialNorm(in_channels, temb_channels)\n",
    "        else:\n",
    "            raise ValueError(\n",
    "                f\" unsupported time_embedding_norm: {self.time_embedding_norm}\"\n",
    "            )\n",
    "\n",
    "        self.conv1 = nn.Conv2d(\n",
    "            in_channels, out_channels, kernel_size=3, stride=1, padding=1\n",
    "        )\n",
    "\n",
    "        if self.time_embedding_norm == \"ada_group\":  # ada_group\n",
    "            self.norm2 = AdaGroupNorm(temb_channels, out_channels, groups_out, eps=eps)\n",
    "        elif self.time_embedding_norm == \"spatial\":  # spatial\n",
    "            self.norm2 = SpatialNorm(out_channels, temb_channels)\n",
    "        else:\n",
    "            raise ValueError(\n",
    "                f\" unsupported time_embedding_norm: {self.time_embedding_norm}\"\n",
    "            )\n",
    "\n",
    "        self.dropout = torch.nn.Dropout(dropout)\n",
    "\n",
    "        conv_2d_out_channels = conv_2d_out_channels or out_channels\n",
    "        self.conv2 = nn.Conv2d(\n",
    "            out_channels, conv_2d_out_channels, kernel_size=3, stride=1, padding=1\n",
    "        )\n",
    "\n",
    "        self.nonlinearity = get_activation(non_linearity)\n",
    "\n",
    "        self.upsample = self.downsample = None\n",
    "        if self.up:\n",
    "            self.upsample = Upsample2D(in_channels, use_conv=False)\n",
    "        elif self.down:\n",
    "            self.downsample = Downsample2D(\n",
    "                in_channels, use_conv=False, padding=1, name=\"op\"\n",
    "            )\n",
    "\n",
    "        self.use_in_shortcut = (\n",
    "            self.in_channels != conv_2d_out_channels\n",
    "            if use_in_shortcut is None\n",
    "            else use_in_shortcut\n",
    "        )\n",
    "\n",
    "        self.conv_shortcut = None\n",
    "        if self.use_in_shortcut:\n",
    "            self.conv_shortcut = nn.Conv2d(\n",
    "                in_channels,\n",
    "                conv_2d_out_channels,\n",
    "                kernel_size=1,\n",
    "                stride=1,\n",
    "                padding=0,\n",
    "                bias=conv_shortcut_bias,\n",
    "            )\n",
    "\n",
    "    def forward(\n",
    "        self, input_tensor: torch.Tensor, temb: torch.Tensor, *args, **kwargs\n",
    "    ) -> torch.Tensor:\n",
    "        if len(args) > 0 or kwargs.get(\"scale\", None) is not None:\n",
    "            deprecation_message = \"The `scale` argument is deprecated and will be ignored. Please remove it, as passing it will raise an error in the future. `scale` should directly be passed while calling the underlying pipeline component i.e., via `cross_attention_kwargs`.\"\n",
    "            # deprecate(\"scale\", \"1.0.0\", deprecation_message)\n",
    "\n",
    "        hidden_states = input_tensor\n",
    "\n",
    "        hidden_states = self.norm1(hidden_states, temb)\n",
    "\n",
    "        hidden_states = self.nonlinearity(hidden_states)\n",
    "\n",
    "        if self.upsample is not None:\n",
    "            # upsample_nearest_nhwc fails with large batch sizes. see https://github.com/huggingface/diffusers/issues/984\n",
    "            if hidden_states.shape[0] >= 64:\n",
    "                input_tensor = input_tensor.contiguous()\n",
    "                hidden_states = hidden_states.contiguous()\n",
    "            input_tensor = self.upsample(input_tensor)\n",
    "            hidden_states = self.upsample(hidden_states)\n",
    "\n",
    "        elif self.downsample is not None:\n",
    "            input_tensor = self.downsample(input_tensor)\n",
    "            hidden_states = self.downsample(hidden_states)\n",
    "\n",
    "        hidden_states = self.conv1(hidden_states)\n",
    "\n",
    "        hidden_states = self.norm2(hidden_states, temb)\n",
    "\n",
    "        hidden_states = self.nonlinearity(hidden_states)\n",
    "\n",
    "        hidden_states = self.dropout(hidden_states)\n",
    "        hidden_states = self.conv2(hidden_states)\n",
    "\n",
    "        if self.conv_shortcut is not None:\n",
    "            input_tensor = self.conv_shortcut(input_tensor)\n",
    "\n",
    "        output_tensor = (input_tensor + hidden_states) / self.output_scale_factor\n",
    "\n",
    "        return output_tensor\n",
    "\n",
    "\n",
    "class ResnetBlock2D(nn.Module):\n",
    "    r\"\"\"\n",
    "    A Resnet block.\n",
    "\n",
    "    Parameters:\n",
    "        in_channels (`int`): The number of channels in the input.\n",
    "        out_channels (`int`, *optional*, default to be `None`):\n",
    "            The number of output channels for the first conv2d layer. If None, same as `in_channels`.\n",
    "        dropout (`float`, *optional*, defaults to `0.0`): The dropout probability to use.\n",
    "        temb_channels (`int`, *optional*, default to `512`): the number of channels in timestep embedding.\n",
    "        groups (`int`, *optional*, default to `32`): The number of groups to use for the first normalization layer.\n",
    "        groups_out (`int`, *optional*, default to None):\n",
    "            The number of groups to use for the second normalization layer. if set to None, same as `groups`.\n",
    "        eps (`float`, *optional*, defaults to `1e-6`): The epsilon to use for the normalization.\n",
    "        non_linearity (`str`, *optional*, default to `\"swish\"`): the activation function to use.\n",
    "        time_embedding_norm (`str`, *optional*, default to `\"default\"` ): Time scale shift config.\n",
    "            By default, apply timestep embedding conditioning with a simple shift mechanism. Choose \"scale_shift\" for a\n",
    "            stronger conditioning with scale and shift.\n",
    "        kernel (`torch.Tensor`, optional, default to None): FIR filter, see\n",
    "            [`~models.resnet.FirUpsample2D`] and [`~models.resnet.FirDownsample2D`].\n",
    "        output_scale_factor (`float`, *optional*, default to be `1.0`): the scale factor to use for the output.\n",
    "        use_in_shortcut (`bool`, *optional*, default to `True`):\n",
    "            If `True`, add a 1x1 nn.conv2d layer for skip-connection.\n",
    "        up (`bool`, *optional*, default to `False`): If `True`, add an upsample layer.\n",
    "        down (`bool`, *optional*, default to `False`): If `True`, add a downsample layer.\n",
    "        conv_shortcut_bias (`bool`, *optional*, default to `True`):  If `True`, adds a learnable bias to the\n",
    "            `conv_shortcut` output.\n",
    "        conv_2d_out_channels (`int`, *optional*, default to `None`): the number of channels in the output.\n",
    "            If None, same as `out_channels`.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(\n",
    "        self,\n",
    "        *,\n",
    "        in_channels: int,\n",
    "        out_channels: Optional[int] = None,\n",
    "        conv_shortcut: bool = False,\n",
    "        dropout: float = 0.0,\n",
    "        temb_channels: int = 512,\n",
    "        groups: int = 32,\n",
    "        groups_out: Optional[int] = None,\n",
    "        pre_norm: bool = True,\n",
    "        eps: float = 1e-6,\n",
    "        non_linearity: str = \"swish\",\n",
    "        skip_time_act: bool = False,\n",
    "        time_embedding_norm: str = \"default\",  # default, scale_shift,\n",
    "        kernel: Optional[torch.Tensor] = None,\n",
    "        output_scale_factor: float = 1.0,\n",
    "        use_in_shortcut: Optional[bool] = None,\n",
    "        up: bool = False,\n",
    "        down: bool = False,\n",
    "        conv_shortcut_bias: bool = True,\n",
    "        conv_2d_out_channels: Optional[int] = None,\n",
    "    ):\n",
    "        super().__init__()\n",
    "        if time_embedding_norm == \"ada_group\":\n",
    "            raise ValueError(\n",
    "                \"This class cannot be used with `time_embedding_norm==ada_group`, please use `ResnetBlockCondNorm2D` instead\",\n",
    "            )\n",
    "        if time_embedding_norm == \"spatial\":\n",
    "            raise ValueError(\n",
    "                \"This class cannot be used with `time_embedding_norm==spatial`, please use `ResnetBlockCondNorm2D` instead\",\n",
    "            )\n",
    "\n",
    "        self.pre_norm = True\n",
    "        self.in_channels = in_channels\n",
    "        out_channels = in_channels if out_channels is None else out_channels\n",
    "        self.out_channels = out_channels\n",
    "        self.use_conv_shortcut = conv_shortcut\n",
    "        self.up = up\n",
    "        self.down = down\n",
    "        self.output_scale_factor = output_scale_factor\n",
    "        self.time_embedding_norm = time_embedding_norm\n",
    "        self.skip_time_act = skip_time_act\n",
    "\n",
    "        if groups_out is None:\n",
    "            groups_out = groups\n",
    "\n",
    "        self.norm1 = torch.nn.GroupNorm(\n",
    "            num_groups=groups, num_channels=in_channels, eps=eps, affine=True\n",
    "        )\n",
    "\n",
    "        self.conv1 = nn.Conv2d(\n",
    "            in_channels, out_channels, kernel_size=3, stride=1, padding=1\n",
    "        )\n",
    "\n",
    "        if temb_channels is not None:\n",
    "            if self.time_embedding_norm == \"default\":\n",
    "                self.time_emb_proj = nn.Linear(temb_channels, out_channels)\n",
    "            elif self.time_embedding_norm == \"scale_shift\":\n",
    "                self.time_emb_proj = nn.Linear(temb_channels, 2 * out_channels)\n",
    "            else:\n",
    "                raise ValueError(\n",
    "                    f\"unknown time_embedding_norm : {self.time_embedding_norm} \"\n",
    "                )\n",
    "        else:\n",
    "            self.time_emb_proj = None\n",
    "\n",
    "        self.norm2 = torch.nn.GroupNorm(\n",
    "            num_groups=groups_out, num_channels=out_channels, eps=eps, affine=True\n",
    "        )\n",
    "\n",
    "        self.dropout = torch.nn.Dropout(dropout)\n",
    "        conv_2d_out_channels = conv_2d_out_channels or out_channels\n",
    "        self.conv2 = nn.Conv2d(\n",
    "            out_channels, conv_2d_out_channels, kernel_size=3, stride=1, padding=1\n",
    "        )\n",
    "\n",
    "        self.nonlinearity = get_activation(non_linearity)\n",
    "\n",
    "        self.upsample = self.downsample = None\n",
    "        if self.up:\n",
    "            if kernel == \"fir\":\n",
    "                fir_kernel = (1, 3, 3, 1)\n",
    "                self.upsample = lambda x: upsample_2d(x, kernel=fir_kernel)\n",
    "            elif kernel == \"sde_vp\":\n",
    "                self.upsample = partial(F.interpolate, scale_factor=2.0, mode=\"nearest\")\n",
    "            else:\n",
    "                self.upsample = Upsample2D(in_channels, use_conv=False)\n",
    "        elif self.down:\n",
    "            if kernel == \"fir\":\n",
    "                fir_kernel = (1, 3, 3, 1)\n",
    "                self.downsample = lambda x: downsample_2d(x, kernel=fir_kernel)\n",
    "            elif kernel == \"sde_vp\":\n",
    "                self.downsample = partial(F.avg_pool2d, kernel_size=2, stride=2)\n",
    "            else:\n",
    "                self.downsample = Downsample2D(\n",
    "                    in_channels, use_conv=False, padding=1, name=\"op\"\n",
    "                )\n",
    "\n",
    "        self.use_in_shortcut = (\n",
    "            self.in_channels != conv_2d_out_channels\n",
    "            if use_in_shortcut is None\n",
    "            else use_in_shortcut\n",
    "        )\n",
    "\n",
    "        self.conv_shortcut = None\n",
    "        if self.use_in_shortcut:\n",
    "            self.conv_shortcut = nn.Conv2d(\n",
    "                in_channels,\n",
    "                conv_2d_out_channels,\n",
    "                kernel_size=1,\n",
    "                stride=1,\n",
    "                padding=0,\n",
    "                bias=conv_shortcut_bias,\n",
    "            )\n",
    "\n",
    "    def forward(\n",
    "        self, input_tensor: torch.Tensor, temb: torch.Tensor, *args, **kwargs\n",
    "    ) -> torch.Tensor:\n",
    "        if len(args) > 0 or kwargs.get(\"scale\", None) is not None:\n",
    "            deprecation_message = \"The `scale` argument is deprecated and will be ignored. Please remove it, as passing it will raise an error in the future. `scale` should directly be passed while calling the underlying pipeline component i.e., via `cross_attention_kwargs`.\"\n",
    "            deprecate(\"scale\", \"1.0.0\", deprecation_message)\n",
    "\n",
    "        hidden_states = input_tensor\n",
    "\n",
    "        hidden_states = self.norm1(hidden_states)\n",
    "        hidden_states = self.nonlinearity(hidden_states)\n",
    "\n",
    "        if self.upsample is not None:\n",
    "            # upsample_nearest_nhwc fails with large batch sizes. see https://github.com/huggingface/diffusers/issues/984\n",
    "            if hidden_states.shape[0] >= 64:\n",
    "                input_tensor = input_tensor.contiguous()\n",
    "                hidden_states = hidden_states.contiguous()\n",
    "            input_tensor = self.upsample(input_tensor)\n",
    "            hidden_states = self.upsample(hidden_states)\n",
    "        elif self.downsample is not None:\n",
    "            input_tensor = self.downsample(input_tensor)\n",
    "            hidden_states = self.downsample(hidden_states)\n",
    "\n",
    "        hidden_states = self.conv1(hidden_states)\n",
    "\n",
    "        if self.time_emb_proj is not None:\n",
    "            if not self.skip_time_act:\n",
    "                temb = self.nonlinearity(temb)\n",
    "            temb = self.time_emb_proj(temb)[:, :, None, None]\n",
    "\n",
    "        if self.time_embedding_norm == \"default\":\n",
    "            if temb is not None:\n",
    "                hidden_states = hidden_states + temb\n",
    "            hidden_states = self.norm2(hidden_states)\n",
    "        elif self.time_embedding_norm == \"scale_shift\":\n",
    "            if temb is None:\n",
    "                raise ValueError(\n",
    "                    f\" `temb` should not be None when `time_embedding_norm` is {self.time_embedding_norm}\"\n",
    "                )\n",
    "            time_scale, time_shift = torch.chunk(temb, 2, dim=1)\n",
    "            hidden_states = self.norm2(hidden_states)\n",
    "            hidden_states = hidden_states * (1 + time_scale) + time_shift\n",
    "        else:\n",
    "            hidden_states = self.norm2(hidden_states)\n",
    "\n",
    "        hidden_states = self.nonlinearity(hidden_states)\n",
    "\n",
    "        hidden_states = self.dropout(hidden_states)\n",
    "        hidden_states = self.conv2(hidden_states)\n",
    "\n",
    "        if self.conv_shortcut is not None:\n",
    "            input_tensor = self.conv_shortcut(input_tensor.contiguous())\n",
    "\n",
    "        output_tensor = (input_tensor + hidden_states) / self.output_scale_factor\n",
    "\n",
    "        return output_tensor\n",
    "\n",
    "\n",
    "class UNetMidBlock2D(nn.Module):\n",
    "    \"\"\"\n",
    "    A 2D UNet mid-block [`UNetMidBlock2D`] with multiple residual blocks and optional attention blocks.\n",
    "\n",
    "    Args:\n",
    "        in_channels (`int`): The number of input channels.\n",
    "        temb_channels (`int`): The number of temporal embedding channels.\n",
    "        dropout (`float`, *optional*, defaults to 0.0): The dropout rate.\n",
    "        num_layers (`int`, *optional*, defaults to 1): The number of residual blocks.\n",
    "        resnet_eps (`float`, *optional*, 1e-6 ): The epsilon value for the resnet blocks.\n",
    "        resnet_time_scale_shift (`str`, *optional*, defaults to `default`):\n",
    "            The type of normalization to apply to the time embeddings. This can help to improve the performance of the\n",
    "            model on tasks with long-range temporal dependencies.\n",
    "        resnet_act_fn (`str`, *optional*, defaults to `swish`): The activation function for the resnet blocks.\n",
    "        resnet_groups (`int`, *optional*, defaults to 32):\n",
    "            The number of groups to use in the group normalization layers of the resnet blocks.\n",
    "        attn_groups (`Optional[int]`, *optional*, defaults to None): The number of groups for the attention blocks.\n",
    "        resnet_pre_norm (`bool`, *optional*, defaults to `True`):\n",
    "            Whether to use pre-normalization for the resnet blocks.\n",
    "        add_attention (`bool`, *optional*, defaults to `True`): Whether to add attention blocks.\n",
    "        attention_head_dim (`int`, *optional*, defaults to 1):\n",
    "            Dimension of a single attention head. The number of attention heads is determined based on this value and\n",
    "            the number of input channels.\n",
    "        output_scale_factor (`float`, *optional*, defaults to 1.0): The output scale factor.\n",
    "\n",
    "    Returns:\n",
    "        `torch.Tensor`: The output of the last residual block, which is a tensor of shape `(batch_size, in_channels,\n",
    "        height, width)`.\n",
    "\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(\n",
    "        self,\n",
    "        in_channels: int,\n",
    "        temb_channels: int,\n",
    "        dropout: float = 0.0,\n",
    "        num_layers: int = 1,\n",
    "        resnet_eps: float = 1e-6,\n",
    "        resnet_time_scale_shift: str = \"default\",  # default, spatial\n",
    "        resnet_act_fn: str = \"swish\",\n",
    "        resnet_groups: int = 32,\n",
    "        attn_groups: Optional[int] = None,\n",
    "        resnet_pre_norm: bool = True,\n",
    "        add_attention: bool = True,\n",
    "        attention_head_dim: int = 1,\n",
    "        output_scale_factor: float = 1.0,\n",
    "    ):\n",
    "        super().__init__()\n",
    "        resnet_groups = (\n",
    "            resnet_groups if resnet_groups is not None else min(in_channels // 4, 32)\n",
    "        )\n",
    "        self.add_attention = add_attention\n",
    "\n",
    "        if attn_groups is None:\n",
    "            attn_groups = (\n",
    "                resnet_groups if resnet_time_scale_shift == \"default\" else None\n",
    "            )\n",
    "\n",
    "        # there is always at least one resnet\n",
    "        if resnet_time_scale_shift == \"spatial\":\n",
    "            resnets = [\n",
    "                ResnetBlockCondNorm2D(\n",
    "                    in_channels=in_channels,\n",
    "                    out_channels=in_channels,\n",
    "                    temb_channels=temb_channels,\n",
    "                    eps=resnet_eps,\n",
    "                    groups=resnet_groups,\n",
    "                    dropout=dropout,\n",
    "                    time_embedding_norm=\"spatial\",\n",
    "                    non_linearity=resnet_act_fn,\n",
    "                    output_scale_factor=output_scale_factor,\n",
    "                )\n",
    "            ]\n",
    "        else:\n",
    "            resnets = [\n",
    "                ResnetBlock2D(\n",
    "                    in_channels=in_channels,\n",
    "                    out_channels=in_channels,\n",
    "                    temb_channels=temb_channels,\n",
    "                    eps=resnet_eps,\n",
    "                    groups=resnet_groups,\n",
    "                    dropout=dropout,\n",
    "                    time_embedding_norm=resnet_time_scale_shift,\n",
    "                    non_linearity=resnet_act_fn,\n",
    "                    output_scale_factor=output_scale_factor,\n",
    "                    pre_norm=resnet_pre_norm,\n",
    "                )\n",
    "            ]\n",
    "        attentions = []\n",
    "\n",
    "        if attention_head_dim is None:\n",
    "            logger.warning(\n",
    "                f\"It is not recommend to pass `attention_head_dim=None`. Defaulting `attention_head_dim` to `in_channels`: {in_channels}.\"\n",
    "            )\n",
    "            attention_head_dim = in_channels\n",
    "\n",
    "        for _ in range(num_layers):\n",
    "            if self.add_attention:\n",
    "                attentions.append(\n",
    "                    Attention(\n",
    "                        in_channels,\n",
    "                        heads=in_channels // attention_head_dim,\n",
    "                        dim_head=attention_head_dim,\n",
    "                        rescale_output_factor=output_scale_factor,\n",
    "                        eps=resnet_eps,\n",
    "                        norm_num_groups=attn_groups,\n",
    "                        spatial_norm_dim=(\n",
    "                            temb_channels\n",
    "                            if resnet_time_scale_shift == \"spatial\"\n",
    "                            else None\n",
    "                        ),\n",
    "                        residual_connection=True,\n",
    "                        bias=True,\n",
    "                        upcast_softmax=True,\n",
    "                        _from_deprecated_attn_block=True,\n",
    "                    )\n",
    "                )\n",
    "            else:\n",
    "                attentions.append(None)\n",
    "\n",
    "            if resnet_time_scale_shift == \"spatial\":\n",
    "                resnets.append(\n",
    "                    ResnetBlockCondNorm2D(\n",
    "                        in_channels=in_channels,\n",
    "                        out_channels=in_channels,\n",
    "                        temb_channels=temb_channels,\n",
    "                        eps=resnet_eps,\n",
    "                        groups=resnet_groups,\n",
    "                        dropout=dropout,\n",
    "                        time_embedding_norm=\"spatial\",\n",
    "                        non_linearity=resnet_act_fn,\n",
    "                        output_scale_factor=output_scale_factor,\n",
    "                    )\n",
    "                )\n",
    "            else:\n",
    "                resnets.append(\n",
    "                    ResnetBlock2D(\n",
    "                        in_channels=in_channels,\n",
    "                        out_channels=in_channels,\n",
    "                        temb_channels=temb_channels,\n",
    "                        eps=resnet_eps,\n",
    "                        groups=resnet_groups,\n",
    "                        dropout=dropout,\n",
    "                        time_embedding_norm=resnet_time_scale_shift,\n",
    "                        non_linearity=resnet_act_fn,\n",
    "                        output_scale_factor=output_scale_factor,\n",
    "                        pre_norm=resnet_pre_norm,\n",
    "                    )\n",
    "                )\n",
    "\n",
    "        self.attentions = nn.ModuleList(attentions)\n",
    "        self.resnets = nn.ModuleList(resnets)\n",
    "\n",
    "        self.gradient_checkpointing = False\n",
    "\n",
    "    def forward(\n",
    "        self, hidden_states: torch.Tensor, temb: Optional[torch.Tensor] = None\n",
    "    ) -> torch.Tensor:\n",
    "        hidden_states = self.resnets[0](hidden_states, temb)\n",
    "        for attn, resnet in zip(self.attentions, self.resnets[1:]):\n",
    "            if torch.is_grad_enabled() and self.gradient_checkpointing:\n",
    "                if attn is not None:\n",
    "                    hidden_states = attn(hidden_states, temb=temb)\n",
    "                hidden_states = self._gradient_checkpointing_func(\n",
    "                    resnet, hidden_states, temb\n",
    "                )\n",
    "            else:\n",
    "                if attn is not None:\n",
    "                    hidden_states = attn(hidden_states, temb=temb)\n",
    "                hidden_states = resnet(hidden_states, temb)\n",
    "\n",
    "        return hidden_states\n",
    "\n",
    "\n",
    "class UNet2DModel(ModelMixin, ConfigMixin):\n",
    "    r\"\"\"\n",
    "    A 2D UNet model that takes a noisy sample and a timestep and returns a sample shaped output.\n",
    "\n",
    "    This model inherits from [`ModelMixin`]. Check the superclass documentation for it's generic methods implemented\n",
    "    for all models (such as downloading or saving).\n",
    "\n",
    "    Parameters:\n",
    "        sample_size (`int` or `Tuple[int, int]`, *optional*, defaults to `None`):\n",
    "            Height and width of input/output sample. Dimensions must be a multiple of `2 ** (len(block_out_channels) -\n",
    "            1)`.\n",
    "        in_channels (`int`, *optional*, defaults to 3): Number of channels in the input sample.\n",
    "        out_channels (`int`, *optional*, defaults to 3): Number of channels in the output.\n",
    "        center_input_sample (`bool`, *optional*, defaults to `False`): Whether to center the input sample.\n",
    "        time_embedding_type (`str`, *optional*, defaults to `\"positional\"`): Type of time embedding to use.\n",
    "        freq_shift (`int`, *optional*, defaults to 0): Frequency shift for Fourier time embedding.\n",
    "        flip_sin_to_cos (`bool`, *optional*, defaults to `True`):\n",
    "            Whether to flip sin to cos for Fourier time embedding.\n",
    "        down_block_types (`Tuple[str]`, *optional*, defaults to `(\"DownBlock2D\", \"AttnDownBlock2D\", \"AttnDownBlock2D\", \"AttnDownBlock2D\")`):\n",
    "            Tuple of downsample block types.\n",
    "        mid_block_type (`str`, *optional*, defaults to `\"UNetMidBlock2D\"`):\n",
    "            Block type for middle of UNet, it can be either `UNetMidBlock2D` or `None`.\n",
    "        up_block_types (`Tuple[str]`, *optional*, defaults to `(\"AttnUpBlock2D\", \"AttnUpBlock2D\", \"AttnUpBlock2D\", \"UpBlock2D\")`):\n",
    "            Tuple of upsample block types.\n",
    "        block_out_channels (`Tuple[int]`, *optional*, defaults to `(224, 448, 672, 896)`):\n",
    "            Tuple of block output channels.\n",
    "        layers_per_block (`int`, *optional*, defaults to `2`): The number of layers per block.\n",
    "        mid_block_scale_factor (`float`, *optional*, defaults to `1`): The scale factor for the mid block.\n",
    "        downsample_padding (`int`, *optional*, defaults to `1`): The padding for the downsample convolution.\n",
    "        downsample_type (`str`, *optional*, defaults to `conv`):\n",
    "            The downsample type for downsampling layers. Choose between \"conv\" and \"resnet\"\n",
    "        upsample_type (`str`, *optional*, defaults to `conv`):\n",
    "            The upsample type for upsampling layers. Choose between \"conv\" and \"resnet\"\n",
    "        dropout (`float`, *optional*, defaults to 0.0): The dropout probability to use.\n",
    "        act_fn (`str`, *optional*, defaults to `\"silu\"`): The activation function to use.\n",
    "        attention_head_dim (`int`, *optional*, defaults to `8`): The attention head dimension.\n",
    "        norm_num_groups (`int`, *optional*, defaults to `32`): The number of groups for normalization.\n",
    "        attn_norm_num_groups (`int`, *optional*, defaults to `None`):\n",
    "            If set to an integer, a group norm layer will be created in the mid block's [`Attention`] layer with the\n",
    "            given number of groups. If left as `None`, the group norm layer will only be created if\n",
    "            `resnet_time_scale_shift` is set to `default`, and if created will have `norm_num_groups` groups.\n",
    "        norm_eps (`float`, *optional*, defaults to `1e-5`): The epsilon for normalization.\n",
    "        resnet_time_scale_shift (`str`, *optional*, defaults to `\"default\"`): Time scale shift config\n",
    "            for ResNet blocks (see [`~models.resnet.ResnetBlock2D`]). Choose from `default` or `scale_shift`.\n",
    "        class_embed_type (`str`, *optional*, defaults to `None`):\n",
    "            The type of class embedding to use which is ultimately summed with the time embeddings. Choose from `None`,\n",
    "            `\"timestep\"`, or `\"identity\"`.\n",
    "        num_class_embeds (`int`, *optional*, defaults to `None`):\n",
    "            Input dimension of the learnable embedding matrix to be projected to `time_embed_dim` when performing class\n",
    "            conditioning with `class_embed_type` equal to `None`.\n",
    "    \"\"\"\n",
    "\n",
    "    _supports_gradient_checkpointing = True\n",
    "    _skip_layerwise_casting_patterns = [\"norm\"]\n",
    "\n",
    "    @register_to_config\n",
    "    def __init__(\n",
    "        self,\n",
    "        sample_size: Optional[Union[int, Tuple[int, int]]] = None,\n",
    "        in_channels: int = 3,\n",
    "        out_channels: int = 3,\n",
    "        center_input_sample: bool = False,\n",
    "        time_embedding_type: str = \"positional\",\n",
    "        time_embedding_dim: Optional[int] = None,\n",
    "        freq_shift: int = 0,\n",
    "        flip_sin_to_cos: bool = True,\n",
    "        down_block_types: Tuple[str, ...] = (\n",
    "            \"DownBlock2D\",\n",
    "            \"AttnDownBlock2D\",\n",
    "            \"AttnDownBlock2D\",\n",
    "            \"AttnDownBlock2D\",\n",
    "        ),\n",
    "        mid_block_type: Optional[str] = \"UNetMidBlock2D\",\n",
    "        up_block_types: Tuple[str, ...] = (\n",
    "            \"AttnUpBlock2D\",\n",
    "            \"AttnUpBlock2D\",\n",
    "            \"AttnUpBlock2D\",\n",
    "            \"UpBlock2D\",\n",
    "        ),\n",
    "        block_out_channels: Tuple[int, ...] = (224, 448, 672, 896),\n",
    "        layers_per_block: int = 2,\n",
    "        mid_block_scale_factor: float = 1,\n",
    "        downsample_padding: int = 1,\n",
    "        downsample_type: str = \"conv\",\n",
    "        upsample_type: str = \"conv\",\n",
    "        dropout: float = 0.0,\n",
    "        act_fn: str = \"silu\",\n",
    "        attention_head_dim: Optional[int] = 8,\n",
    "        norm_num_groups: int = 32,\n",
    "        attn_norm_num_groups: Optional[int] = None,\n",
    "        norm_eps: float = 1e-5,\n",
    "        resnet_time_scale_shift: str = \"default\",\n",
    "        add_attention: bool = True,\n",
    "        class_embed_type: Optional[str] = None,\n",
    "        num_class_embeds: Optional[int] = None,\n",
    "        num_train_timesteps: Optional[int] = None,\n",
    "    ):\n",
    "        super().__init__()\n",
    "\n",
    "        self.sample_size = sample_size\n",
    "        time_embed_dim = time_embedding_dim or block_out_channels[0] * 4\n",
    "\n",
    "        # Check inputs\n",
    "        if len(down_block_types) != len(up_block_types):\n",
    "            raise ValueError(\n",
    "                f\"Must provide the same number of `down_block_types` as `up_block_types`. `down_block_types`: {down_block_types}. `up_block_types`: {up_block_types}.\"\n",
    "            )\n",
    "\n",
    "        if len(block_out_channels) != len(down_block_types):\n",
    "            raise ValueError(\n",
    "                f\"Must provide the same number of `block_out_channels` as `down_block_types`. `block_out_channels`: {block_out_channels}. `down_block_types`: {down_block_types}.\"\n",
    "            )\n",
    "\n",
    "        # input\n",
    "        self.conv_in = nn.Conv2d(\n",
    "            in_channels, block_out_channels[0], kernel_size=3, padding=(1, 1)\n",
    "        )\n",
    "\n",
    "        # time\n",
    "        if time_embedding_type == \"fourier\":\n",
    "            self.time_proj = GaussianFourierProjection(\n",
    "                embedding_size=block_out_channels[0], scale=16\n",
    "            )\n",
    "            timestep_input_dim = 2 * block_out_channels[0]\n",
    "        elif time_embedding_type == \"positional\":\n",
    "            self.time_proj = Timesteps(\n",
    "                block_out_channels[0], flip_sin_to_cos, freq_shift\n",
    "            )\n",
    "            timestep_input_dim = block_out_channels[0]\n",
    "        elif time_embedding_type == \"learned\":\n",
    "            self.time_proj = nn.Embedding(num_train_timesteps, block_out_channels[0])\n",
    "            timestep_input_dim = block_out_channels[0]\n",
    "\n",
    "        self.time_embedding = TimestepEmbedding(timestep_input_dim, time_embed_dim)\n",
    "\n",
    "        # class embedding\n",
    "        if class_embed_type is None and num_class_embeds is not None:\n",
    "            self.class_embedding = nn.Embedding(num_class_embeds, time_embed_dim)\n",
    "        elif class_embed_type == \"timestep\":\n",
    "            self.class_embedding = TimestepEmbedding(timestep_input_dim, time_embed_dim)\n",
    "        elif class_embed_type == \"identity\":\n",
    "            self.class_embedding = nn.Identity(time_embed_dim, time_embed_dim)\n",
    "        else:\n",
    "            self.class_embedding = None\n",
    "\n",
    "        self.down_blocks = nn.ModuleList([])\n",
    "        self.mid_block = None\n",
    "        self.up_blocks = nn.ModuleList([])\n",
    "\n",
    "        # down\n",
    "        output_channel = block_out_channels[0]\n",
    "        for i, down_block_type in enumerate(down_block_types):\n",
    "            input_channel = output_channel\n",
    "            output_channel = block_out_channels[i]\n",
    "            is_final_block = i == len(block_out_channels) - 1\n",
    "\n",
    "            down_block = get_down_block(\n",
    "                down_block_type,\n",
    "                num_layers=layers_per_block,\n",
    "                in_channels=input_channel,\n",
    "                out_channels=output_channel,\n",
    "                temb_channels=time_embed_dim,\n",
    "                add_downsample=not is_final_block,\n",
    "                resnet_eps=norm_eps,\n",
    "                resnet_act_fn=act_fn,\n",
    "                resnet_groups=norm_num_groups,\n",
    "                attention_head_dim=(\n",
    "                    attention_head_dim\n",
    "                    if attention_head_dim is not None\n",
    "                    else output_channel\n",
    "                ),\n",
    "                downsample_padding=downsample_padding,\n",
    "                resnet_time_scale_shift=resnet_time_scale_shift,\n",
    "                downsample_type=downsample_type,\n",
    "                dropout=dropout,\n",
    "            )\n",
    "            self.down_blocks.append(down_block)\n",
    "\n",
    "        # mid\n",
    "        if mid_block_type is None:\n",
    "            self.mid_block = None\n",
    "        else:\n",
    "            self.mid_block = UNetMidBlock2D(\n",
    "                in_channels=block_out_channels[-1],\n",
    "                temb_channels=time_embed_dim,\n",
    "                dropout=dropout,\n",
    "                resnet_eps=norm_eps,\n",
    "                resnet_act_fn=act_fn,\n",
    "                output_scale_factor=mid_block_scale_factor,\n",
    "                resnet_time_scale_shift=resnet_time_scale_shift,\n",
    "                attention_head_dim=(\n",
    "                    attention_head_dim\n",
    "                    if attention_head_dim is not None\n",
    "                    else block_out_channels[-1]\n",
    "                ),\n",
    "                resnet_groups=norm_num_groups,\n",
    "                attn_groups=attn_norm_num_groups,\n",
    "                add_attention=add_attention,\n",
    "            )\n",
    "\n",
    "        # up\n",
    "        reversed_block_out_channels = list(reversed(block_out_channels))\n",
    "        output_channel = reversed_block_out_channels[0]\n",
    "        for i, up_block_type in enumerate(up_block_types):\n",
    "            prev_output_channel = output_channel\n",
    "            output_channel = reversed_block_out_channels[i]\n",
    "            input_channel = reversed_block_out_channels[\n",
    "                min(i + 1, len(block_out_channels) - 1)\n",
    "            ]\n",
    "\n",
    "            is_final_block = i == len(block_out_channels) - 1\n",
    "\n",
    "            up_block = get_up_block(\n",
    "                up_block_type,\n",
    "                num_layers=layers_per_block + 1,\n",
    "                in_channels=input_channel,\n",
    "                out_channels=output_channel,\n",
    "                prev_output_channel=prev_output_channel,\n",
    "                temb_channels=time_embed_dim,\n",
    "                add_upsample=not is_final_block,\n",
    "                resnet_eps=norm_eps,\n",
    "                resnet_act_fn=act_fn,\n",
    "                resnet_groups=norm_num_groups,\n",
    "                attention_head_dim=(\n",
    "                    attention_head_dim\n",
    "                    if attention_head_dim is not None\n",
    "                    else output_channel\n",
    "                ),\n",
    "                resnet_time_scale_shift=resnet_time_scale_shift,\n",
    "                upsample_type=upsample_type,\n",
    "                dropout=dropout,\n",
    "            )\n",
    "            self.up_blocks.append(up_block)\n",
    "\n",
    "        # out\n",
    "        num_groups_out = (\n",
    "            norm_num_groups\n",
    "            if norm_num_groups is not None\n",
    "            else min(block_out_channels[0] // 4, 32)\n",
    "        )\n",
    "        self.conv_norm_out = nn.GroupNorm(\n",
    "            num_channels=block_out_channels[0], num_groups=num_groups_out, eps=norm_eps\n",
    "        )\n",
    "        self.conv_act = nn.SiLU()\n",
    "        self.conv_out = nn.Conv2d(\n",
    "            block_out_channels[0], out_channels, kernel_size=3, padding=1\n",
    "        )\n",
    "\n",
    "    def forward(\n",
    "        self,\n",
    "        sample: torch.Tensor,\n",
    "        timestep: Union[torch.Tensor, float, int],\n",
    "        class_labels: Optional[torch.Tensor] = None,\n",
    "        return_dict: bool = True,\n",
    "    ) -> Union[UNet2DOutput, Tuple]:\n",
    "        r\"\"\"\n",
    "        The [`UNet2DModel`] forward method.\n",
    "\n",
    "        Args:\n",
    "            sample (`torch.Tensor`):\n",
    "                The noisy input tensor with the following shape `(batch, channel, height, width)`.\n",
    "            timestep (`torch.Tensor` or `float` or `int`): The number of timesteps to denoise an input.\n",
    "            class_labels (`torch.Tensor`, *optional*, defaults to `None`):\n",
    "                Optional class labels for conditioning. Their embeddings will be summed with the timestep embeddings.\n",
    "            return_dict (`bool`, *optional*, defaults to `True`):\n",
    "                Whether or not to return a [`~models.unets.unet_2d.UNet2DOutput`] instead of a plain tuple.\n",
    "\n",
    "        Returns:\n",
    "            [`~models.unets.unet_2d.UNet2DOutput`] or `tuple`:\n",
    "                If `return_dict` is True, an [`~models.unets.unet_2d.UNet2DOutput`] is returned, otherwise a `tuple` is\n",
    "                returned where the first element is the sample tensor.\n",
    "        \"\"\"\n",
    "        # 0. center input if necessary\n",
    "        if self.config.center_input_sample:\n",
    "            sample = 2 * sample - 1.0\n",
    "\n",
    "        # 1. time\n",
    "        timesteps = timestep\n",
    "        if not torch.is_tensor(timesteps):\n",
    "            timesteps = torch.tensor(\n",
    "                [timesteps], dtype=torch.long, device=sample.device\n",
    "            )\n",
    "        elif torch.is_tensor(timesteps) and len(timesteps.shape) == 0:\n",
    "            timesteps = timesteps[None].to(sample.device)\n",
    "\n",
    "        # broadcast to batch dimension in a way that's compatible with ONNX/Core ML\n",
    "        timesteps = timesteps * torch.ones(\n",
    "            sample.shape[0], dtype=timesteps.dtype, device=timesteps.device\n",
    "        )\n",
    "\n",
    "        t_emb = self.time_proj(timesteps)\n",
    "\n",
    "        # timesteps does not contain any weights and will always return f32 tensors\n",
    "        # but time_embedding might actually be running in fp16. so we need to cast here.\n",
    "        # there might be better ways to encapsulate this.\n",
    "        t_emb = t_emb.to(dtype=self.dtype)\n",
    "        emb = self.time_embedding(t_emb)\n",
    "\n",
    "        if self.class_embedding is not None:\n",
    "            if class_labels is None:\n",
    "                raise ValueError(\n",
    "                    \"class_labels should be provided when doing class conditioning\"\n",
    "                )\n",
    "\n",
    "            if self.config.class_embed_type == \"timestep\":\n",
    "                class_labels = self.time_proj(class_labels)\n",
    "\n",
    "            class_emb = self.class_embedding(class_labels).to(dtype=self.dtype)\n",
    "            emb = emb + class_emb\n",
    "        elif self.class_embedding is None and class_labels is not None:\n",
    "            raise ValueError(\n",
    "                \"class_embedding needs to be initialized in order to use class conditioning\"\n",
    "            )\n",
    "\n",
    "        # 2. pre-process\n",
    "        skip_sample = sample\n",
    "        sample = self.conv_in(sample)\n",
    "\n",
    "        # 3. down\n",
    "        down_block_res_samples = (sample,)\n",
    "        for downsample_block in self.down_blocks:\n",
    "            if hasattr(downsample_block, \"skip_conv\"):\n",
    "                sample, res_samples, skip_sample = downsample_block(\n",
    "                    hidden_states=sample, temb=emb, skip_sample=skip_sample\n",
    "                )\n",
    "            else:\n",
    "                sample, res_samples = downsample_block(hidden_states=sample, temb=emb)\n",
    "\n",
    "            down_block_res_samples += res_samples\n",
    "\n",
    "        # 4. mid\n",
    "        if self.mid_block is not None:\n",
    "            sample = self.mid_block(sample, emb)\n",
    "\n",
    "        # 5. up\n",
    "        skip_sample = None\n",
    "        for upsample_block in self.up_blocks:\n",
    "            res_samples = down_block_res_samples[-len(upsample_block.resnets) :]\n",
    "            down_block_res_samples = down_block_res_samples[\n",
    "                : -len(upsample_block.resnets)\n",
    "            ]\n",
    "\n",
    "            if hasattr(upsample_block, \"skip_conv\"):\n",
    "                sample, skip_sample = upsample_block(\n",
    "                    sample, res_samples, emb, skip_sample\n",
    "                )\n",
    "            else:\n",
    "                sample = upsample_block(sample, res_samples, emb)\n",
    "\n",
    "        # 6. post-process\n",
    "        sample = self.conv_norm_out(sample)\n",
    "        sample = self.conv_act(sample)\n",
    "        sample = self.conv_out(sample)\n",
    "\n",
    "        if skip_sample is not None:\n",
    "            sample += skip_sample\n",
    "\n",
    "        if self.config.time_embedding_type == \"fourier\":\n",
    "            timesteps = timesteps.reshape(\n",
    "                (sample.shape[0], *([1] * len(sample.shape[1:])))\n",
    "            )\n",
    "            sample = sample / timesteps\n",
    "\n",
    "        if not return_dict:\n",
    "            return (sample,)\n",
    "\n",
    "        return UNet2DOutput(sample=sample)\n",
    "\n",
    "\n",
    "@dataclass\n",
    "class Args:\n",
    "    resolution: int = 64\n",
    "    ddpm_num_steps: int = 1000\n",
    "    ddpm_beta_schedule: str = \"linear\"\n",
    "    learning_rate: float = 1e-4\n",
    "    adam_beta1: float = 0.95\n",
    "    adam_beta2: float = 0.999\n",
    "    adam_weight_decay: float = 1e-6\n",
    "    adam_epsilon: float = 1e-08\n",
    "    dataset_name: str = \"huggan/flowers-102-categories\"\n",
    "    dataset_config_name: str = None\n",
    "    cache_dir: str = None\n",
    "    ema_max_decay: float = 0.9999\n",
    "    ema_inv_gamma: float = 1.0\n",
    "    ema_power: float = 3 / 4\n",
    "    train_batch_size: int = 16\n",
    "    eval_batch_size: int = 16\n",
    "    gradient_accumulation_steps: int = 1\n",
    "    lr_scheduler: str = \"cosine\"\n",
    "    lr_warmup_steps: int = 500\n",
    "    num_epochs: int = 1\n",
    "    center_crop: bool = False\n",
    "    random_flip: bool = False\n",
    "    dataloader_num_workers: int = 0\n",
    "    use_ema: bool = True\n",
    "    checkpointing_steps: int = 500\n",
    "    checkpoints_total_limit: int = None\n",
    "    output_dir: str = \"minimal_diffusion/unconditional_diffusion/ddpm-model-64\"\n",
    "    save_images_epochs: int = 10\n",
    "    save_model_epochs: int = 1\n",
    "    prediction_type: str = \"epsilon\"\n",
    "    logging_dir: str = \"logs\"\n",
    "    mixed_precision: str = \"no\"\n",
    "    logger: str = \"wandb\"\n",
    "    ddpm_num_inference_steps: int = 1000\n",
    "    project_name: str = \"train_unconditional\"\n",
    "\n",
    "\n",
    "args = Args()\n",
    "\n",
    "model = UNet2DModel(\n",
    "    sample_size=args.resolution,\n",
    "    in_channels=3,\n",
    "    out_channels=3,\n",
    "    layers_per_block=2,\n",
    "    block_out_channels=(128, 128, 256, 256, 512, 512),\n",
    "    down_block_types=(\n",
    "        \"DownBlock2D\",\n",
    "        \"DownBlock2D\",\n",
    "        \"DownBlock2D\",\n",
    "        \"DownBlock2D\",\n",
    "        \"AttnDownBlock2D\",\n",
    "        \"DownBlock2D\",\n",
    "    ),\n",
    "    up_block_types=(\n",
    "        \"UpBlock2D\",\n",
    "        \"AttnUpBlock2D\",\n",
    "        \"UpBlock2D\",\n",
    "        \"UpBlock2D\",\n",
    "        \"UpBlock2D\",\n",
    "        \"UpBlock2D\",\n",
    "    ),\n",
    ")\n",
    "model = model.cuda()\n",
    "ema_model = EMAModel(\n",
    "    model.parameters(),\n",
    "    decay=args.ema_max_decay,\n",
    "    use_ema_warmup=True,\n",
    "    inv_gamma=args.ema_inv_gamma,\n",
    "    power=args.ema_power,\n",
    "    model_cls=UNet2DModel,\n",
    "    model_config=model.config,\n",
    ")\n",
    "\n",
    "noise_scheduler = DDPMScheduler(\n",
    "    num_train_timesteps=args.ddpm_num_steps,\n",
    "    beta_schedule=args.ddpm_beta_schedule,\n",
    ")\n",
    "\n",
    "optimizer = torch.optim.AdamW(\n",
    "    model.parameters(),\n",
    "    lr=args.learning_rate,\n",
    "    betas=(args.adam_beta1, args.adam_beta2),\n",
    "    weight_decay=args.adam_weight_decay,\n",
    "    eps=args.adam_epsilon,\n",
    ")\n",
    "\n",
    "dataset = load_dataset(\n",
    "    args.dataset_name,\n",
    "    args.dataset_config_name,\n",
    "    cache_dir=args.cache_dir,\n",
    "    split=\"train\",\n",
    ")\n",
    "\n",
    "augmentations = transforms.Compose(\n",
    "    [\n",
    "        transforms.Resize(\n",
    "            args.resolution, interpolation=transforms.InterpolationMode.BILINEAR\n",
    "        ),\n",
    "        (\n",
    "            transforms.CenterCrop(args.resolution)\n",
    "            if args.center_crop\n",
    "            else transforms.RandomCrop(args.resolution)\n",
    "        ),\n",
    "        (\n",
    "            transforms.RandomHorizontalFlip()\n",
    "            if args.random_flip\n",
    "            else transforms.Lambda(lambda x: x)\n",
    "        ),\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize(\n",
    "            [0.5],\n",
    "            [0.5],\n",
    "        ),\n",
    "    ]\n",
    ")\n",
    "\n",
    "\n",
    "def transform_images(examples):\n",
    "    images = [augmentations(image.convert(\"RGB\")) for image in examples[\"image\"]]\n",
    "    return {\"input\": images}\n",
    "\n",
    "\n",
    "dataset.set_transform(transform_images)\n",
    "train_dataloader = torch.utils.data.DataLoader(\n",
    "    dataset,\n",
    "    batch_size=args.train_batch_size,\n",
    "    shuffle=True,\n",
    "    num_workers=args.dataloader_num_workers,\n",
    ")\n",
    "\n",
    "# Initialize the learning rate scheduler\n",
    "lr_scheduler = get_scheduler(\n",
    "    args.lr_scheduler,\n",
    "    optimizer=optimizer,\n",
    "    num_warmup_steps=args.lr_warmup_steps * args.gradient_accumulation_steps,\n",
    "    num_training_steps=(len(train_dataloader) * args.num_epochs),\n",
    ")\n",
    "\n",
    "\n",
    "total_batch_size = args.train_batch_size * args.gradient_accumulation_steps\n",
    "num_update_steps_per_epoch = math.ceil(\n",
    "    len(train_dataloader) / args.gradient_accumulation_steps\n",
    ")\n",
    "max_train_steps = args.num_epochs * num_update_steps_per_epoch\n",
    "\n",
    "\n",
    "print(\"***** Running training *****\")\n",
    "print(f\"  Num examples = {len(dataset)}\")\n",
    "print(f\"  Num Epochs = {args.num_epochs}\")\n",
    "print(f\"  Instantaneous batch size per device = {args.train_batch_size}\")\n",
    "print(\n",
    "    f\"  Total train batch size (w. parallel, distributed & accumulation) = {total_batch_size}\"\n",
    ")\n",
    "print(f\"  Gradient Accumulation steps = {args.gradient_accumulation_steps}\")\n",
    "print(f\"  Total optimization steps = {max_train_steps}\")\n",
    "\n",
    "global_step = 0\n",
    "first_epoch = 0\n",
    "\n",
    "weight_dtype = torch.float32\n",
    "# model.enable_xformers_memory_efficient_attention()\n",
    "logging_dir = os.path.join(args.output_dir, args.logging_dir)\n",
    "accelerator_project_config = ProjectConfiguration(\n",
    "    project_dir=args.output_dir, logging_dir=logging_dir\n",
    ")\n",
    "accelerator = Accelerator(\n",
    "    gradient_accumulation_steps=args.gradient_accumulation_steps,\n",
    "    mixed_precision=args.mixed_precision,\n",
    "    log_with=args.logger,\n",
    "    project_config=accelerator_project_config,\n",
    "    # kwargs_handlers=[kwargs],\n",
    ")\n",
    "model, optimizer, train_dataloader, lr_scheduler = accelerator.prepare(\n",
    "    model, optimizer, train_dataloader, lr_scheduler\n",
    ")\n",
    "\n",
    "if accelerator.is_main_process:\n",
    "    run = os.path.split(args.project_name)[-1].split(\".\")[0]\n",
    "    accelerator.init_trackers(run)\n",
    "\n",
    "# Train!\n",
    "for epoch in range(first_epoch, args.num_epochs):\n",
    "    model.train()\n",
    "    progress_bar = tqdm(\n",
    "        total=num_update_steps_per_epoch,\n",
    "        disable=not accelerator.is_local_main_process,\n",
    "    )\n",
    "    progress_bar.set_description(f\"Epoch {epoch}\")\n",
    "    for step, batch in enumerate(train_dataloader):\n",
    "        # Skip steps until we reach the resumed step\n",
    "\n",
    "        clean_images = batch[\"input\"].to(weight_dtype)\n",
    "        # Sample noise that we'll add to the images\n",
    "        noise = torch.randn(\n",
    "            clean_images.shape, dtype=weight_dtype, device=clean_images.device\n",
    "        )\n",
    "        bsz = clean_images.shape[0]\n",
    "        # Sample a random timestep for each image\n",
    "        timesteps = torch.randint(\n",
    "            0,\n",
    "            noise_scheduler.config.num_train_timesteps,\n",
    "            (bsz,),\n",
    "            device=clean_images.device,\n",
    "        ).long()\n",
    "\n",
    "        # Add noise to the clean images according to the noise magnitude at each timestep\n",
    "        # (this is the forward diffusion process)\n",
    "        noisy_images = noise_scheduler.add_noise(clean_images, noise, timesteps)\n",
    "\n",
    "        with accelerator.accumulate(model):\n",
    "            # Predict the noise residual\n",
    "            model_output = model(noisy_images, timesteps).sample\n",
    "\n",
    "            if args.prediction_type == \"epsilon\":\n",
    "                loss = F.mse_loss(\n",
    "                    model_output.float(),\n",
    "                    noise.float(),\n",
    "                )\n",
    "\n",
    "            accelerator.backward(loss)\n",
    "\n",
    "            if accelerator.sync_gradients:\n",
    "                accelerator.clip_grad_norm_(model.parameters(), 1.0)\n",
    "            optimizer.step()\n",
    "            lr_scheduler.step()\n",
    "            optimizer.zero_grad()\n",
    "\n",
    "        # Checks if the accelerator has performed an optimization step behind the scenes\n",
    "        if accelerator.sync_gradients:\n",
    "            if args.use_ema:\n",
    "                ema_model.step(model.parameters())\n",
    "            progress_bar.update(1)\n",
    "            global_step += 1\n",
    "\n",
    "            if accelerator.is_main_process:\n",
    "                if global_step % args.checkpointing_steps == 0:\n",
    "                    # _before_ saving state, check if this save would set us over the `checkpoints_total_limit`\n",
    "\n",
    "                    save_path = os.path.join(\n",
    "                        args.output_dir, f\"checkpoint-{global_step}\"\n",
    "                    )\n",
    "                    accelerator.save_state(save_path)\n",
    "                    print(f\"Saved state to {save_path}\")\n",
    "\n",
    "        logs = {\n",
    "            \"loss\": loss.detach().item(),\n",
    "            \"lr\": lr_scheduler.get_last_lr()[0],\n",
    "            \"step\": global_step,\n",
    "        }\n",
    "        if args.use_ema:\n",
    "            logs[\"ema_decay\"] = ema_model.cur_decay_value\n",
    "        progress_bar.set_postfix(**logs)\n",
    "        accelerator.log(logs, step=global_step)\n",
    "    progress_bar.close()\n",
    "\n",
    "    accelerator.wait_for_everyone()\n",
    "\n",
    "    # Generate sample images for visual inspection\n",
    "    if accelerator.is_main_process:\n",
    "        if epoch % args.save_images_epochs == 0 or epoch == args.num_epochs - 1:\n",
    "            unet = accelerator.unwrap_model(model)\n",
    "\n",
    "            if args.use_ema:\n",
    "                ema_model.store(unet.parameters())\n",
    "                ema_model.copy_to(unet.parameters())\n",
    "\n",
    "            pipeline = DDPMPipeline(\n",
    "                unet=unet,\n",
    "                scheduler=noise_scheduler,\n",
    "            )\n",
    "\n",
    "            generator = torch.Generator(device=pipeline.device).manual_seed(0)\n",
    "            # run pipeline in inference (sample random noise and denoise)\n",
    "            images = pipeline(\n",
    "                generator=generator,\n",
    "                batch_size=args.eval_batch_size,\n",
    "                num_inference_steps=args.ddpm_num_inference_steps,\n",
    "                output_type=\"np\",\n",
    "            ).images\n",
    "\n",
    "            if args.use_ema:\n",
    "                ema_model.restore(unet.parameters())\n",
    "\n",
    "            # denormalize the images and save to tensorboard\n",
    "            images_processed = (images * 255).round().astype(\"uint8\")\n",
    "\n",
    "            if args.logger == \"wandb\":\n",
    "                # Upcoming `log_images` helper coming in https://github.com/huggingface/accelerate/pull/962/files\n",
    "                accelerator.get_tracker(\"wandb\").log(\n",
    "                    {\n",
    "                        \"test_samples\": [wandb.Image(img) for img in images_processed],\n",
    "                        \"epoch\": epoch,\n",
    "                    },\n",
    "                    step=global_step,\n",
    "                )\n",
    "\n",
    "        if epoch % args.save_model_epochs == 0 or epoch == args.num_epochs - 1:\n",
    "            # save the model\n",
    "            unet = accelerator.unwrap_model(model)\n",
    "\n",
    "            if args.use_ema:\n",
    "                ema_model.store(unet.parameters())\n",
    "                ema_model.copy_to(unet.parameters())\n",
    "\n",
    "            pipeline = DDPMPipeline(\n",
    "                unet=unet,\n",
    "                scheduler=noise_scheduler,\n",
    "            )\n",
    "\n",
    "            pipeline.save_pretrained(args.output_dir)\n",
    "\n",
    "            if args.use_ema:\n",
    "                ema_model.restore(unet.parameters())\n",
    "\n",
    "\n",
    "accelerator.end_training()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
