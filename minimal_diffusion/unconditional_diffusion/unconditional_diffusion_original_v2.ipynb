{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "d9ee4c81",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.11/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "***** Running training *****\n",
      "  Num examples = 8189\n",
      "  Num Epochs = 1\n",
      "  Instantaneous batch size per device = 16\n",
      "  Total train batch size (w. parallel, distributed & accumulation) = 16\n",
      "  Gradient Accumulation steps = 1\n",
      "  Total optimization steps = 512\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mdimweb\u001b[0m to \u001b[32mhttps://api.wandb.ai\u001b[0m. Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.19.11"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/code/wandb/run-20250601_192841-mewtp7ku</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/dimweb/train_unconditional/runs/mewtp7ku' target=\"_blank\">graceful-wood-28</a></strong> to <a href='https://wandb.ai/dimweb/train_unconditional' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/developer-guide' target=\"_blank\">docs</a>)<br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/dimweb/train_unconditional' target=\"_blank\">https://wandb.ai/dimweb/train_unconditional</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/dimweb/train_unconditional/runs/mewtp7ku' target=\"_blank\">https://wandb.ai/dimweb/train_unconditional/runs/mewtp7ku</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 0:  98%|█████████▊| 502/512 [00:49<00:02,  3.54it/s, ema_decay=0.991, loss=0.0396, lr=9.33e-5, step=502]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved state to minimal_diffusion/unconditional_diffusion/ddpm-model-64/checkpoint-500\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 0: 100%|██████████| 512/512 [00:50<00:00, 10.06it/s, ema_decay=0.991, loss=0.0665, lr=0, step=512]      \n",
      "100%|██████████| 1000/1000 [00:17<00:00, 56.07it/s]\n"
     ]
    },
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<br>    <style><br>        .wandb-row {<br>            display: flex;<br>            flex-direction: row;<br>            flex-wrap: wrap;<br>            justify-content: flex-start;<br>            width: 100%;<br>        }<br>        .wandb-col {<br>            display: flex;<br>            flex-direction: column;<br>            flex-basis: 100%;<br>            flex: 1;<br>            padding: 10px;<br>        }<br>    </style><br><div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>ema_decay</td><td>▁▂▅▆▆▇▇▇▇▇▇█████████████████████████████</td></tr><tr><td>epoch</td><td>▁</td></tr><tr><td>loss</td><td>█▇▇▆▅▃▃▂▂▂▂▂▂▂▂▁▂▁▁▂▂▁▁▁▁▂▁▁▁▁▁▁▁▁▁▂▁▁▁▁</td></tr><tr><td>lr</td><td>▁▁▁▁▁▁▂▂▂▃▃▄▄▄▄▄▅▅▅▅▅▅▅▅▅▆▆▆▆▆▇▇▇▇▇████▄</td></tr><tr><td>step</td><td>▁▁▁▁▁▁▁▂▂▂▂▂▂▃▃▃▃▄▄▄▄▄▄▅▅▅▅▅▅▅▆▆▇▇▇▇▇███</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>ema_decay</td><td>0.99071</td></tr><tr><td>epoch</td><td>0</td></tr><tr><td>loss</td><td>0.06651</td></tr><tr><td>lr</td><td>0</td></tr><tr><td>step</td><td>512</td></tr></table><br/></div></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run <strong style=\"color:#cdcd00\">graceful-wood-28</strong> at: <a href='https://wandb.ai/dimweb/train_unconditional/runs/mewtp7ku' target=\"_blank\">https://wandb.ai/dimweb/train_unconditional/runs/mewtp7ku</a><br> View project at: <a href='https://wandb.ai/dimweb/train_unconditional' target=\"_blank\">https://wandb.ai/dimweb/train_unconditional</a><br>Synced 5 W&B file(s), 16 media file(s), 0 artifact file(s) and 0 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code>./wandb/run-20250601_192841-mewtp7ku/logs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import os\n",
    "import math\n",
    "\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "from accelerate import Accelerator\n",
    "from accelerate.logging import get_logger\n",
    "from accelerate.utils import ProjectConfiguration\n",
    "from datasets import load_dataset\n",
    "from huggingface_hub import create_repo, upload_folder\n",
    "from torchvision import transforms\n",
    "from tqdm.auto import tqdm\n",
    "\n",
    "from diffusers import UNet2DModel\n",
    "from diffusers.optimization import get_scheduler\n",
    "from diffusers.training_utils import EMAModel\n",
    "\n",
    "from dataclasses import dataclass\n",
    "import wandb\n",
    "import math\n",
    "from dataclasses import dataclass\n",
    "from typing import List, Optional, Tuple, Union\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "from diffusers.configuration_utils import ConfigMixin, register_to_config\n",
    "from diffusers.utils.torch_utils import randn_tensor\n",
    "from diffusers.schedulers.scheduling_utils import (\n",
    "    KarrasDiffusionSchedulers,\n",
    "    SchedulerMixin,\n",
    ")\n",
    "from diffusers.models import UNet2DModel\n",
    "from diffusers.pipelines.pipeline_utils import DiffusionPipeline, ImagePipelineOutput\n",
    "\n",
    "from diffusers.configuration_utils import ConfigMixin, register_to_config\n",
    "\n",
    "\n",
    "from diffusers.models.modeling_utils import ModelMixin\n",
    "from diffusers.models.unets.unet_2d_blocks import get_down_block, get_up_block\n",
    "from diffusers.models.unets.unet_2d import UNet2DOutput\n",
    "\n",
    "import torch.nn as nn\n",
    "\n",
    "from diffusers.schedulers.scheduling_ddpm import DDPMSchedulerOutput\n",
    "from diffusers.utils.torch_utils import is_torch_version\n",
    "\n",
    "\n",
    "from diffusers.models.activations import get_activation\n",
    "\n",
    "from diffusers.utils import deprecate\n",
    "from functools import partial\n",
    "from diffusers.utils import is_torch_npu_available\n",
    "import numbers\n",
    "\n",
    "from diffusers.image_processor import IPAdapterMaskProcessor\n",
    "from diffusers.utils import deprecate, is_torch_xla_available, logging\n",
    "from diffusers.utils.import_utils import (\n",
    "    is_torch_npu_available,\n",
    "    is_torch_xla_version,\n",
    "    is_xformers_available,\n",
    ")\n",
    "from diffusers.utils.torch_utils import is_torch_version, maybe_allow_in_graph\n",
    "from diffusers.models.attention_processor import (\n",
    "    # AttnProcessor2_0,\n",
    "    # AttnProcessor,\n",
    "    XLAFluxFlashAttnProcessor2_0,\n",
    "    XLAFlashAttnProcessor2_0,\n",
    "    AttnProcessorNPU,\n",
    "    XFormersAttnAddedKVProcessor,\n",
    "    IPAdapterXFormersAttnProcessor,\n",
    "    CustomDiffusionAttnProcessor,\n",
    "    CustomDiffusionXFormersAttnProcessor,\n",
    "    CustomDiffusionAttnProcessor2_0,\n",
    "    AttnAddedKVProcessor,\n",
    "    AttnAddedKVProcessor2_0,\n",
    "    SlicedAttnAddedKVProcessor,\n",
    "    IPAdapterAttnProcessor,\n",
    "    IPAdapterAttnProcessor2_0,\n",
    "    JointAttnProcessor2_0,\n",
    "    XFormersJointAttnProcessor,\n",
    "    XFormersAttnProcessor,\n",
    "    SlicedAttnProcessor,\n",
    "    AttentionProcessor,\n",
    ")\n",
    "import inspect\n",
    "import math\n",
    "from typing import Callable, List, Optional, Tuple, Union\n",
    "import xformers\n",
    "\n",
    "logger = logging.get_logger(__name__)\n",
    "\n",
    "\n",
    "def upfirdn2d_native(\n",
    "    tensor: torch.Tensor,\n",
    "    kernel: torch.Tensor,\n",
    "    up: int = 1,\n",
    "    down: int = 1,\n",
    "    pad: Tuple[int, int] = (0, 0),\n",
    ") -> torch.Tensor:\n",
    "    up_x = up_y = up\n",
    "    down_x = down_y = down\n",
    "    pad_x0 = pad_y0 = pad[0]\n",
    "    pad_x1 = pad_y1 = pad[1]\n",
    "\n",
    "    _, channel, in_h, in_w = tensor.shape\n",
    "    tensor = tensor.reshape(-1, in_h, in_w, 1)\n",
    "\n",
    "    _, in_h, in_w, minor = tensor.shape\n",
    "    kernel_h, kernel_w = kernel.shape\n",
    "\n",
    "    out = tensor.view(-1, in_h, 1, in_w, 1, minor)\n",
    "    out = F.pad(out, [0, 0, 0, up_x - 1, 0, 0, 0, up_y - 1])\n",
    "    out = out.view(-1, in_h * up_y, in_w * up_x, minor)\n",
    "\n",
    "    out = F.pad(\n",
    "        out, [0, 0, max(pad_x0, 0), max(pad_x1, 0), max(pad_y0, 0), max(pad_y1, 0)]\n",
    "    )\n",
    "    out = out.to(tensor.device)  # Move back to mps if necessary\n",
    "    out = out[\n",
    "        :,\n",
    "        max(-pad_y0, 0) : out.shape[1] - max(-pad_y1, 0),\n",
    "        max(-pad_x0, 0) : out.shape[2] - max(-pad_x1, 0),\n",
    "        :,\n",
    "    ]\n",
    "\n",
    "    out = out.permute(0, 3, 1, 2)\n",
    "    out = out.reshape(\n",
    "        [-1, 1, in_h * up_y + pad_y0 + pad_y1, in_w * up_x + pad_x0 + pad_x1]\n",
    "    )\n",
    "    w = torch.flip(kernel, [0, 1]).view(1, 1, kernel_h, kernel_w)\n",
    "    out = F.conv2d(out, w)\n",
    "    out = out.reshape(\n",
    "        -1,\n",
    "        minor,\n",
    "        in_h * up_y + pad_y0 + pad_y1 - kernel_h + 1,\n",
    "        in_w * up_x + pad_x0 + pad_x1 - kernel_w + 1,\n",
    "    )\n",
    "    out = out.permute(0, 2, 3, 1)\n",
    "    out = out[:, ::down_y, ::down_x, :]\n",
    "\n",
    "    out_h = (in_h * up_y + pad_y0 + pad_y1 - kernel_h) // down_y + 1\n",
    "    out_w = (in_w * up_x + pad_x0 + pad_x1 - kernel_w) // down_x + 1\n",
    "\n",
    "    return out.view(-1, channel, out_h, out_w)\n",
    "\n",
    "\n",
    "def upsample_2d(\n",
    "    hidden_states: torch.Tensor,\n",
    "    kernel: Optional[torch.Tensor] = None,\n",
    "    factor: int = 2,\n",
    "    gain: float = 1,\n",
    ") -> torch.Tensor:\n",
    "    r\"\"\"Upsample2D a batch of 2D images with the given filter.\n",
    "    Accepts a batch of 2D images of the shape `[N, C, H, W]` or `[N, H, W, C]` and upsamples each image with the given\n",
    "    filter. The filter is normalized so that if the input pixels are constant, they will be scaled by the specified\n",
    "    `gain`. Pixels outside the image are assumed to be zero, and the filter is padded with zeros so that its shape is\n",
    "    a: multiple of the upsampling factor.\n",
    "\n",
    "    Args:\n",
    "        hidden_states (`torch.Tensor`):\n",
    "            Input tensor of the shape `[N, C, H, W]` or `[N, H, W, C]`.\n",
    "        kernel (`torch.Tensor`, *optional*):\n",
    "            FIR filter of the shape `[firH, firW]` or `[firN]` (separable). The default is `[1] * factor`, which\n",
    "            corresponds to nearest-neighbor upsampling.\n",
    "        factor (`int`, *optional*, default to `2`):\n",
    "            Integer upsampling factor.\n",
    "        gain (`float`, *optional*, default to `1.0`):\n",
    "            Scaling factor for signal magnitude (default: 1.0).\n",
    "\n",
    "    Returns:\n",
    "        output (`torch.Tensor`):\n",
    "            Tensor of the shape `[N, C, H * factor, W * factor]`\n",
    "    \"\"\"\n",
    "    assert isinstance(factor, int) and factor >= 1\n",
    "    if kernel is None:\n",
    "        kernel = [1] * factor\n",
    "\n",
    "    kernel = torch.tensor(kernel, dtype=torch.float32)\n",
    "    if kernel.ndim == 1:\n",
    "        kernel = torch.outer(kernel, kernel)\n",
    "    kernel /= torch.sum(kernel)\n",
    "\n",
    "    kernel = kernel * (gain * (factor**2))\n",
    "    pad_value = kernel.shape[0] - factor\n",
    "    output = upfirdn2d_native(\n",
    "        hidden_states,\n",
    "        kernel.to(device=hidden_states.device),\n",
    "        up=factor,\n",
    "        pad=((pad_value + 1) // 2 + factor - 1, pad_value // 2),\n",
    "    )\n",
    "    return output\n",
    "\n",
    "\n",
    "def downsample_2d(\n",
    "    hidden_states: torch.Tensor,\n",
    "    kernel: Optional[torch.Tensor] = None,\n",
    "    factor: int = 2,\n",
    "    gain: float = 1,\n",
    ") -> torch.Tensor:\n",
    "    r\"\"\"Downsample2D a batch of 2D images with the given filter.\n",
    "    Accepts a batch of 2D images of the shape `[N, C, H, W]` or `[N, H, W, C]` and downsamples each image with the\n",
    "    given filter. The filter is normalized so that if the input pixels are constant, they will be scaled by the\n",
    "    specified `gain`. Pixels outside the image are assumed to be zero, and the filter is padded with zeros so that its\n",
    "    shape is a multiple of the downsampling factor.\n",
    "\n",
    "    Args:\n",
    "        hidden_states (`torch.Tensor`)\n",
    "            Input tensor of the shape `[N, C, H, W]` or `[N, H, W, C]`.\n",
    "        kernel (`torch.Tensor`, *optional*):\n",
    "            FIR filter of the shape `[firH, firW]` or `[firN]` (separable). The default is `[1] * factor`, which\n",
    "            corresponds to average pooling.\n",
    "        factor (`int`, *optional*, default to `2`):\n",
    "            Integer downsampling factor.\n",
    "        gain (`float`, *optional*, default to `1.0`):\n",
    "            Scaling factor for signal magnitude.\n",
    "\n",
    "    Returns:\n",
    "        output (`torch.Tensor`):\n",
    "            Tensor of the shape `[N, C, H // factor, W // factor]`\n",
    "    \"\"\"\n",
    "\n",
    "    assert isinstance(factor, int) and factor >= 1\n",
    "    if kernel is None:\n",
    "        kernel = [1] * factor\n",
    "\n",
    "    kernel = torch.tensor(kernel, dtype=torch.float32)\n",
    "    if kernel.ndim == 1:\n",
    "        kernel = torch.outer(kernel, kernel)\n",
    "    kernel /= torch.sum(kernel)\n",
    "\n",
    "    kernel = kernel * gain\n",
    "    pad_value = kernel.shape[0] - factor\n",
    "    output = upfirdn2d_native(\n",
    "        hidden_states,\n",
    "        kernel.to(device=hidden_states.device),\n",
    "        down=factor,\n",
    "        pad=((pad_value + 1) // 2, pad_value // 2),\n",
    "    )\n",
    "    return output\n",
    "\n",
    "\n",
    "def betas_for_alpha_bar(\n",
    "    num_diffusion_timesteps,\n",
    "    max_beta=0.999,\n",
    "    alpha_transform_type=\"cosine\",\n",
    "):\n",
    "    \"\"\"\n",
    "    ok Create a beta schedule that discretizes the given alpha_t_bar function, which defines the cumulative product of\n",
    "    (1-beta) over time from t = [0,1].\n",
    "\n",
    "    Contains a function alpha_bar that takes an argument t and transforms it to the cumulative product of (1-beta) up\n",
    "    to that part of the diffusion process.\n",
    "\n",
    "\n",
    "    Args:\n",
    "        num_diffusion_timesteps (`int`): the number of betas to produce.\n",
    "        max_beta (`float`): the maximum beta to use; use values lower than 1 to\n",
    "                     prevent singularities.\n",
    "        alpha_transform_type (`str`, *optional*, default to `cosine`): the type of noise schedule for alpha_bar.\n",
    "                     Choose from `cosine` or `exp`\n",
    "\n",
    "    Returns:\n",
    "        betas (`np.ndarray`): the betas used by the scheduler to step the model outputs\n",
    "    \"\"\"\n",
    "    if alpha_transform_type == \"cosine\":\n",
    "\n",
    "        def alpha_bar_fn(t):\n",
    "            return math.cos((t + 0.008) / 1.008 * math.pi / 2) ** 2\n",
    "\n",
    "    elif alpha_transform_type == \"exp\":\n",
    "\n",
    "        def alpha_bar_fn(t):\n",
    "            return math.exp(t * -12.0)\n",
    "\n",
    "    else:\n",
    "        raise ValueError(f\"Unsupported alpha_transform_type: {alpha_transform_type}\")\n",
    "\n",
    "    betas = []\n",
    "    for i in range(num_diffusion_timesteps):\n",
    "        t1 = i / num_diffusion_timesteps\n",
    "        t2 = (i + 1) / num_diffusion_timesteps\n",
    "        betas.append(min(1 - alpha_bar_fn(t2) / alpha_bar_fn(t1), max_beta))\n",
    "    return torch.tensor(betas, dtype=torch.float32)\n",
    "\n",
    "\n",
    "# Copied from diffusers.schedulers.scheduling_ddim.rescale_zero_terminal_snr\n",
    "def rescale_zero_terminal_snr(betas):\n",
    "    \"\"\"\n",
    "    ok Rescales betas to have zero terminal SNR Based on https://arxiv.org/pdf/2305.08891.pdf (Algorithm 1)\n",
    "\n",
    "\n",
    "    Args:\n",
    "        betas (`torch.Tensor`):\n",
    "            the betas that the scheduler is being initialized with.\n",
    "\n",
    "    Returns:\n",
    "        `torch.Tensor`: rescaled betas with zero terminal SNR\n",
    "    \"\"\"\n",
    "    # Convert betas to alphas_bar_sqrt\n",
    "    alphas = 1.0 - betas\n",
    "    alphas_cumprod = torch.cumprod(alphas, dim=0)\n",
    "    alphas_bar_sqrt = alphas_cumprod.sqrt()\n",
    "\n",
    "    # Store old values.\n",
    "    alphas_bar_sqrt_0 = alphas_bar_sqrt[0].clone()\n",
    "    alphas_bar_sqrt_T = alphas_bar_sqrt[-1].clone()\n",
    "\n",
    "    # Shift so the last timestep is zero.\n",
    "    alphas_bar_sqrt -= alphas_bar_sqrt_T\n",
    "\n",
    "    # Scale so the first timestep is back to the old value.\n",
    "    alphas_bar_sqrt *= alphas_bar_sqrt_0 / (alphas_bar_sqrt_0 - alphas_bar_sqrt_T)\n",
    "\n",
    "    # Convert alphas_bar_sqrt to betas\n",
    "    alphas_bar = alphas_bar_sqrt**2  # Revert sqrt\n",
    "    alphas = alphas_bar[1:] / alphas_bar[:-1]  # Revert cumprod\n",
    "    alphas = torch.cat([alphas_bar[0:1], alphas])\n",
    "    betas = 1 - alphas\n",
    "\n",
    "    return betas\n",
    "\n",
    "\n",
    "@maybe_allow_in_graph\n",
    "class Attention(nn.Module):\n",
    "    r\"\"\"\n",
    "    A cross attention layer.\n",
    "\n",
    "    Parameters:\n",
    "        query_dim (`int`):\n",
    "            The number of channels in the query.\n",
    "        cross_attention_dim (`int`, *optional*):\n",
    "            The number of channels in the encoder_hidden_states. If not given, defaults to `query_dim`.\n",
    "        heads (`int`,  *optional*, defaults to 8):\n",
    "            The number of heads to use for multi-head attention.\n",
    "        kv_heads (`int`,  *optional*, defaults to `None`):\n",
    "            The number of key and value heads to use for multi-head attention. Defaults to `heads`. If\n",
    "            `kv_heads=heads`, the model will use Multi Head Attention (MHA), if `kv_heads=1` the model will use Multi\n",
    "            Query Attention (MQA) otherwise GQA is used.\n",
    "        dim_head (`int`,  *optional*, defaults to 64):\n",
    "            The number of channels in each head.\n",
    "        dropout (`float`, *optional*, defaults to 0.0):\n",
    "            The dropout probability to use.\n",
    "        bias (`bool`, *optional*, defaults to False):\n",
    "            Set to `True` for the query, key, and value linear layers to contain a bias parameter.\n",
    "        upcast_attention (`bool`, *optional*, defaults to False):\n",
    "            Set to `True` to upcast the attention computation to `float32`.\n",
    "        upcast_softmax (`bool`, *optional*, defaults to False):\n",
    "            Set to `True` to upcast the softmax computation to `float32`.\n",
    "        cross_attention_norm (`str`, *optional*, defaults to `None`):\n",
    "            The type of normalization to use for the cross attention. Can be `None`, `layer_norm`, or `group_norm`.\n",
    "        cross_attention_norm_num_groups (`int`, *optional*, defaults to 32):\n",
    "            The number of groups to use for the group norm in the cross attention.\n",
    "        added_kv_proj_dim (`int`, *optional*, defaults to `None`):\n",
    "            The number of channels to use for the added key and value projections. If `None`, no projection is used.\n",
    "        norm_num_groups (`int`, *optional*, defaults to `None`):\n",
    "            The number of groups to use for the group norm in the attention.\n",
    "        spatial_norm_dim (`int`, *optional*, defaults to `None`):\n",
    "            The number of channels to use for the spatial normalization.\n",
    "        out_bias (`bool`, *optional*, defaults to `True`):\n",
    "            Set to `True` to use a bias in the output linear layer.\n",
    "        scale_qk (`bool`, *optional*, defaults to `True`):\n",
    "            Set to `True` to scale the query and key by `1 / sqrt(dim_head)`.\n",
    "        only_cross_attention (`bool`, *optional*, defaults to `False`):\n",
    "            Set to `True` to only use cross attention and not added_kv_proj_dim. Can only be set to `True` if\n",
    "            `added_kv_proj_dim` is not `None`.\n",
    "        eps (`float`, *optional*, defaults to 1e-5):\n",
    "            An additional value added to the denominator in group normalization that is used for numerical stability.\n",
    "        rescale_output_factor (`float`, *optional*, defaults to 1.0):\n",
    "            A factor to rescale the output by dividing it with this value.\n",
    "        residual_connection (`bool`, *optional*, defaults to `False`):\n",
    "            Set to `True` to add the residual connection to the output.\n",
    "        _from_deprecated_attn_block (`bool`, *optional*, defaults to `False`):\n",
    "            Set to `True` if the attention block is loaded from a deprecated state dict.\n",
    "        processor (`AttnProcessor`, *optional*, defaults to `None`):\n",
    "            The attention processor to use. If `None`, defaults to `AttnProcessor2_0` if `torch 2.x` is used and\n",
    "            `AttnProcessor` otherwise.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(\n",
    "        self,\n",
    "        query_dim: int,\n",
    "        cross_attention_dim: Optional[int] = None,\n",
    "        heads: int = 8,\n",
    "        kv_heads: Optional[int] = None,\n",
    "        dim_head: int = 64,\n",
    "        dropout: float = 0.0,\n",
    "        bias: bool = False,\n",
    "        upcast_attention: bool = False,\n",
    "        upcast_softmax: bool = False,\n",
    "        cross_attention_norm: Optional[str] = None,\n",
    "        cross_attention_norm_num_groups: int = 32,\n",
    "        qk_norm: Optional[str] = None,\n",
    "        added_kv_proj_dim: Optional[int] = None,\n",
    "        added_proj_bias: Optional[bool] = True,\n",
    "        norm_num_groups: Optional[int] = None,\n",
    "        spatial_norm_dim: Optional[int] = None,\n",
    "        out_bias: bool = True,\n",
    "        scale_qk: bool = True,\n",
    "        only_cross_attention: bool = False,\n",
    "        eps: float = 1e-5,\n",
    "        rescale_output_factor: float = 1.0,\n",
    "        residual_connection: bool = False,\n",
    "        _from_deprecated_attn_block: bool = False,\n",
    "        processor: Optional[\"AttnProcessor\"] = None,\n",
    "        out_dim: int = None,\n",
    "        out_context_dim: int = None,\n",
    "        context_pre_only=None,\n",
    "        pre_only=False,\n",
    "        elementwise_affine: bool = True,\n",
    "        is_causal: bool = False,\n",
    "    ):\n",
    "        super().__init__()\n",
    "\n",
    "        # To prevent circular import.\n",
    "        from diffusers.models.normalization import FP32LayerNorm, LpNorm, RMSNorm\n",
    "\n",
    "        self.inner_dim = out_dim if out_dim is not None else dim_head * heads\n",
    "        self.inner_kv_dim = self.inner_dim if kv_heads is None else dim_head * kv_heads\n",
    "        self.query_dim = query_dim\n",
    "        self.use_bias = bias\n",
    "        self.is_cross_attention = cross_attention_dim is not None\n",
    "        self.cross_attention_dim = (\n",
    "            cross_attention_dim if cross_attention_dim is not None else query_dim\n",
    "        )\n",
    "        self.upcast_attention = upcast_attention\n",
    "        self.upcast_softmax = upcast_softmax\n",
    "        self.rescale_output_factor = rescale_output_factor\n",
    "        self.residual_connection = residual_connection\n",
    "        self.dropout = dropout\n",
    "        self.fused_projections = False\n",
    "        self.out_dim = out_dim if out_dim is not None else query_dim\n",
    "        self.out_context_dim = (\n",
    "            out_context_dim if out_context_dim is not None else query_dim\n",
    "        )\n",
    "        self.context_pre_only = context_pre_only\n",
    "        self.pre_only = pre_only\n",
    "        self.is_causal = is_causal\n",
    "\n",
    "        # we make use of this private variable to know whether this class is loaded\n",
    "        # with an deprecated state dict so that we can convert it on the fly\n",
    "        self._from_deprecated_attn_block = _from_deprecated_attn_block\n",
    "\n",
    "        self.scale_qk = scale_qk\n",
    "        self.scale = dim_head**-0.5 if self.scale_qk else 1.0\n",
    "\n",
    "        self.heads = out_dim // dim_head if out_dim is not None else heads\n",
    "        # for slice_size > 0 the attention score computation\n",
    "        # is split across the batch axis to save memory\n",
    "        # You can set slice_size with `set_attention_slice`\n",
    "        self.sliceable_head_dim = heads\n",
    "\n",
    "        self.added_kv_proj_dim = added_kv_proj_dim\n",
    "        self.only_cross_attention = only_cross_attention\n",
    "\n",
    "        if self.added_kv_proj_dim is None and self.only_cross_attention:\n",
    "            raise ValueError(\n",
    "                \"`only_cross_attention` can only be set to True if `added_kv_proj_dim` is not None. Make sure to set either `only_cross_attention=False` or define `added_kv_proj_dim`.\"\n",
    "            )\n",
    "\n",
    "        if norm_num_groups is not None:\n",
    "            self.group_norm = nn.GroupNorm(\n",
    "                num_channels=query_dim, num_groups=norm_num_groups, eps=eps, affine=True\n",
    "            )\n",
    "        else:\n",
    "            self.group_norm = None\n",
    "\n",
    "        if spatial_norm_dim is not None:\n",
    "            self.spatial_norm = SpatialNorm(\n",
    "                f_channels=query_dim, zq_channels=spatial_norm_dim\n",
    "            )\n",
    "        else:\n",
    "            self.spatial_norm = None\n",
    "\n",
    "        if qk_norm is None:\n",
    "            self.norm_q = None\n",
    "            self.norm_k = None\n",
    "        elif qk_norm == \"layer_norm\":\n",
    "            self.norm_q = nn.LayerNorm(\n",
    "                dim_head, eps=eps, elementwise_affine=elementwise_affine\n",
    "            )\n",
    "            self.norm_k = nn.LayerNorm(\n",
    "                dim_head, eps=eps, elementwise_affine=elementwise_affine\n",
    "            )\n",
    "        elif qk_norm == \"fp32_layer_norm\":\n",
    "            self.norm_q = FP32LayerNorm(\n",
    "                dim_head, elementwise_affine=False, bias=False, eps=eps\n",
    "            )\n",
    "            self.norm_k = FP32LayerNorm(\n",
    "                dim_head, elementwise_affine=False, bias=False, eps=eps\n",
    "            )\n",
    "        elif qk_norm == \"layer_norm_across_heads\":\n",
    "            # Lumina applies qk norm across all heads\n",
    "            self.norm_q = nn.LayerNorm(dim_head * heads, eps=eps)\n",
    "            self.norm_k = nn.LayerNorm(dim_head * kv_heads, eps=eps)\n",
    "        elif qk_norm == \"rms_norm\":\n",
    "            self.norm_q = RMSNorm(dim_head, eps=eps)\n",
    "            self.norm_k = RMSNorm(dim_head, eps=eps)\n",
    "        elif qk_norm == \"rms_norm_across_heads\":\n",
    "            # LTX applies qk norm across all heads\n",
    "            self.norm_q = RMSNorm(dim_head * heads, eps=eps)\n",
    "            self.norm_k = RMSNorm(dim_head * kv_heads, eps=eps)\n",
    "        elif qk_norm == \"l2\":\n",
    "            self.norm_q = LpNorm(p=2, dim=-1, eps=eps)\n",
    "            self.norm_k = LpNorm(p=2, dim=-1, eps=eps)\n",
    "        else:\n",
    "            raise ValueError(\n",
    "                f\"unknown qk_norm: {qk_norm}. Should be one of None, 'layer_norm', 'fp32_layer_norm', 'layer_norm_across_heads', 'rms_norm', 'rms_norm_across_heads', 'l2'.\"\n",
    "            )\n",
    "\n",
    "        if cross_attention_norm is None:\n",
    "            self.norm_cross = None\n",
    "        elif cross_attention_norm == \"layer_norm\":\n",
    "            self.norm_cross = nn.LayerNorm(self.cross_attention_dim)\n",
    "        elif cross_attention_norm == \"group_norm\":\n",
    "            if self.added_kv_proj_dim is not None:\n",
    "                # The given `encoder_hidden_states` are initially of shape\n",
    "                # (batch_size, seq_len, added_kv_proj_dim) before being projected\n",
    "                # to (batch_size, seq_len, cross_attention_dim). The norm is applied\n",
    "                # before the projection, so we need to use `added_kv_proj_dim` as\n",
    "                # the number of channels for the group norm.\n",
    "                norm_cross_num_channels = added_kv_proj_dim\n",
    "            else:\n",
    "                norm_cross_num_channels = self.cross_attention_dim\n",
    "\n",
    "            self.norm_cross = nn.GroupNorm(\n",
    "                num_channels=norm_cross_num_channels,\n",
    "                num_groups=cross_attention_norm_num_groups,\n",
    "                eps=1e-5,\n",
    "                affine=True,\n",
    "            )\n",
    "        else:\n",
    "            raise ValueError(\n",
    "                f\"unknown cross_attention_norm: {cross_attention_norm}. Should be None, 'layer_norm' or 'group_norm'\"\n",
    "            )\n",
    "\n",
    "        self.to_q = nn.Linear(query_dim, self.inner_dim, bias=bias)\n",
    "\n",
    "        if not self.only_cross_attention:\n",
    "            # only relevant for the `AddedKVProcessor` classes\n",
    "            self.to_k = nn.Linear(\n",
    "                self.cross_attention_dim, self.inner_kv_dim, bias=bias\n",
    "            )\n",
    "            self.to_v = nn.Linear(\n",
    "                self.cross_attention_dim, self.inner_kv_dim, bias=bias\n",
    "            )\n",
    "        else:\n",
    "            self.to_k = None\n",
    "            self.to_v = None\n",
    "\n",
    "        self.added_proj_bias = added_proj_bias\n",
    "        if self.added_kv_proj_dim is not None:\n",
    "            self.add_k_proj = nn.Linear(\n",
    "                added_kv_proj_dim, self.inner_kv_dim, bias=added_proj_bias\n",
    "            )\n",
    "            self.add_v_proj = nn.Linear(\n",
    "                added_kv_proj_dim, self.inner_kv_dim, bias=added_proj_bias\n",
    "            )\n",
    "            if self.context_pre_only is not None:\n",
    "                self.add_q_proj = nn.Linear(\n",
    "                    added_kv_proj_dim, self.inner_dim, bias=added_proj_bias\n",
    "                )\n",
    "        else:\n",
    "            self.add_q_proj = None\n",
    "            self.add_k_proj = None\n",
    "            self.add_v_proj = None\n",
    "\n",
    "        if not self.pre_only:\n",
    "            self.to_out = nn.ModuleList([])\n",
    "            self.to_out.append(nn.Linear(self.inner_dim, self.out_dim, bias=out_bias))\n",
    "            self.to_out.append(nn.Dropout(dropout))\n",
    "        else:\n",
    "            self.to_out = None\n",
    "\n",
    "        if self.context_pre_only is not None and not self.context_pre_only:\n",
    "            self.to_add_out = nn.Linear(\n",
    "                self.inner_dim, self.out_context_dim, bias=out_bias\n",
    "            )\n",
    "        else:\n",
    "            self.to_add_out = None\n",
    "\n",
    "        if qk_norm is not None and added_kv_proj_dim is not None:\n",
    "            if qk_norm == \"layer_norm\":\n",
    "                self.norm_added_q = nn.LayerNorm(\n",
    "                    dim_head, eps=eps, elementwise_affine=elementwise_affine\n",
    "                )\n",
    "                self.norm_added_k = nn.LayerNorm(\n",
    "                    dim_head, eps=eps, elementwise_affine=elementwise_affine\n",
    "                )\n",
    "            elif qk_norm == \"fp32_layer_norm\":\n",
    "                self.norm_added_q = FP32LayerNorm(\n",
    "                    dim_head, elementwise_affine=False, bias=False, eps=eps\n",
    "                )\n",
    "                self.norm_added_k = FP32LayerNorm(\n",
    "                    dim_head, elementwise_affine=False, bias=False, eps=eps\n",
    "                )\n",
    "            elif qk_norm == \"rms_norm\":\n",
    "                self.norm_added_q = RMSNorm(dim_head, eps=eps)\n",
    "                self.norm_added_k = RMSNorm(dim_head, eps=eps)\n",
    "            elif qk_norm == \"rms_norm_across_heads\":\n",
    "                # Wan applies qk norm across all heads\n",
    "                # Wan also doesn't apply a q norm\n",
    "                self.norm_added_q = None\n",
    "                self.norm_added_k = RMSNorm(dim_head * kv_heads, eps=eps)\n",
    "            else:\n",
    "                raise ValueError(\n",
    "                    f\"unknown qk_norm: {qk_norm}. Should be one of `None,'layer_norm','fp32_layer_norm','rms_norm'`\"\n",
    "                )\n",
    "        else:\n",
    "            self.norm_added_q = None\n",
    "            self.norm_added_k = None\n",
    "\n",
    "        # set attention processor\n",
    "        # We use the AttnProcessor2_0 by default when torch 2.x is used which uses\n",
    "        # torch.nn.functional.scaled_dot_product_attention for native Flash/memory_efficient_attention\n",
    "        # but only if it has the default `scale` argument. TODO remove scale_qk check when we move to torch 2.1\n",
    "        if processor is None:\n",
    "            processor = (\n",
    "                AttnProcessor2_0()\n",
    "                if hasattr(F, \"scaled_dot_product_attention\") and self.scale_qk\n",
    "                else AttnProcessor()\n",
    "            )\n",
    "        self.set_processor(processor)\n",
    "\n",
    "    def set_use_xla_flash_attention(\n",
    "        self,\n",
    "        use_xla_flash_attention: bool,\n",
    "        partition_spec: Optional[Tuple[Optional[str], ...]] = None,\n",
    "        is_flux=False,\n",
    "    ) -> None:\n",
    "        r\"\"\"\n",
    "        Set whether to use xla flash attention from `torch_xla` or not.\n",
    "\n",
    "        Args:\n",
    "            use_xla_flash_attention (`bool`):\n",
    "                Whether to use pallas flash attention kernel from `torch_xla` or not.\n",
    "            partition_spec (`Tuple[]`, *optional*):\n",
    "                Specify the partition specification if using SPMD. Otherwise None.\n",
    "        \"\"\"\n",
    "        if use_xla_flash_attention:\n",
    "            if not is_torch_xla_available:\n",
    "                raise \"torch_xla is not available\"\n",
    "            elif is_torch_xla_version(\"<\", \"2.3\"):\n",
    "                raise \"flash attention pallas kernel is supported from torch_xla version 2.3\"\n",
    "            # elif is_spmd() and is_torch_xla_version(\"<\", \"2.4\"):\n",
    "            #     raise \"flash attention pallas kernel using SPMD is supported from torch_xla version 2.4\"\n",
    "            else:\n",
    "                if is_flux:\n",
    "                    processor = XLAFluxFlashAttnProcessor2_0(partition_spec)\n",
    "                else:\n",
    "                    processor = XLAFlashAttnProcessor2_0(partition_spec)\n",
    "        else:\n",
    "            processor = (\n",
    "                AttnProcessor2_0()\n",
    "                if hasattr(F, \"scaled_dot_product_attention\") and self.scale_qk\n",
    "                else AttnProcessor()\n",
    "            )\n",
    "        self.set_processor(processor)\n",
    "\n",
    "    def set_use_npu_flash_attention(self, use_npu_flash_attention: bool) -> None:\n",
    "        r\"\"\"\n",
    "        Set whether to use npu flash attention from `torch_npu` or not.\n",
    "\n",
    "        \"\"\"\n",
    "        if use_npu_flash_attention:\n",
    "            processor = AttnProcessorNPU()\n",
    "        else:\n",
    "            # set attention processor\n",
    "            # We use the AttnProcessor2_0 by default when torch 2.x is used which uses\n",
    "            # torch.nn.functional.scaled_dot_product_attention for native Flash/memory_efficient_attention\n",
    "            # but only if it has the default `scale` argument. TODO remove scale_qk check when we move to torch 2.1\n",
    "            processor = (\n",
    "                AttnProcessor2_0()\n",
    "                if hasattr(F, \"scaled_dot_product_attention\") and self.scale_qk\n",
    "                else AttnProcessor()\n",
    "            )\n",
    "        self.set_processor(processor)\n",
    "\n",
    "    def set_use_memory_efficient_attention_xformers(\n",
    "        self,\n",
    "        use_memory_efficient_attention_xformers: bool,\n",
    "        attention_op: Optional[Callable] = None,\n",
    "    ) -> None:\n",
    "        r\"\"\"\n",
    "        Set whether to use memory efficient attention from `xformers` or not.\n",
    "\n",
    "        Args:\n",
    "            use_memory_efficient_attention_xformers (`bool`):\n",
    "                Whether to use memory efficient attention from `xformers` or not.\n",
    "            attention_op (`Callable`, *optional*):\n",
    "                The attention operation to use. Defaults to `None` which uses the default attention operation from\n",
    "                `xformers`.\n",
    "        \"\"\"\n",
    "        is_custom_diffusion = hasattr(self, \"processor\") and isinstance(\n",
    "            self.processor,\n",
    "            (\n",
    "                CustomDiffusionAttnProcessor,\n",
    "                CustomDiffusionXFormersAttnProcessor,\n",
    "                CustomDiffusionAttnProcessor2_0,\n",
    "            ),\n",
    "        )\n",
    "        is_added_kv_processor = hasattr(self, \"processor\") and isinstance(\n",
    "            self.processor,\n",
    "            (\n",
    "                AttnAddedKVProcessor,\n",
    "                AttnAddedKVProcessor2_0,\n",
    "                SlicedAttnAddedKVProcessor,\n",
    "                XFormersAttnAddedKVProcessor,\n",
    "            ),\n",
    "        )\n",
    "        is_ip_adapter = hasattr(self, \"processor\") and isinstance(\n",
    "            self.processor,\n",
    "            (\n",
    "                IPAdapterAttnProcessor,\n",
    "                IPAdapterAttnProcessor2_0,\n",
    "                IPAdapterXFormersAttnProcessor,\n",
    "            ),\n",
    "        )\n",
    "        is_joint_processor = hasattr(self, \"processor\") and isinstance(\n",
    "            self.processor,\n",
    "            (\n",
    "                JointAttnProcessor2_0,\n",
    "                XFormersJointAttnProcessor,\n",
    "            ),\n",
    "        )\n",
    "\n",
    "        if use_memory_efficient_attention_xformers:\n",
    "            if is_added_kv_processor and is_custom_diffusion:\n",
    "                raise NotImplementedError(\n",
    "                    f\"Memory efficient attention is currently not supported for custom diffusion for attention processor type {self.processor}\"\n",
    "                )\n",
    "            if not is_xformers_available():\n",
    "                raise ModuleNotFoundError(\n",
    "                    (\n",
    "                        \"Refer to https://github.com/facebookresearch/xformers for more information on how to install\"\n",
    "                        \" xformers\"\n",
    "                    ),\n",
    "                    name=\"xformers\",\n",
    "                )\n",
    "            elif not torch.cuda.is_available():\n",
    "                raise ValueError(\n",
    "                    \"torch.cuda.is_available() should be True but is False. xformers' memory efficient attention is\"\n",
    "                    \" only available for GPU \"\n",
    "                )\n",
    "            else:\n",
    "                try:\n",
    "                    # Make sure we can run the memory efficient attention\n",
    "                    dtype = None\n",
    "                    if attention_op is not None:\n",
    "                        op_fw, op_bw = attention_op\n",
    "                        dtype, *_ = op_fw.SUPPORTED_DTYPES\n",
    "                    q = torch.randn((1, 2, 40), device=\"cuda\", dtype=dtype)\n",
    "                    _ = xformers.ops.memory_efficient_attention(q, q, q)\n",
    "                except Exception as e:\n",
    "                    raise e\n",
    "\n",
    "            if is_custom_diffusion:\n",
    "                processor = CustomDiffusionXFormersAttnProcessor(\n",
    "                    train_kv=self.processor.train_kv,\n",
    "                    train_q_out=self.processor.train_q_out,\n",
    "                    hidden_size=self.processor.hidden_size,\n",
    "                    cross_attention_dim=self.processor.cross_attention_dim,\n",
    "                    attention_op=attention_op,\n",
    "                )\n",
    "                processor.load_state_dict(self.processor.state_dict())\n",
    "                if hasattr(self.processor, \"to_k_custom_diffusion\"):\n",
    "                    processor.to(self.processor.to_k_custom_diffusion.weight.device)\n",
    "            elif is_added_kv_processor:\n",
    "                # TODO(Patrick, Suraj, William) - currently xformers doesn't work for UnCLIP\n",
    "                # which uses this type of cross attention ONLY because the attention mask of format\n",
    "                # [0, ..., -10.000, ..., 0, ...,] is not supported\n",
    "                # throw warning\n",
    "                logger.info(\n",
    "                    \"Memory efficient attention with `xformers` might currently not work correctly if an attention mask is required for the attention operation.\"\n",
    "                )\n",
    "                processor = XFormersAttnAddedKVProcessor(attention_op=attention_op)\n",
    "            elif is_ip_adapter:\n",
    "                processor = IPAdapterXFormersAttnProcessor(\n",
    "                    hidden_size=self.processor.hidden_size,\n",
    "                    cross_attention_dim=self.processor.cross_attention_dim,\n",
    "                    num_tokens=self.processor.num_tokens,\n",
    "                    scale=self.processor.scale,\n",
    "                    attention_op=attention_op,\n",
    "                )\n",
    "                processor.load_state_dict(self.processor.state_dict())\n",
    "                if hasattr(self.processor, \"to_k_ip\"):\n",
    "                    processor.to(\n",
    "                        device=self.processor.to_k_ip[0].weight.device,\n",
    "                        dtype=self.processor.to_k_ip[0].weight.dtype,\n",
    "                    )\n",
    "            elif is_joint_processor:\n",
    "                processor = XFormersJointAttnProcessor(attention_op=attention_op)\n",
    "            else:\n",
    "                processor = XFormersAttnProcessor(attention_op=attention_op)\n",
    "        else:\n",
    "            if is_custom_diffusion:\n",
    "                attn_processor_class = (\n",
    "                    CustomDiffusionAttnProcessor2_0\n",
    "                    if hasattr(F, \"scaled_dot_product_attention\")\n",
    "                    else CustomDiffusionAttnProcessor\n",
    "                )\n",
    "                processor = attn_processor_class(\n",
    "                    train_kv=self.processor.train_kv,\n",
    "                    train_q_out=self.processor.train_q_out,\n",
    "                    hidden_size=self.processor.hidden_size,\n",
    "                    cross_attention_dim=self.processor.cross_attention_dim,\n",
    "                )\n",
    "                processor.load_state_dict(self.processor.state_dict())\n",
    "                if hasattr(self.processor, \"to_k_custom_diffusion\"):\n",
    "                    processor.to(self.processor.to_k_custom_diffusion.weight.device)\n",
    "            elif is_ip_adapter:\n",
    "                processor = IPAdapterAttnProcessor2_0(\n",
    "                    hidden_size=self.processor.hidden_size,\n",
    "                    cross_attention_dim=self.processor.cross_attention_dim,\n",
    "                    num_tokens=self.processor.num_tokens,\n",
    "                    scale=self.processor.scale,\n",
    "                )\n",
    "                processor.load_state_dict(self.processor.state_dict())\n",
    "                if hasattr(self.processor, \"to_k_ip\"):\n",
    "                    processor.to(\n",
    "                        device=self.processor.to_k_ip[0].weight.device,\n",
    "                        dtype=self.processor.to_k_ip[0].weight.dtype,\n",
    "                    )\n",
    "            else:\n",
    "                # set attention processor\n",
    "                # We use the AttnProcessor2_0 by default when torch 2.x is used which uses\n",
    "                # torch.nn.functional.scaled_dot_product_attention for native Flash/memory_efficient_attention\n",
    "                # but only if it has the default `scale` argument. TODO remove scale_qk check when we move to torch 2.1\n",
    "                processor = (\n",
    "                    AttnProcessor2_0()\n",
    "                    if hasattr(F, \"scaled_dot_product_attention\") and self.scale_qk\n",
    "                    else AttnProcessor()\n",
    "                )\n",
    "\n",
    "        self.set_processor(processor)\n",
    "\n",
    "    def set_attention_slice(self, slice_size: int) -> None:\n",
    "        r\"\"\"\n",
    "        Set the slice size for attention computation.\n",
    "\n",
    "        Args:\n",
    "            slice_size (`int`):\n",
    "                The slice size for attention computation.\n",
    "        \"\"\"\n",
    "        if slice_size is not None and slice_size > self.sliceable_head_dim:\n",
    "            raise ValueError(\n",
    "                f\"slice_size {slice_size} has to be smaller or equal to {self.sliceable_head_dim}.\"\n",
    "            )\n",
    "\n",
    "        if slice_size is not None and self.added_kv_proj_dim is not None:\n",
    "            processor = SlicedAttnAddedKVProcessor(slice_size)\n",
    "        elif slice_size is not None:\n",
    "            processor = SlicedAttnProcessor(slice_size)\n",
    "        elif self.added_kv_proj_dim is not None:\n",
    "            processor = AttnAddedKVProcessor()\n",
    "        else:\n",
    "            # set attention processor\n",
    "            # We use the AttnProcessor2_0 by default when torch 2.x is used which uses\n",
    "            # torch.nn.functional.scaled_dot_product_attention for native Flash/memory_efficient_attention\n",
    "            # but only if it has the default `scale` argument. TODO remove scale_qk check when we move to torch 2.1\n",
    "            processor = (\n",
    "                AttnProcessor2_0()\n",
    "                if hasattr(F, \"scaled_dot_product_attention\") and self.scale_qk\n",
    "                else AttnProcessor()\n",
    "            )\n",
    "\n",
    "        self.set_processor(processor)\n",
    "\n",
    "    def set_processor(self, processor: \"AttnProcessor\") -> None:\n",
    "        r\"\"\"\n",
    "        Set the attention processor to use.\n",
    "\n",
    "        Args:\n",
    "            processor (`AttnProcessor`):\n",
    "                The attention processor to use.\n",
    "        \"\"\"\n",
    "        # if current processor is in `self._modules` and if passed `processor` is not, we need to\n",
    "        # pop `processor` from `self._modules`\n",
    "        if (\n",
    "            hasattr(self, \"processor\")\n",
    "            and isinstance(self.processor, torch.nn.Module)\n",
    "            and not isinstance(processor, torch.nn.Module)\n",
    "        ):\n",
    "            logger.info(\n",
    "                f\"You are removing possibly trained weights of {self.processor} with {processor}\"\n",
    "            )\n",
    "            self._modules.pop(\"processor\")\n",
    "\n",
    "        self.processor = processor\n",
    "\n",
    "    def get_processor(\n",
    "        self, return_deprecated_lora: bool = False\n",
    "    ) -> \"AttentionProcessor\":\n",
    "        r\"\"\"\n",
    "        Get the attention processor in use.\n",
    "\n",
    "        Args:\n",
    "            return_deprecated_lora (`bool`, *optional*, defaults to `False`):\n",
    "                Set to `True` to return the deprecated LoRA attention processor.\n",
    "\n",
    "        Returns:\n",
    "            \"AttentionProcessor\": The attention processor in use.\n",
    "        \"\"\"\n",
    "        if not return_deprecated_lora:\n",
    "            return self.processor\n",
    "\n",
    "    def forward(\n",
    "        self,\n",
    "        hidden_states: torch.Tensor,\n",
    "        encoder_hidden_states: Optional[torch.Tensor] = None,\n",
    "        attention_mask: Optional[torch.Tensor] = None,\n",
    "        **cross_attention_kwargs,\n",
    "    ) -> torch.Tensor:\n",
    "        r\"\"\"\n",
    "        The forward method of the `Attention` class.\n",
    "\n",
    "        Args:\n",
    "            hidden_states (`torch.Tensor`):\n",
    "                The hidden states of the query.\n",
    "            encoder_hidden_states (`torch.Tensor`, *optional*):\n",
    "                The hidden states of the encoder.\n",
    "            attention_mask (`torch.Tensor`, *optional*):\n",
    "                The attention mask to use. If `None`, no mask is applied.\n",
    "            **cross_attention_kwargs:\n",
    "                Additional keyword arguments to pass along to the cross attention.\n",
    "\n",
    "        Returns:\n",
    "            `torch.Tensor`: The output of the attention layer.\n",
    "        \"\"\"\n",
    "        # The `Attention` class can call different attention processors / attention functions\n",
    "        # here we simply pass along all tensors to the selected processor class\n",
    "        # For standard processors that are defined here, `**cross_attention_kwargs` is empty\n",
    "\n",
    "        attn_parameters = set(\n",
    "            inspect.signature(self.processor.__call__).parameters.keys()\n",
    "        )\n",
    "        quiet_attn_parameters = {\"ip_adapter_masks\", \"ip_hidden_states\"}\n",
    "        unused_kwargs = [\n",
    "            k\n",
    "            for k, _ in cross_attention_kwargs.items()\n",
    "            if k not in attn_parameters and k not in quiet_attn_parameters\n",
    "        ]\n",
    "        if len(unused_kwargs) > 0:\n",
    "            logger.warning(\n",
    "                f\"cross_attention_kwargs {unused_kwargs} are not expected by {self.processor.__class__.__name__} and will be ignored.\"\n",
    "            )\n",
    "        cross_attention_kwargs = {\n",
    "            k: w for k, w in cross_attention_kwargs.items() if k in attn_parameters\n",
    "        }\n",
    "\n",
    "        return self.processor(\n",
    "            self,\n",
    "            hidden_states,\n",
    "            encoder_hidden_states=encoder_hidden_states,\n",
    "            attention_mask=attention_mask,\n",
    "            **cross_attention_kwargs,\n",
    "        )\n",
    "\n",
    "    def batch_to_head_dim(self, tensor: torch.Tensor) -> torch.Tensor:\n",
    "        r\"\"\"\n",
    "        Reshape the tensor from `[batch_size, seq_len, dim]` to `[batch_size // heads, seq_len, dim * heads]`. `heads`\n",
    "        is the number of heads initialized while constructing the `Attention` class.\n",
    "\n",
    "        Args:\n",
    "            tensor (`torch.Tensor`): The tensor to reshape.\n",
    "\n",
    "        Returns:\n",
    "            `torch.Tensor`: The reshaped tensor.\n",
    "        \"\"\"\n",
    "        head_size = self.heads\n",
    "        batch_size, seq_len, dim = tensor.shape\n",
    "        tensor = tensor.reshape(batch_size // head_size, head_size, seq_len, dim)\n",
    "        tensor = tensor.permute(0, 2, 1, 3).reshape(\n",
    "            batch_size // head_size, seq_len, dim * head_size\n",
    "        )\n",
    "        return tensor\n",
    "\n",
    "    def head_to_batch_dim(self, tensor: torch.Tensor, out_dim: int = 3) -> torch.Tensor:\n",
    "        r\"\"\"\n",
    "        Reshape the tensor from `[batch_size, seq_len, dim]` to `[batch_size, seq_len, heads, dim // heads]` `heads` is\n",
    "        the number of heads initialized while constructing the `Attention` class.\n",
    "\n",
    "        Args:\n",
    "            tensor (`torch.Tensor`): The tensor to reshape.\n",
    "            out_dim (`int`, *optional*, defaults to `3`): The output dimension of the tensor. If `3`, the tensor is\n",
    "                reshaped to `[batch_size * heads, seq_len, dim // heads]`.\n",
    "\n",
    "        Returns:\n",
    "            `torch.Tensor`: The reshaped tensor.\n",
    "        \"\"\"\n",
    "        head_size = self.heads\n",
    "        if tensor.ndim == 3:\n",
    "            batch_size, seq_len, dim = tensor.shape\n",
    "            extra_dim = 1\n",
    "        else:\n",
    "            batch_size, extra_dim, seq_len, dim = tensor.shape\n",
    "        tensor = tensor.reshape(\n",
    "            batch_size, seq_len * extra_dim, head_size, dim // head_size\n",
    "        )\n",
    "        tensor = tensor.permute(0, 2, 1, 3)\n",
    "\n",
    "        if out_dim == 3:\n",
    "            tensor = tensor.reshape(\n",
    "                batch_size * head_size, seq_len * extra_dim, dim // head_size\n",
    "            )\n",
    "\n",
    "        return tensor\n",
    "\n",
    "    def get_attention_scores(\n",
    "        self,\n",
    "        query: torch.Tensor,\n",
    "        key: torch.Tensor,\n",
    "        attention_mask: Optional[torch.Tensor] = None,\n",
    "    ) -> torch.Tensor:\n",
    "        r\"\"\"\n",
    "        Compute the attention scores.\n",
    "\n",
    "        Args:\n",
    "            query (`torch.Tensor`): The query tensor.\n",
    "            key (`torch.Tensor`): The key tensor.\n",
    "            attention_mask (`torch.Tensor`, *optional*): The attention mask to use. If `None`, no mask is applied.\n",
    "\n",
    "        Returns:\n",
    "            `torch.Tensor`: The attention probabilities/scores.\n",
    "        \"\"\"\n",
    "        dtype = query.dtype\n",
    "        if self.upcast_attention:\n",
    "            query = query.float()\n",
    "            key = key.float()\n",
    "\n",
    "        if attention_mask is None:\n",
    "            baddbmm_input = torch.empty(\n",
    "                query.shape[0],\n",
    "                query.shape[1],\n",
    "                key.shape[1],\n",
    "                dtype=query.dtype,\n",
    "                device=query.device,\n",
    "            )\n",
    "            beta = 0\n",
    "        else:\n",
    "            baddbmm_input = attention_mask\n",
    "            beta = 1\n",
    "\n",
    "        attention_scores = torch.baddbmm(\n",
    "            baddbmm_input,\n",
    "            query,\n",
    "            key.transpose(-1, -2),\n",
    "            beta=beta,\n",
    "            alpha=self.scale,\n",
    "        )\n",
    "        del baddbmm_input\n",
    "\n",
    "        if self.upcast_softmax:\n",
    "            attention_scores = attention_scores.float()\n",
    "\n",
    "        attention_probs = attention_scores.softmax(dim=-1)\n",
    "        del attention_scores\n",
    "\n",
    "        attention_probs = attention_probs.to(dtype)\n",
    "\n",
    "        return attention_probs\n",
    "\n",
    "    def prepare_attention_mask(\n",
    "        self,\n",
    "        attention_mask: torch.Tensor,\n",
    "        target_length: int,\n",
    "        batch_size: int,\n",
    "        out_dim: int = 3,\n",
    "    ) -> torch.Tensor:\n",
    "        r\"\"\"\n",
    "        Prepare the attention mask for the attention computation.\n",
    "\n",
    "        Args:\n",
    "            attention_mask (`torch.Tensor`):\n",
    "                The attention mask to prepare.\n",
    "            target_length (`int`):\n",
    "                The target length of the attention mask. This is the length of the attention mask after padding.\n",
    "            batch_size (`int`):\n",
    "                The batch size, which is used to repeat the attention mask.\n",
    "            out_dim (`int`, *optional*, defaults to `3`):\n",
    "                The output dimension of the attention mask. Can be either `3` or `4`.\n",
    "\n",
    "        Returns:\n",
    "            `torch.Tensor`: The prepared attention mask.\n",
    "        \"\"\"\n",
    "        head_size = self.heads\n",
    "        if attention_mask is None:\n",
    "            return attention_mask\n",
    "\n",
    "        current_length: int = attention_mask.shape[-1]\n",
    "        if current_length != target_length:\n",
    "            if attention_mask.device.type == \"mps\":\n",
    "                # HACK: MPS: Does not support padding by greater than dimension of input tensor.\n",
    "                # Instead, we can manually construct the padding tensor.\n",
    "                padding_shape = (\n",
    "                    attention_mask.shape[0],\n",
    "                    attention_mask.shape[1],\n",
    "                    target_length,\n",
    "                )\n",
    "                padding = torch.zeros(\n",
    "                    padding_shape,\n",
    "                    dtype=attention_mask.dtype,\n",
    "                    device=attention_mask.device,\n",
    "                )\n",
    "                attention_mask = torch.cat([attention_mask, padding], dim=2)\n",
    "            else:\n",
    "                # TODO: for pipelines such as stable-diffusion, padding cross-attn mask:\n",
    "                #       we want to instead pad by (0, remaining_length), where remaining_length is:\n",
    "                #       remaining_length: int = target_length - current_length\n",
    "                # TODO: re-enable tests/models/test_models_unet_2d_condition.py#test_model_xattn_padding\n",
    "                attention_mask = F.pad(attention_mask, (0, target_length), value=0.0)\n",
    "\n",
    "        if out_dim == 3:\n",
    "            if attention_mask.shape[0] < batch_size * head_size:\n",
    "                attention_mask = attention_mask.repeat_interleave(\n",
    "                    head_size, dim=0, output_size=attention_mask.shape[0] * head_size\n",
    "                )\n",
    "        elif out_dim == 4:\n",
    "            attention_mask = attention_mask.unsqueeze(1)\n",
    "            attention_mask = attention_mask.repeat_interleave(\n",
    "                head_size, dim=1, output_size=attention_mask.shape[1] * head_size\n",
    "            )\n",
    "\n",
    "        return attention_mask\n",
    "\n",
    "    def norm_encoder_hidden_states(\n",
    "        self, encoder_hidden_states: torch.Tensor\n",
    "    ) -> torch.Tensor:\n",
    "        r\"\"\"\n",
    "        Normalize the encoder hidden states. Requires `self.norm_cross` to be specified when constructing the\n",
    "        `Attention` class.\n",
    "\n",
    "        Args:\n",
    "            encoder_hidden_states (`torch.Tensor`): Hidden states of the encoder.\n",
    "\n",
    "        Returns:\n",
    "            `torch.Tensor`: The normalized encoder hidden states.\n",
    "        \"\"\"\n",
    "        assert (\n",
    "            self.norm_cross is not None\n",
    "        ), \"self.norm_cross must be defined to call self.norm_encoder_hidden_states\"\n",
    "\n",
    "        if isinstance(self.norm_cross, nn.LayerNorm):\n",
    "            encoder_hidden_states = self.norm_cross(encoder_hidden_states)\n",
    "        elif isinstance(self.norm_cross, nn.GroupNorm):\n",
    "            # Group norm norms along the channels dimension and expects\n",
    "            # input to be in the shape of (N, C, *). In this case, we want\n",
    "            # to norm along the hidden dimension, so we need to move\n",
    "            # (batch_size, sequence_length, hidden_size) ->\n",
    "            # (batch_size, hidden_size, sequence_length)\n",
    "            encoder_hidden_states = encoder_hidden_states.transpose(1, 2)\n",
    "            encoder_hidden_states = self.norm_cross(encoder_hidden_states)\n",
    "            encoder_hidden_states = encoder_hidden_states.transpose(1, 2)\n",
    "        else:\n",
    "            assert False\n",
    "\n",
    "        return encoder_hidden_states\n",
    "\n",
    "    @torch.no_grad()\n",
    "    def fuse_projections(self, fuse=True):\n",
    "        device = self.to_q.weight.data.device\n",
    "        dtype = self.to_q.weight.data.dtype\n",
    "\n",
    "        if not self.is_cross_attention:\n",
    "            # fetch weight matrices.\n",
    "            concatenated_weights = torch.cat(\n",
    "                [self.to_q.weight.data, self.to_k.weight.data, self.to_v.weight.data]\n",
    "            )\n",
    "            in_features = concatenated_weights.shape[1]\n",
    "            out_features = concatenated_weights.shape[0]\n",
    "\n",
    "            # create a new single projection layer and copy over the weights.\n",
    "            self.to_qkv = nn.Linear(\n",
    "                in_features,\n",
    "                out_features,\n",
    "                bias=self.use_bias,\n",
    "                device=device,\n",
    "                dtype=dtype,\n",
    "            )\n",
    "            self.to_qkv.weight.copy_(concatenated_weights)\n",
    "            if self.use_bias:\n",
    "                concatenated_bias = torch.cat(\n",
    "                    [self.to_q.bias.data, self.to_k.bias.data, self.to_v.bias.data]\n",
    "                )\n",
    "                self.to_qkv.bias.copy_(concatenated_bias)\n",
    "\n",
    "        else:\n",
    "            concatenated_weights = torch.cat(\n",
    "                [self.to_k.weight.data, self.to_v.weight.data]\n",
    "            )\n",
    "            in_features = concatenated_weights.shape[1]\n",
    "            out_features = concatenated_weights.shape[0]\n",
    "\n",
    "            self.to_kv = nn.Linear(\n",
    "                in_features,\n",
    "                out_features,\n",
    "                bias=self.use_bias,\n",
    "                device=device,\n",
    "                dtype=dtype,\n",
    "            )\n",
    "            self.to_kv.weight.copy_(concatenated_weights)\n",
    "            if self.use_bias:\n",
    "                concatenated_bias = torch.cat(\n",
    "                    [self.to_k.bias.data, self.to_v.bias.data]\n",
    "                )\n",
    "                self.to_kv.bias.copy_(concatenated_bias)\n",
    "\n",
    "        # handle added projections for SD3 and others.\n",
    "        if (\n",
    "            getattr(self, \"add_q_proj\", None) is not None\n",
    "            and getattr(self, \"add_k_proj\", None) is not None\n",
    "            and getattr(self, \"add_v_proj\", None) is not None\n",
    "        ):\n",
    "            concatenated_weights = torch.cat(\n",
    "                [\n",
    "                    self.add_q_proj.weight.data,\n",
    "                    self.add_k_proj.weight.data,\n",
    "                    self.add_v_proj.weight.data,\n",
    "                ]\n",
    "            )\n",
    "            in_features = concatenated_weights.shape[1]\n",
    "            out_features = concatenated_weights.shape[0]\n",
    "\n",
    "            self.to_added_qkv = nn.Linear(\n",
    "                in_features,\n",
    "                out_features,\n",
    "                bias=self.added_proj_bias,\n",
    "                device=device,\n",
    "                dtype=dtype,\n",
    "            )\n",
    "            self.to_added_qkv.weight.copy_(concatenated_weights)\n",
    "            if self.added_proj_bias:\n",
    "                concatenated_bias = torch.cat(\n",
    "                    [\n",
    "                        self.add_q_proj.bias.data,\n",
    "                        self.add_k_proj.bias.data,\n",
    "                        self.add_v_proj.bias.data,\n",
    "                    ]\n",
    "                )\n",
    "                self.to_added_qkv.bias.copy_(concatenated_bias)\n",
    "\n",
    "        self.fused_projections = fuse\n",
    "\n",
    "\n",
    "class AttnProcessor2_0:\n",
    "    r\"\"\"\n",
    "    Processor for implementing scaled dot-product attention (enabled by default if you're using PyTorch 2.0).\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self):\n",
    "        if not hasattr(F, \"scaled_dot_product_attention\"):\n",
    "            raise ImportError(\n",
    "                \"AttnProcessor2_0 requires PyTorch 2.0, to use it, please upgrade PyTorch to 2.0.\"\n",
    "            )\n",
    "\n",
    "    def __call__(\n",
    "        self,\n",
    "        attn: Attention,\n",
    "        hidden_states: torch.Tensor,\n",
    "        encoder_hidden_states: Optional[torch.Tensor] = None,\n",
    "        attention_mask: Optional[torch.Tensor] = None,\n",
    "        temb: Optional[torch.Tensor] = None,\n",
    "        *args,\n",
    "        **kwargs,\n",
    "    ) -> torch.Tensor:\n",
    "        if len(args) > 0 or kwargs.get(\"scale\", None) is not None:\n",
    "            deprecation_message = \"The `scale` argument is deprecated and will be ignored. Please remove it, as passing it will raise an error in the future. `scale` should directly be passed while calling the underlying pipeline component i.e., via `cross_attention_kwargs`.\"\n",
    "            deprecate(\"scale\", \"1.0.0\", deprecation_message)\n",
    "\n",
    "        residual = hidden_states\n",
    "        if attn.spatial_norm is not None:\n",
    "            hidden_states = attn.spatial_norm(hidden_states, temb)\n",
    "\n",
    "        input_ndim = hidden_states.ndim\n",
    "\n",
    "        if input_ndim == 4:\n",
    "            batch_size, channel, height, width = hidden_states.shape\n",
    "            hidden_states = hidden_states.view(\n",
    "                batch_size, channel, height * width\n",
    "            ).transpose(1, 2)\n",
    "\n",
    "        batch_size, sequence_length, _ = (\n",
    "            hidden_states.shape\n",
    "            if encoder_hidden_states is None\n",
    "            else encoder_hidden_states.shape\n",
    "        )\n",
    "\n",
    "        if attention_mask is not None:\n",
    "            attention_mask = attn.prepare_attention_mask(\n",
    "                attention_mask, sequence_length, batch_size\n",
    "            )\n",
    "            # scaled_dot_product_attention expects attention_mask shape to be\n",
    "            # (batch, heads, source_length, target_length)\n",
    "            attention_mask = attention_mask.view(\n",
    "                batch_size, attn.heads, -1, attention_mask.shape[-1]\n",
    "            )\n",
    "\n",
    "        if attn.group_norm is not None:\n",
    "            hidden_states = attn.group_norm(hidden_states.transpose(1, 2)).transpose(\n",
    "                1, 2\n",
    "            )\n",
    "\n",
    "        query = attn.to_q(hidden_states)\n",
    "\n",
    "        if encoder_hidden_states is None:\n",
    "            encoder_hidden_states = hidden_states\n",
    "        elif attn.norm_cross:\n",
    "            encoder_hidden_states = attn.norm_encoder_hidden_states(\n",
    "                encoder_hidden_states\n",
    "            )\n",
    "\n",
    "        key = attn.to_k(encoder_hidden_states)\n",
    "        value = attn.to_v(encoder_hidden_states)\n",
    "\n",
    "        inner_dim = key.shape[-1]\n",
    "        head_dim = inner_dim // attn.heads\n",
    "\n",
    "        query = query.view(batch_size, -1, attn.heads, head_dim).transpose(1, 2)\n",
    "\n",
    "        key = key.view(batch_size, -1, attn.heads, head_dim).transpose(1, 2)\n",
    "        value = value.view(batch_size, -1, attn.heads, head_dim).transpose(1, 2)\n",
    "\n",
    "        if attn.norm_q is not None:\n",
    "            query = attn.norm_q(query)\n",
    "        if attn.norm_k is not None:\n",
    "            key = attn.norm_k(key)\n",
    "\n",
    "        # the output of sdp = (batch, num_heads, seq_len, head_dim)\n",
    "        # TODO: add support for attn.scale when we move to Torch 2.1\n",
    "        hidden_states = F.scaled_dot_product_attention(\n",
    "            query, key, value, attn_mask=attention_mask, dropout_p=0.0, is_causal=False\n",
    "        )\n",
    "\n",
    "        hidden_states = hidden_states.transpose(1, 2).reshape(\n",
    "            batch_size, -1, attn.heads * head_dim\n",
    "        )\n",
    "        hidden_states = hidden_states.to(query.dtype)\n",
    "\n",
    "        # linear proj\n",
    "        hidden_states = attn.to_out[0](hidden_states)\n",
    "        # dropout\n",
    "        hidden_states = attn.to_out[1](hidden_states)\n",
    "\n",
    "        if input_ndim == 4:\n",
    "            hidden_states = hidden_states.transpose(-1, -2).reshape(\n",
    "                batch_size, channel, height, width\n",
    "            )\n",
    "\n",
    "        if attn.residual_connection:\n",
    "            hidden_states = hidden_states + residual\n",
    "\n",
    "        hidden_states = hidden_states / attn.rescale_output_factor\n",
    "\n",
    "        return hidden_states\n",
    "\n",
    "\n",
    "class AttnProcessor:\n",
    "    r\"\"\"\n",
    "    Default processor for performing attention-related computations.\n",
    "    \"\"\"\n",
    "\n",
    "    def __call__(\n",
    "        self,\n",
    "        attn: Attention,\n",
    "        hidden_states: torch.Tensor,\n",
    "        encoder_hidden_states: Optional[torch.Tensor] = None,\n",
    "        attention_mask: Optional[torch.Tensor] = None,\n",
    "        temb: Optional[torch.Tensor] = None,\n",
    "        *args,\n",
    "        **kwargs,\n",
    "    ) -> torch.Tensor:\n",
    "        if len(args) > 0 or kwargs.get(\"scale\", None) is not None:\n",
    "            deprecation_message = \"The `scale` argument is deprecated and will be ignored. Please remove it, as passing it will raise an error in the future. `scale` should directly be passed while calling the underlying pipeline component i.e., via `cross_attention_kwargs`.\"\n",
    "            deprecate(\"scale\", \"1.0.0\", deprecation_message)\n",
    "\n",
    "        residual = hidden_states\n",
    "\n",
    "        if attn.spatial_norm is not None:\n",
    "            hidden_states = attn.spatial_norm(hidden_states, temb)\n",
    "\n",
    "        input_ndim = hidden_states.ndim\n",
    "\n",
    "        if input_ndim == 4:\n",
    "            batch_size, channel, height, width = hidden_states.shape\n",
    "            hidden_states = hidden_states.view(\n",
    "                batch_size, channel, height * width\n",
    "            ).transpose(1, 2)\n",
    "\n",
    "        batch_size, sequence_length, _ = (\n",
    "            hidden_states.shape\n",
    "            if encoder_hidden_states is None\n",
    "            else encoder_hidden_states.shape\n",
    "        )\n",
    "        attention_mask = attn.prepare_attention_mask(\n",
    "            attention_mask, sequence_length, batch_size\n",
    "        )\n",
    "\n",
    "        if attn.group_norm is not None:\n",
    "            hidden_states = attn.group_norm(hidden_states.transpose(1, 2)).transpose(\n",
    "                1, 2\n",
    "            )\n",
    "\n",
    "        query = attn.to_q(hidden_states)\n",
    "\n",
    "        if encoder_hidden_states is None:\n",
    "            encoder_hidden_states = hidden_states\n",
    "        elif attn.norm_cross:\n",
    "            encoder_hidden_states = attn.norm_encoder_hidden_states(\n",
    "                encoder_hidden_states\n",
    "            )\n",
    "\n",
    "        key = attn.to_k(encoder_hidden_states)\n",
    "        value = attn.to_v(encoder_hidden_states)\n",
    "\n",
    "        query = attn.head_to_batch_dim(query)\n",
    "        key = attn.head_to_batch_dim(key)\n",
    "        value = attn.head_to_batch_dim(value)\n",
    "\n",
    "        attention_probs = attn.get_attention_scores(query, key, attention_mask)\n",
    "        hidden_states = torch.bmm(attention_probs, value)\n",
    "        hidden_states = attn.batch_to_head_dim(hidden_states)\n",
    "\n",
    "        # linear proj\n",
    "        hidden_states = attn.to_out[0](hidden_states)\n",
    "        # dropout\n",
    "        hidden_states = attn.to_out[1](hidden_states)\n",
    "\n",
    "        if input_ndim == 4:\n",
    "            hidden_states = hidden_states.transpose(-1, -2).reshape(\n",
    "                batch_size, channel, height, width\n",
    "            )\n",
    "\n",
    "        if attn.residual_connection:\n",
    "            hidden_states = hidden_states + residual\n",
    "\n",
    "        hidden_states = hidden_states / attn.rescale_output_factor\n",
    "\n",
    "        return hidden_states\n",
    "\n",
    "\n",
    "class DDPMScheduler(SchedulerMixin, ConfigMixin):\n",
    "    \"\"\"\n",
    "    ok `DDPMScheduler` explores the connections between denoising score matching and Langevin dynamics sampling.\n",
    "\n",
    "    This model inherits from [`SchedulerMixin`] and [`ConfigMixin`]. Check the superclass documentation for the generic\n",
    "    methods the library implements for all schedulers such as loading and saving.\n",
    "\n",
    "    Args:\n",
    "        num_train_timesteps (`int`, defaults to 1000):\n",
    "            The number of diffusion steps to train the model.\n",
    "        beta_start (`float`, defaults to 0.0001):\n",
    "            The starting `beta` value of inference.\n",
    "        beta_end (`float`, defaults to 0.02):\n",
    "            The final `beta` value.\n",
    "        beta_schedule (`str`, defaults to `\"linear\"`):\n",
    "            The beta schedule, a mapping from a beta range to a sequence of betas for stepping the model. Choose from\n",
    "            `linear`, `scaled_linear`, `squaredcos_cap_v2`, or `sigmoid`.\n",
    "        trained_betas (`np.ndarray`, *optional*):\n",
    "            An array of betas to pass directly to the constructor without using `beta_start` and `beta_end`.\n",
    "        variance_type (`str`, defaults to `\"fixed_small\"`):\n",
    "            Clip the variance when adding noise to the denoised sample. Choose from `fixed_small`, `fixed_small_log`,\n",
    "            `fixed_large`, `fixed_large_log`, `learned` or `learned_range`.\n",
    "        clip_sample (`bool`, defaults to `True`):\n",
    "            Clip the predicted sample for numerical stability.\n",
    "        clip_sample_range (`float`, defaults to 1.0):\n",
    "            The maximum magnitude for sample clipping. Valid only when `clip_sample=True`.\n",
    "        prediction_type (`str`, defaults to `epsilon`, *optional*):\n",
    "            Prediction type of the scheduler function; can be `epsilon` (predicts the noise of the diffusion process),\n",
    "            `sample` (directly predicts the noisy sample`) or `v_prediction` (see section 2.4 of [Imagen\n",
    "            Video](https://imagen.research.google/video/paper.pdf) paper).\n",
    "        thresholding (`bool`, defaults to `False`):\n",
    "            Whether to use the \"dynamic thresholding\" method. This is unsuitable for latent-space diffusion models such\n",
    "            as Stable Diffusion.\n",
    "        dynamic_thresholding_ratio (`float`, defaults to 0.995):\n",
    "            The ratio for the dynamic thresholding method. Valid only when `thresholding=True`.\n",
    "        sample_max_value (`float`, defaults to 1.0):\n",
    "            The threshold value for dynamic thresholding. Valid only when `thresholding=True`.\n",
    "        timestep_spacing (`str`, defaults to `\"leading\"`):\n",
    "            The way the timesteps should be scaled. Refer to Table 2 of the [Common Diffusion Noise Schedules and\n",
    "            Sample Steps are Flawed](https://huggingface.co/papers/2305.08891) for more information.\n",
    "        steps_offset (`int`, defaults to 0):\n",
    "            An offset added to the inference steps, as required by some model families.\n",
    "        rescale_betas_zero_snr (`bool`, defaults to `False`):\n",
    "            Whether to rescale the betas to have zero terminal SNR. This enables the model to generate very bright and\n",
    "            dark samples instead of limiting it to samples with medium brightness. Loosely related to\n",
    "            [`--offset_noise`](https://github.com/huggingface/diffusers/blob/74fd735eb073eb1d774b1ab4154a0876eb82f055/examples/dreambooth/train_dreambooth.py#L506).\n",
    "    \"\"\"\n",
    "\n",
    "    _compatibles = [e.name for e in KarrasDiffusionSchedulers]\n",
    "    order = 1\n",
    "\n",
    "    @register_to_config\n",
    "    def __init__(\n",
    "        self,\n",
    "        num_train_timesteps: int = 1000,\n",
    "        beta_start: float = 0.0001,\n",
    "        beta_end: float = 0.02,\n",
    "        beta_schedule: str = \"linear\",\n",
    "        trained_betas: Optional[Union[np.ndarray, List[float]]] = None,\n",
    "        variance_type: str = \"fixed_small\",\n",
    "        clip_sample: bool = True,\n",
    "        prediction_type: str = \"epsilon\",\n",
    "        thresholding: bool = False,\n",
    "        dynamic_thresholding_ratio: float = 0.995,\n",
    "        clip_sample_range: float = 1.0,\n",
    "        sample_max_value: float = 1.0,\n",
    "        timestep_spacing: str = \"leading\",\n",
    "        steps_offset: int = 0,\n",
    "        rescale_betas_zero_snr: bool = False,\n",
    "    ):\n",
    "        if trained_betas is not None:\n",
    "            self.betas = torch.tensor(trained_betas, dtype=torch.float32)\n",
    "        elif beta_schedule == \"linear\":\n",
    "            self.betas = torch.linspace(\n",
    "                beta_start, beta_end, num_train_timesteps, dtype=torch.float32\n",
    "            )\n",
    "        elif beta_schedule == \"scaled_linear\":\n",
    "            # this schedule is very specific to the latent diffusion model.\n",
    "            self.betas = (\n",
    "                torch.linspace(\n",
    "                    beta_start**0.5,\n",
    "                    beta_end**0.5,\n",
    "                    num_train_timesteps,\n",
    "                    dtype=torch.float32,\n",
    "                )\n",
    "                ** 2\n",
    "            )\n",
    "        elif beta_schedule == \"squaredcos_cap_v2\":\n",
    "            # Glide cosine schedule\n",
    "            self.betas = betas_for_alpha_bar(num_train_timesteps)\n",
    "        elif beta_schedule == \"sigmoid\":\n",
    "            # GeoDiff sigmoid schedule\n",
    "            betas = torch.linspace(-6, 6, num_train_timesteps)\n",
    "            self.betas = torch.sigmoid(betas) * (beta_end - beta_start) + beta_start\n",
    "        else:\n",
    "            raise NotImplementedError(\n",
    "                f\"{beta_schedule} is not implemented for {self.__class__}\"\n",
    "            )\n",
    "\n",
    "        # Rescale for zero SNR\n",
    "        if rescale_betas_zero_snr:\n",
    "            self.betas = rescale_zero_terminal_snr(self.betas)\n",
    "\n",
    "        self.alphas = 1.0 - self.betas\n",
    "        self.alphas_cumprod = torch.cumprod(self.alphas, dim=0)\n",
    "        self.one = torch.tensor(1.0)\n",
    "\n",
    "        # standard deviation of the initial noise distribution\n",
    "        self.init_noise_sigma = 1.0\n",
    "\n",
    "        # setable values\n",
    "        self.custom_timesteps = False\n",
    "        self.num_inference_steps = None\n",
    "        self.timesteps = torch.from_numpy(\n",
    "            np.arange(0, num_train_timesteps)[::-1].copy()\n",
    "        )\n",
    "\n",
    "        self.variance_type = variance_type\n",
    "\n",
    "    def scale_model_input(\n",
    "        self, sample: torch.Tensor, timestep: Optional[int] = None\n",
    "    ) -> torch.Tensor:\n",
    "        \"\"\"\n",
    "        Ensures interchangeability with schedulers that need to scale the denoising model input depending on the\n",
    "        current timestep.\n",
    "\n",
    "        Args:\n",
    "            sample (`torch.Tensor`):\n",
    "                The input sample.\n",
    "            timestep (`int`, *optional*):\n",
    "                The current timestep in the diffusion chain.\n",
    "\n",
    "        Returns:\n",
    "            `torch.Tensor`:\n",
    "                A scaled input sample.\n",
    "        \"\"\"\n",
    "        return sample\n",
    "\n",
    "    def set_timesteps(\n",
    "        self,\n",
    "        num_inference_steps: Optional[int] = None,\n",
    "        device: Union[str, torch.device] = None,\n",
    "        timesteps: Optional[List[int]] = None,\n",
    "    ):\n",
    "        \"\"\"\n",
    "        Sets the discrete timesteps used for the diffusion chain (to be run before inference).\n",
    "\n",
    "        Args:\n",
    "            num_inference_steps (`int`):\n",
    "                The number of diffusion steps used when generating samples with a pre-trained model. If used,\n",
    "                `timesteps` must be `None`.\n",
    "            device (`str` or `torch.device`, *optional*):\n",
    "                The device to which the timesteps should be moved to. If `None`, the timesteps are not moved.\n",
    "            timesteps (`List[int]`, *optional*):\n",
    "                Custom timesteps used to support arbitrary spacing between timesteps. If `None`, then the default\n",
    "                timestep spacing strategy of equal spacing between timesteps is used. If `timesteps` is passed,\n",
    "                `num_inference_steps` must be `None`.\n",
    "\n",
    "        \"\"\"\n",
    "        if num_inference_steps is not None and timesteps is not None:\n",
    "            raise ValueError(\n",
    "                \"Can only pass one of `num_inference_steps` or `custom_timesteps`.\"\n",
    "            )\n",
    "\n",
    "        if timesteps is not None:\n",
    "            for i in range(1, len(timesteps)):\n",
    "                if timesteps[i] >= timesteps[i - 1]:\n",
    "                    raise ValueError(\"`custom_timesteps` must be in descending order.\")\n",
    "\n",
    "            if timesteps[0] >= self.config.num_train_timesteps:\n",
    "                raise ValueError(\n",
    "                    f\"`timesteps` must start before `self.config.train_timesteps`: {self.config.num_train_timesteps}.\"\n",
    "                )\n",
    "\n",
    "            timesteps = np.array(timesteps, dtype=np.int64)\n",
    "            self.custom_timesteps = True\n",
    "        else:\n",
    "            if num_inference_steps > self.config.num_train_timesteps:\n",
    "                raise ValueError(\n",
    "                    f\"`num_inference_steps`: {num_inference_steps} cannot be larger than `self.config.train_timesteps`:\"\n",
    "                    f\" {self.config.num_train_timesteps} as the unet model trained with this scheduler can only handle\"\n",
    "                    f\" maximal {self.config.num_train_timesteps} timesteps.\"\n",
    "                )\n",
    "\n",
    "            self.num_inference_steps = num_inference_steps\n",
    "            self.custom_timesteps = False\n",
    "\n",
    "            # \"linspace\", \"leading\", \"trailing\" corresponds to annotation of Table 2. of https://arxiv.org/abs/2305.08891\n",
    "            if self.config.timestep_spacing == \"linspace\":\n",
    "                timesteps = (\n",
    "                    np.linspace(\n",
    "                        0, self.config.num_train_timesteps - 1, num_inference_steps\n",
    "                    )\n",
    "                    .round()[::-1]\n",
    "                    .copy()\n",
    "                    .astype(np.int64)\n",
    "                )\n",
    "            elif self.config.timestep_spacing == \"leading\":\n",
    "                step_ratio = self.config.num_train_timesteps // self.num_inference_steps\n",
    "                # creates integer timesteps by multiplying by ratio\n",
    "                # casting to int to avoid issues when num_inference_step is power of 3\n",
    "                timesteps = (\n",
    "                    (np.arange(0, num_inference_steps) * step_ratio)\n",
    "                    .round()[::-1]\n",
    "                    .copy()\n",
    "                    .astype(np.int64)\n",
    "                )\n",
    "                timesteps += self.config.steps_offset\n",
    "            elif self.config.timestep_spacing == \"trailing\":\n",
    "                step_ratio = self.config.num_train_timesteps / self.num_inference_steps\n",
    "                # creates integer timesteps by multiplying by ratio\n",
    "                # casting to int to avoid issues when num_inference_step is power of 3\n",
    "                timesteps = np.round(\n",
    "                    np.arange(self.config.num_train_timesteps, 0, -step_ratio)\n",
    "                ).astype(np.int64)\n",
    "                timesteps -= 1\n",
    "            else:\n",
    "                raise ValueError(\n",
    "                    f\"{self.config.timestep_spacing} is not supported. Please make sure to choose one of 'linspace', 'leading' or 'trailing'.\"\n",
    "                )\n",
    "\n",
    "        self.timesteps = torch.from_numpy(timesteps).to(device)\n",
    "\n",
    "    def _get_variance(self, t, predicted_variance=None, variance_type=None):\n",
    "        prev_t = self.previous_timestep(t)\n",
    "\n",
    "        alpha_prod_t = self.alphas_cumprod[t]\n",
    "        alpha_prod_t_prev = self.alphas_cumprod[prev_t] if prev_t >= 0 else self.one\n",
    "        current_beta_t = 1 - alpha_prod_t / alpha_prod_t_prev\n",
    "\n",
    "        # For t > 0, compute predicted variance βt (see formula (6) and (7) from https://arxiv.org/pdf/2006.11239.pdf)\n",
    "        # and sample from it to get previous sample\n",
    "        # x_{t-1} ~ N(pred_prev_sample, variance) == add variance to pred_sample\n",
    "        variance = (1 - alpha_prod_t_prev) / (1 - alpha_prod_t) * current_beta_t\n",
    "\n",
    "        # we always take the log of variance, so clamp it to ensure it's not 0\n",
    "        variance = torch.clamp(variance, min=1e-20)\n",
    "\n",
    "        if variance_type is None:\n",
    "            variance_type = self.config.variance_type\n",
    "\n",
    "        # hacks - were probably added for training stability\n",
    "        if variance_type == \"fixed_small\":\n",
    "            variance = variance\n",
    "        # for rl-diffuser https://arxiv.org/abs/2205.09991\n",
    "        elif variance_type == \"fixed_small_log\":\n",
    "            variance = torch.log(variance)\n",
    "            variance = torch.exp(0.5 * variance)\n",
    "        elif variance_type == \"fixed_large\":\n",
    "            variance = current_beta_t\n",
    "        elif variance_type == \"fixed_large_log\":\n",
    "            # Glide max_log\n",
    "            variance = torch.log(current_beta_t)\n",
    "        elif variance_type == \"learned\":\n",
    "            return predicted_variance\n",
    "        elif variance_type == \"learned_range\":\n",
    "            min_log = torch.log(variance)\n",
    "            max_log = torch.log(current_beta_t)\n",
    "            frac = (predicted_variance + 1) / 2\n",
    "            variance = frac * max_log + (1 - frac) * min_log\n",
    "\n",
    "        return variance\n",
    "\n",
    "    def _threshold_sample(self, sample: torch.Tensor) -> torch.Tensor:\n",
    "        \"\"\"\n",
    "        \"Dynamic thresholding: At each sampling step we set s to a certain percentile absolute pixel value in xt0 (the\n",
    "        prediction of x_0 at timestep t), and if s > 1, then we threshold xt0 to the range [-s, s] and then divide by\n",
    "        s. Dynamic thresholding pushes saturated pixels (those near -1 and 1) inwards, thereby actively preventing\n",
    "        pixels from saturation at each step. We find that dynamic thresholding results in significantly better\n",
    "        photorealism as well as better image-text alignment, especially when using very large guidance weights.\"\n",
    "\n",
    "        https://arxiv.org/abs/2205.11487\n",
    "        \"\"\"\n",
    "        dtype = sample.dtype\n",
    "        batch_size, channels, *remaining_dims = sample.shape\n",
    "\n",
    "        if dtype not in (torch.float32, torch.float64):\n",
    "            sample = (\n",
    "                sample.float()\n",
    "            )  # upcast for quantile calculation, and clamp not implemented for cpu half\n",
    "\n",
    "        # Flatten sample for doing quantile calculation along each image\n",
    "        sample = sample.reshape(batch_size, channels * np.prod(remaining_dims))\n",
    "\n",
    "        abs_sample = sample.abs()  # \"a certain percentile absolute pixel value\"\n",
    "\n",
    "        s = torch.quantile(abs_sample, self.config.dynamic_thresholding_ratio, dim=1)\n",
    "        s = torch.clamp(\n",
    "            s, min=1, max=self.config.sample_max_value\n",
    "        )  # When clamped to min=1, equivalent to standard clipping to [-1, 1]\n",
    "        s = s.unsqueeze(1)  # (batch_size, 1) because clamp will broadcast along dim=0\n",
    "        sample = (\n",
    "            torch.clamp(sample, -s, s) / s\n",
    "        )  # \"we threshold xt0 to the range [-s, s] and then divide by s\"\n",
    "\n",
    "        sample = sample.reshape(batch_size, channels, *remaining_dims)\n",
    "        sample = sample.to(dtype)\n",
    "\n",
    "        return sample\n",
    "\n",
    "    def step(\n",
    "        self,\n",
    "        model_output: torch.Tensor,\n",
    "        timestep: int,\n",
    "        sample: torch.Tensor,\n",
    "        generator=None,\n",
    "        return_dict: bool = True,\n",
    "    ) -> Union[DDPMSchedulerOutput, Tuple]:\n",
    "        \"\"\"\n",
    "        Predict the sample from the previous timestep by reversing the SDE. This function propagates the diffusion\n",
    "        process from the learned model outputs (most often the predicted noise).\n",
    "\n",
    "        Args:\n",
    "            model_output (`torch.Tensor`):\n",
    "                The direct output from learned diffusion model.\n",
    "            timestep (`float`):\n",
    "                The current discrete timestep in the diffusion chain.\n",
    "            sample (`torch.Tensor`):\n",
    "                A current instance of a sample created by the diffusion process.\n",
    "            generator (`torch.Generator`, *optional*):\n",
    "                A random number generator.\n",
    "            return_dict (`bool`, *optional*, defaults to `True`):\n",
    "                Whether or not to return a [`~schedulers.scheduling_ddpm.DDPMSchedulerOutput`] or `tuple`.\n",
    "\n",
    "        Returns:\n",
    "            [`~schedulers.scheduling_ddpm.DDPMSchedulerOutput`] or `tuple`:\n",
    "                If return_dict is `True`, [`~schedulers.scheduling_ddpm.DDPMSchedulerOutput`] is returned, otherwise a\n",
    "                tuple is returned where the first element is the sample tensor.\n",
    "\n",
    "        \"\"\"\n",
    "        t = timestep\n",
    "\n",
    "        prev_t = self.previous_timestep(t)\n",
    "\n",
    "        if model_output.shape[1] == sample.shape[1] * 2 and self.variance_type in [\n",
    "            \"learned\",\n",
    "            \"learned_range\",\n",
    "        ]:\n",
    "            model_output, predicted_variance = torch.split(\n",
    "                model_output, sample.shape[1], dim=1\n",
    "            )\n",
    "        else:\n",
    "            predicted_variance = None\n",
    "\n",
    "        # 1. compute alphas, betas\n",
    "        alpha_prod_t = self.alphas_cumprod[t]\n",
    "        alpha_prod_t_prev = self.alphas_cumprod[prev_t] if prev_t >= 0 else self.one\n",
    "        beta_prod_t = 1 - alpha_prod_t\n",
    "        beta_prod_t_prev = 1 - alpha_prod_t_prev\n",
    "        current_alpha_t = alpha_prod_t / alpha_prod_t_prev\n",
    "        current_beta_t = 1 - current_alpha_t\n",
    "\n",
    "        # 2. compute predicted original sample from predicted noise also called\n",
    "        # \"predicted x_0\" of formula (15) from https://arxiv.org/pdf/2006.11239.pdf\n",
    "        if self.config.prediction_type == \"epsilon\":\n",
    "            pred_original_sample = (\n",
    "                sample - beta_prod_t ** (0.5) * model_output\n",
    "            ) / alpha_prod_t ** (0.5)\n",
    "        elif self.config.prediction_type == \"sample\":\n",
    "            pred_original_sample = model_output\n",
    "        elif self.config.prediction_type == \"v_prediction\":\n",
    "            pred_original_sample = (alpha_prod_t**0.5) * sample - (\n",
    "                beta_prod_t**0.5\n",
    "            ) * model_output\n",
    "        else:\n",
    "            raise ValueError(\n",
    "                f\"prediction_type given as {self.config.prediction_type} must be one of `epsilon`, `sample` or\"\n",
    "                \" `v_prediction`  for the DDPMScheduler.\"\n",
    "            )\n",
    "\n",
    "        # 3. Clip or threshold \"predicted x_0\"\n",
    "        if self.config.thresholding:\n",
    "            pred_original_sample = self._threshold_sample(pred_original_sample)\n",
    "        elif self.config.clip_sample:\n",
    "            pred_original_sample = pred_original_sample.clamp(\n",
    "                -self.config.clip_sample_range, self.config.clip_sample_range\n",
    "            )\n",
    "\n",
    "        # 4. Compute coefficients for pred_original_sample x_0 and current sample x_t\n",
    "        # See formula (7) from https://arxiv.org/pdf/2006.11239.pdf\n",
    "        pred_original_sample_coeff = (\n",
    "            alpha_prod_t_prev ** (0.5) * current_beta_t\n",
    "        ) / beta_prod_t\n",
    "        current_sample_coeff = current_alpha_t ** (0.5) * beta_prod_t_prev / beta_prod_t\n",
    "\n",
    "        # 5. Compute predicted previous sample µ_t\n",
    "        # See formula (7) from https://arxiv.org/pdf/2006.11239.pdf\n",
    "        pred_prev_sample = (\n",
    "            pred_original_sample_coeff * pred_original_sample\n",
    "            + current_sample_coeff * sample\n",
    "        )\n",
    "\n",
    "        # 6. Add noise\n",
    "        variance = 0\n",
    "        if t > 0:\n",
    "            device = model_output.device\n",
    "            variance_noise = randn_tensor(\n",
    "                model_output.shape,\n",
    "                generator=generator,\n",
    "                device=device,\n",
    "                dtype=model_output.dtype,\n",
    "            )\n",
    "            if self.variance_type == \"fixed_small_log\":\n",
    "                variance = (\n",
    "                    self._get_variance(t, predicted_variance=predicted_variance)\n",
    "                    * variance_noise\n",
    "                )\n",
    "            elif self.variance_type == \"learned_range\":\n",
    "                variance = self._get_variance(t, predicted_variance=predicted_variance)\n",
    "                variance = torch.exp(0.5 * variance) * variance_noise\n",
    "            else:\n",
    "                variance = (\n",
    "                    self._get_variance(t, predicted_variance=predicted_variance) ** 0.5\n",
    "                ) * variance_noise\n",
    "\n",
    "        pred_prev_sample = pred_prev_sample + variance\n",
    "\n",
    "        if not return_dict:\n",
    "            return (\n",
    "                pred_prev_sample,\n",
    "                pred_original_sample,\n",
    "            )\n",
    "\n",
    "        return DDPMSchedulerOutput(\n",
    "            prev_sample=pred_prev_sample, pred_original_sample=pred_original_sample\n",
    "        )\n",
    "\n",
    "    def add_noise(\n",
    "        self,\n",
    "        original_samples: torch.Tensor,\n",
    "        noise: torch.Tensor,\n",
    "        timesteps: torch.IntTensor,\n",
    "    ) -> torch.Tensor:\n",
    "        # Make sure alphas_cumprod and timestep have same device and dtype as original_samples\n",
    "        # Move the self.alphas_cumprod to device to avoid redundant CPU to GPU data movement\n",
    "        # for the subsequent add_noise calls\n",
    "        self.alphas_cumprod = self.alphas_cumprod.to(device=original_samples.device)\n",
    "        alphas_cumprod = self.alphas_cumprod.to(dtype=original_samples.dtype)\n",
    "        timesteps = timesteps.to(original_samples.device)\n",
    "\n",
    "        sqrt_alpha_prod = alphas_cumprod[timesteps] ** 0.5\n",
    "        sqrt_alpha_prod = sqrt_alpha_prod.flatten()\n",
    "        while len(sqrt_alpha_prod.shape) < len(original_samples.shape):\n",
    "            sqrt_alpha_prod = sqrt_alpha_prod.unsqueeze(-1)\n",
    "\n",
    "        sqrt_one_minus_alpha_prod = (1 - alphas_cumprod[timesteps]) ** 0.5\n",
    "        sqrt_one_minus_alpha_prod = sqrt_one_minus_alpha_prod.flatten()\n",
    "        while len(sqrt_one_minus_alpha_prod.shape) < len(original_samples.shape):\n",
    "            sqrt_one_minus_alpha_prod = sqrt_one_minus_alpha_prod.unsqueeze(-1)\n",
    "\n",
    "        noisy_samples = (\n",
    "            sqrt_alpha_prod * original_samples + sqrt_one_minus_alpha_prod * noise\n",
    "        )\n",
    "        return noisy_samples\n",
    "\n",
    "    def get_velocity(\n",
    "        self, sample: torch.Tensor, noise: torch.Tensor, timesteps: torch.IntTensor\n",
    "    ) -> torch.Tensor:\n",
    "        # Make sure alphas_cumprod and timestep have same device and dtype as sample\n",
    "        self.alphas_cumprod = self.alphas_cumprod.to(device=sample.device)\n",
    "        alphas_cumprod = self.alphas_cumprod.to(dtype=sample.dtype)\n",
    "        timesteps = timesteps.to(sample.device)\n",
    "\n",
    "        sqrt_alpha_prod = alphas_cumprod[timesteps] ** 0.5\n",
    "        sqrt_alpha_prod = sqrt_alpha_prod.flatten()\n",
    "        while len(sqrt_alpha_prod.shape) < len(sample.shape):\n",
    "            sqrt_alpha_prod = sqrt_alpha_prod.unsqueeze(-1)\n",
    "\n",
    "        sqrt_one_minus_alpha_prod = (1 - alphas_cumprod[timesteps]) ** 0.5\n",
    "        sqrt_one_minus_alpha_prod = sqrt_one_minus_alpha_prod.flatten()\n",
    "        while len(sqrt_one_minus_alpha_prod.shape) < len(sample.shape):\n",
    "            sqrt_one_minus_alpha_prod = sqrt_one_minus_alpha_prod.unsqueeze(-1)\n",
    "\n",
    "        velocity = sqrt_alpha_prod * noise - sqrt_one_minus_alpha_prod * sample\n",
    "        return velocity\n",
    "\n",
    "    def __len__(self):\n",
    "        return self.config.num_train_timesteps\n",
    "\n",
    "    def previous_timestep(self, timestep):\n",
    "        if self.custom_timesteps or self.num_inference_steps:\n",
    "            index = (self.timesteps == timestep).nonzero(as_tuple=True)[0][0]\n",
    "            if index == self.timesteps.shape[0] - 1:\n",
    "                prev_t = torch.tensor(-1)\n",
    "            else:\n",
    "                prev_t = self.timesteps[index + 1]\n",
    "        else:\n",
    "            prev_t = timestep - 1\n",
    "        return prev_t\n",
    "\n",
    "\n",
    "class DDPMPipeline(DiffusionPipeline):\n",
    "    r\"\"\"\n",
    "    ok Pipeline for image generation.\n",
    "\n",
    "    This model inherits from [`DiffusionPipeline`]. Check the superclass documentation for the generic methods\n",
    "    implemented for all pipelines (downloading, saving, running on a particular device, etc.).\n",
    "\n",
    "    Parameters:\n",
    "        unet ([`UNet2DModel`]):\n",
    "            A `UNet2DModel` to denoise the encoded image latents.\n",
    "        scheduler ([`SchedulerMixin`]):\n",
    "            A scheduler to be used in combination with `unet` to denoise the encoded image. Can be one of\n",
    "            [`DDPMScheduler`], or [`DDIMScheduler`].\n",
    "    \"\"\"\n",
    "\n",
    "    model_cpu_offload_seq = \"unet\"\n",
    "\n",
    "    def __init__(self, unet: UNet2DModel, scheduler: DDPMScheduler):\n",
    "        super().__init__()\n",
    "        self.register_modules(unet=unet, scheduler=scheduler)\n",
    "\n",
    "    @torch.no_grad()\n",
    "    def __call__(\n",
    "        self,\n",
    "        batch_size: int = 1,\n",
    "        generator: Optional[Union[torch.Generator, List[torch.Generator]]] = None,\n",
    "        num_inference_steps: int = 1000,\n",
    "        output_type: Optional[str] = \"pil\",\n",
    "        return_dict: bool = True,\n",
    "    ) -> Union[ImagePipelineOutput, Tuple]:\n",
    "        r\"\"\"\n",
    "        The call function to the pipeline for generation.\n",
    "\n",
    "        Args:\n",
    "            batch_size (`int`, *optional*, defaults to 1):\n",
    "                The number of images to generate.\n",
    "            generator (`torch.Generator`, *optional*):\n",
    "                A [`torch.Generator`](https://pytorch.org/docs/stable/generated/torch.Generator.html) to make\n",
    "                generation deterministic.\n",
    "            num_inference_steps (`int`, *optional*, defaults to 1000):\n",
    "                The number of denoising steps. More denoising steps usually lead to a higher quality image at the\n",
    "                expense of slower inference.\n",
    "            output_type (`str`, *optional*, defaults to `\"pil\"`):\n",
    "                The output format of the generated image. Choose between `PIL.Image` or `np.array`.\n",
    "            return_dict (`bool`, *optional*, defaults to `True`):\n",
    "                Whether or not to return a [`~pipelines.ImagePipelineOutput`] instead of a plain tuple.\n",
    "\n",
    "        Example:\n",
    "\n",
    "        ```py\n",
    "        >>> from diffusers import DDPMPipeline\n",
    "\n",
    "        >>> # load model and scheduler\n",
    "        >>> pipe = DDPMPipeline.from_pretrained(\"google/ddpm-cat-256\")\n",
    "\n",
    "        >>> # run pipeline in inference (sample random noise and denoise)\n",
    "        >>> image = pipe().images[0]\n",
    "\n",
    "        >>> # save image\n",
    "        >>> image.save(\"ddpm_generated_image.png\")\n",
    "        ```\n",
    "\n",
    "        Returns:\n",
    "            [`~pipelines.ImagePipelineOutput`] or `tuple`:\n",
    "                If `return_dict` is `True`, [`~pipelines.ImagePipelineOutput`] is returned, otherwise a `tuple` is\n",
    "                returned where the first element is a list with the generated images\n",
    "        \"\"\"\n",
    "        # Sample gaussian noise to begin loop\n",
    "        if isinstance(self.unet.config.sample_size, int):\n",
    "            image_shape = (\n",
    "                batch_size,\n",
    "                self.unet.config.in_channels,\n",
    "                self.unet.config.sample_size,\n",
    "                self.unet.config.sample_size,\n",
    "            )\n",
    "        else:\n",
    "            image_shape = (\n",
    "                batch_size,\n",
    "                self.unet.config.in_channels,\n",
    "                *self.unet.config.sample_size,\n",
    "            )\n",
    "\n",
    "        if self.device.type == \"mps\":\n",
    "            # randn does not work reproducibly on mps\n",
    "            image = randn_tensor(\n",
    "                image_shape, generator=generator, dtype=self.unet.dtype\n",
    "            )\n",
    "            image = image.to(self.device)\n",
    "        else:\n",
    "            image = randn_tensor(\n",
    "                image_shape,\n",
    "                generator=generator,\n",
    "                device=self.device,\n",
    "                dtype=self.unet.dtype,\n",
    "            )\n",
    "\n",
    "        # set step values\n",
    "        self.scheduler.set_timesteps(num_inference_steps)\n",
    "\n",
    "        for t in self.progress_bar(self.scheduler.timesteps):\n",
    "            # 1. predict noise model_output\n",
    "            model_output = self.unet(image, t).sample\n",
    "\n",
    "            # 2. compute previous image: x_t -> x_t-1\n",
    "            image = self.scheduler.step(\n",
    "                model_output, t, image, generator=generator\n",
    "            ).prev_sample\n",
    "\n",
    "        image = (image / 2 + 0.5).clamp(0, 1)\n",
    "        image = image.cpu().permute(0, 2, 3, 1).numpy()\n",
    "        if output_type == \"pil\":\n",
    "            image = self.numpy_to_pil(image)\n",
    "\n",
    "        if not return_dict:\n",
    "            return (image,)\n",
    "\n",
    "        return ImagePipelineOutput(images=image)\n",
    "\n",
    "\n",
    "class AdaGroupNorm(nn.Module):\n",
    "    r\"\"\"\n",
    "    ok GroupNorm layer modified to incorporate timestep embeddings.\n",
    "\n",
    "    Parameters:\n",
    "        embedding_dim (`int`): The size of each embedding vector.\n",
    "        num_embeddings (`int`): The size of the embeddings dictionary.\n",
    "        num_groups (`int`): The number of groups to separate the channels into.\n",
    "        act_fn (`str`, *optional*, defaults to `None`): The activation function to use.\n",
    "        eps (`float`, *optional*, defaults to `1e-5`): The epsilon value to use for numerical stability.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(\n",
    "        self,\n",
    "        embedding_dim: int,\n",
    "        out_dim: int,\n",
    "        num_groups: int,\n",
    "        act_fn: Optional[str] = None,\n",
    "        eps: float = 1e-5,\n",
    "    ):\n",
    "        super().__init__()\n",
    "        self.num_groups = num_groups\n",
    "        self.eps = eps\n",
    "\n",
    "        if act_fn is None:\n",
    "            self.act = None\n",
    "        else:\n",
    "            self.act = get_activation(act_fn)\n",
    "\n",
    "        self.linear = nn.Linear(embedding_dim, out_dim * 2)\n",
    "\n",
    "    def forward(self, x: torch.Tensor, emb: torch.Tensor) -> torch.Tensor:\n",
    "        if self.act:\n",
    "            emb = self.act(emb)\n",
    "        emb = self.linear(emb)\n",
    "        emb = emb[:, :, None, None]\n",
    "        scale, shift = emb.chunk(2, dim=1)\n",
    "\n",
    "        x = F.group_norm(x, self.num_groups, eps=self.eps)\n",
    "        x = x * (1 + scale) + shift\n",
    "        return x\n",
    "\n",
    "\n",
    "class SpatialNorm(nn.Module):\n",
    "    \"\"\"\n",
    "    ok Spatially conditioned normalization as defined in https://arxiv.org/abs/2209.09002.\n",
    "\n",
    "    Args:\n",
    "        f_channels (`int`):\n",
    "            The number of channels for input to group normalization layer, and output of the spatial norm layer.\n",
    "        zq_channels (`int`):\n",
    "            The number of channels for the quantized vector as described in the paper.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(\n",
    "        self,\n",
    "        f_channels: int,\n",
    "        zq_channels: int,\n",
    "    ):\n",
    "        super().__init__()\n",
    "        self.norm_layer = nn.GroupNorm(\n",
    "            num_channels=f_channels, num_groups=32, eps=1e-6, affine=True\n",
    "        )\n",
    "        self.conv_y = nn.Conv2d(\n",
    "            zq_channels, f_channels, kernel_size=1, stride=1, padding=0\n",
    "        )\n",
    "        self.conv_b = nn.Conv2d(\n",
    "            zq_channels, f_channels, kernel_size=1, stride=1, padding=0\n",
    "        )\n",
    "\n",
    "    def forward(self, f: torch.Tensor, zq: torch.Tensor) -> torch.Tensor:\n",
    "        f_size = f.shape[-2:]\n",
    "        zq = F.interpolate(zq, size=f_size, mode=\"nearest\")\n",
    "        norm_f = self.norm_layer(f)\n",
    "        new_f = norm_f * self.conv_y(zq) + self.conv_b(zq)\n",
    "        return new_f\n",
    "\n",
    "\n",
    "class ResnetBlockCondNorm2D(nn.Module):\n",
    "    r\"\"\"\n",
    "    ok\n",
    "    A Resnet block that use normalization layer that incorporate conditioning information.\n",
    "\n",
    "    Parameters:\n",
    "        in_channels (`int`): The number of channels in the input.\n",
    "        out_channels (`int`, *optional*, default to be `None`):\n",
    "            The number of output channels for the first conv2d layer. If None, same as `in_channels`.\n",
    "        dropout (`float`, *optional*, defaults to `0.0`): The dropout probability to use.\n",
    "        temb_channels (`int`, *optional*, default to `512`): the number of channels in timestep embedding.\n",
    "        groups (`int`, *optional*, default to `32`): The number of groups to use for the first normalization layer.\n",
    "        groups_out (`int`, *optional*, default to None):\n",
    "            The number of groups to use for the second normalization layer. if set to None, same as `groups`.\n",
    "        eps (`float`, *optional*, defaults to `1e-6`): The epsilon to use for the normalization.\n",
    "        non_linearity (`str`, *optional*, default to `\"swish\"`): the activation function to use.\n",
    "        time_embedding_norm (`str`, *optional*, default to `\"ada_group\"` ):\n",
    "            The normalization layer for time embedding `temb`. Currently only support \"ada_group\" or \"spatial\".\n",
    "        kernel (`torch.Tensor`, optional, default to None): FIR filter, see\n",
    "            [`~models.resnet.FirUpsample2D`] and [`~models.resnet.FirDownsample2D`].\n",
    "        output_scale_factor (`float`, *optional*, default to be `1.0`): the scale factor to use for the output.\n",
    "        use_in_shortcut (`bool`, *optional*, default to `True`):\n",
    "            If `True`, add a 1x1 nn.conv2d layer for skip-connection.\n",
    "        up (`bool`, *optional*, default to `False`): If `True`, add an upsample layer.\n",
    "        down (`bool`, *optional*, default to `False`): If `True`, add a downsample layer.\n",
    "        conv_shortcut_bias (`bool`, *optional*, default to `True`):  If `True`, adds a learnable bias to the\n",
    "            `conv_shortcut` output.\n",
    "        conv_2d_out_channels (`int`, *optional*, default to `None`): the number of channels in the output.\n",
    "            If None, same as `out_channels`.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(\n",
    "        self,\n",
    "        *,\n",
    "        in_channels: int,\n",
    "        out_channels: Optional[int] = None,\n",
    "        conv_shortcut: bool = False,\n",
    "        dropout: float = 0.0,\n",
    "        temb_channels: int = 512,\n",
    "        groups: int = 32,\n",
    "        groups_out: Optional[int] = None,\n",
    "        eps: float = 1e-6,\n",
    "        non_linearity: str = \"swish\",\n",
    "        time_embedding_norm: str = \"ada_group\",  # ada_group, spatial\n",
    "        output_scale_factor: float = 1.0,\n",
    "        use_in_shortcut: Optional[bool] = None,\n",
    "        up: bool = False,\n",
    "        down: bool = False,\n",
    "        conv_shortcut_bias: bool = True,\n",
    "        conv_2d_out_channels: Optional[int] = None,\n",
    "    ):\n",
    "        super().__init__()\n",
    "        self.in_channels = in_channels\n",
    "        out_channels = in_channels if out_channels is None else out_channels\n",
    "        self.out_channels = out_channels\n",
    "        self.use_conv_shortcut = conv_shortcut\n",
    "        self.up = up\n",
    "        self.down = down\n",
    "        self.output_scale_factor = output_scale_factor\n",
    "        self.time_embedding_norm = time_embedding_norm\n",
    "\n",
    "        if groups_out is None:\n",
    "            groups_out = groups\n",
    "\n",
    "        if self.time_embedding_norm == \"ada_group\":  # ada_group\n",
    "            self.norm1 = AdaGroupNorm(temb_channels, in_channels, groups, eps=eps)\n",
    "        elif self.time_embedding_norm == \"spatial\":\n",
    "            self.norm1 = SpatialNorm(in_channels, temb_channels)\n",
    "        else:\n",
    "            raise ValueError(\n",
    "                f\" unsupported time_embedding_norm: {self.time_embedding_norm}\"\n",
    "            )\n",
    "\n",
    "        self.conv1 = nn.Conv2d(\n",
    "            in_channels, out_channels, kernel_size=3, stride=1, padding=1\n",
    "        )\n",
    "\n",
    "        if self.time_embedding_norm == \"ada_group\":  # ada_group\n",
    "            self.norm2 = AdaGroupNorm(temb_channels, out_channels, groups_out, eps=eps)\n",
    "        elif self.time_embedding_norm == \"spatial\":  # spatial\n",
    "            self.norm2 = SpatialNorm(out_channels, temb_channels)\n",
    "        else:\n",
    "            raise ValueError(\n",
    "                f\" unsupported time_embedding_norm: {self.time_embedding_norm}\"\n",
    "            )\n",
    "\n",
    "        self.dropout = torch.nn.Dropout(dropout)\n",
    "\n",
    "        conv_2d_out_channels = conv_2d_out_channels or out_channels\n",
    "        self.conv2 = nn.Conv2d(\n",
    "            out_channels, conv_2d_out_channels, kernel_size=3, stride=1, padding=1\n",
    "        )\n",
    "\n",
    "        self.nonlinearity = get_activation(non_linearity)\n",
    "\n",
    "        self.upsample = self.downsample = None\n",
    "        if self.up:\n",
    "            self.upsample = Upsample2D(in_channels, use_conv=False)\n",
    "        elif self.down:\n",
    "            self.downsample = Downsample2D(\n",
    "                in_channels, use_conv=False, padding=1, name=\"op\"\n",
    "            )\n",
    "\n",
    "        self.use_in_shortcut = (\n",
    "            self.in_channels != conv_2d_out_channels\n",
    "            if use_in_shortcut is None\n",
    "            else use_in_shortcut\n",
    "        )\n",
    "\n",
    "        self.conv_shortcut = None\n",
    "        if self.use_in_shortcut:\n",
    "            self.conv_shortcut = nn.Conv2d(\n",
    "                in_channels,\n",
    "                conv_2d_out_channels,\n",
    "                kernel_size=1,\n",
    "                stride=1,\n",
    "                padding=0,\n",
    "                bias=conv_shortcut_bias,\n",
    "            )\n",
    "\n",
    "    def forward(\n",
    "        self, input_tensor: torch.Tensor, temb: torch.Tensor, *args, **kwargs\n",
    "    ) -> torch.Tensor:\n",
    "        if len(args) > 0 or kwargs.get(\"scale\", None) is not None:\n",
    "            deprecation_message = \"The `scale` argument is deprecated and will be ignored. Please remove it, as passing it will raise an error in the future. `scale` should directly be passed while calling the underlying pipeline component i.e., via `cross_attention_kwargs`.\"\n",
    "            # deprecate(\"scale\", \"1.0.0\", deprecation_message)\n",
    "\n",
    "        hidden_states = input_tensor\n",
    "\n",
    "        hidden_states = self.norm1(hidden_states, temb)\n",
    "\n",
    "        hidden_states = self.nonlinearity(hidden_states)\n",
    "\n",
    "        if self.upsample is not None:\n",
    "            # upsample_nearest_nhwc fails with large batch sizes. see https://github.com/huggingface/diffusers/issues/984\n",
    "            if hidden_states.shape[0] >= 64:\n",
    "                input_tensor = input_tensor.contiguous()\n",
    "                hidden_states = hidden_states.contiguous()\n",
    "            input_tensor = self.upsample(input_tensor)\n",
    "            hidden_states = self.upsample(hidden_states)\n",
    "\n",
    "        elif self.downsample is not None:\n",
    "            input_tensor = self.downsample(input_tensor)\n",
    "            hidden_states = self.downsample(hidden_states)\n",
    "\n",
    "        hidden_states = self.conv1(hidden_states)\n",
    "\n",
    "        hidden_states = self.norm2(hidden_states, temb)\n",
    "\n",
    "        hidden_states = self.nonlinearity(hidden_states)\n",
    "\n",
    "        hidden_states = self.dropout(hidden_states)\n",
    "        hidden_states = self.conv2(hidden_states)\n",
    "\n",
    "        if self.conv_shortcut is not None:\n",
    "            input_tensor = self.conv_shortcut(input_tensor)\n",
    "\n",
    "        output_tensor = (input_tensor + hidden_states) / self.output_scale_factor\n",
    "\n",
    "        return output_tensor\n",
    "\n",
    "\n",
    "class RMSNorm(nn.Module):\n",
    "    r\"\"\"\n",
    "    ok RMS Norm as introduced in https://arxiv.org/abs/1910.07467 by Zhang et al.\n",
    "\n",
    "    Args:\n",
    "        dim (`int`): Number of dimensions to use for `weights`. Only effective when `elementwise_affine` is True.\n",
    "        eps (`float`): Small value to use when calculating the reciprocal of the square-root.\n",
    "        elementwise_affine (`bool`, defaults to `True`):\n",
    "            Boolean flag to denote if affine transformation should be applied.\n",
    "        bias (`bool`, defaults to False): If also training the `bias` param.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(\n",
    "        self, dim, eps: float, elementwise_affine: bool = True, bias: bool = False\n",
    "    ):\n",
    "        super().__init__()\n",
    "\n",
    "        self.eps = eps\n",
    "        self.elementwise_affine = elementwise_affine\n",
    "\n",
    "        if isinstance(dim, numbers.Integral):\n",
    "            dim = (dim,)\n",
    "\n",
    "        self.dim = torch.Size(dim)\n",
    "\n",
    "        self.weight = None\n",
    "        self.bias = None\n",
    "\n",
    "        if elementwise_affine:\n",
    "            self.weight = nn.Parameter(torch.ones(dim))\n",
    "            if bias:\n",
    "                self.bias = nn.Parameter(torch.zeros(dim))\n",
    "\n",
    "    def forward(self, hidden_states):\n",
    "        if is_torch_npu_available():\n",
    "            pass\n",
    "            # import torch_npu\n",
    "\n",
    "            # if self.weight is not None:\n",
    "            #     # convert into half-precision if necessary\n",
    "            #     if self.weight.dtype in [torch.float16, torch.bfloat16]:\n",
    "            #         hidden_states = hidden_states.to(self.weight.dtype)\n",
    "            # hidden_states = torch_npu.npu_rms_norm(\n",
    "            #     hidden_states, self.weight, epsilon=self.eps\n",
    "            # )[0]\n",
    "            # if self.bias is not None:\n",
    "            #     hidden_states = hidden_states + self.bias\n",
    "        else:\n",
    "            input_dtype = hidden_states.dtype\n",
    "            variance = hidden_states.to(torch.float32).pow(2).mean(-1, keepdim=True)\n",
    "            hidden_states = hidden_states * torch.rsqrt(variance + self.eps)\n",
    "\n",
    "            if self.weight is not None:\n",
    "                # convert into half-precision if necessary\n",
    "                if self.weight.dtype in [torch.float16, torch.bfloat16]:\n",
    "                    hidden_states = hidden_states.to(self.weight.dtype)\n",
    "                hidden_states = hidden_states * self.weight\n",
    "                if self.bias is not None:\n",
    "                    hidden_states = hidden_states + self.bias\n",
    "            else:\n",
    "                hidden_states = hidden_states.to(input_dtype)\n",
    "\n",
    "        return hidden_states\n",
    "\n",
    "\n",
    "class Upsample2D(nn.Module):\n",
    "    \"\"\"ok A 2D upsampling layer with an optional convolution.\n",
    "\n",
    "    Parameters:\n",
    "        channels (`int`):\n",
    "            number of channels in the inputs and outputs.\n",
    "        use_conv (`bool`, default `False`):\n",
    "            option to use a convolution.\n",
    "        use_conv_transpose (`bool`, default `False`):\n",
    "            option to use a convolution transpose.\n",
    "        out_channels (`int`, optional):\n",
    "            number of output channels. Defaults to `channels`.\n",
    "        name (`str`, default `conv`):\n",
    "            name of the upsampling 2D layer.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(\n",
    "        self,\n",
    "        channels: int,\n",
    "        use_conv: bool = False,\n",
    "        use_conv_transpose: bool = False,\n",
    "        out_channels: Optional[int] = None,\n",
    "        name: str = \"conv\",\n",
    "        kernel_size: Optional[int] = None,\n",
    "        padding=1,\n",
    "        norm_type=None,\n",
    "        eps=None,\n",
    "        elementwise_affine=None,\n",
    "        bias=True,\n",
    "        interpolate=True,\n",
    "    ):\n",
    "        super().__init__()\n",
    "        self.channels = channels\n",
    "        self.out_channels = out_channels or channels\n",
    "        self.use_conv = use_conv\n",
    "        self.use_conv_transpose = use_conv_transpose\n",
    "        self.name = name\n",
    "        self.interpolate = interpolate\n",
    "\n",
    "        if norm_type == \"ln_norm\":\n",
    "            self.norm = nn.LayerNorm(channels, eps, elementwise_affine)\n",
    "        elif norm_type == \"rms_norm\":\n",
    "            self.norm = RMSNorm(channels, eps, elementwise_affine)\n",
    "        elif norm_type is None:\n",
    "            self.norm = None\n",
    "        else:\n",
    "            raise ValueError(f\"unknown norm_type: {norm_type}\")\n",
    "\n",
    "        conv = None\n",
    "        if use_conv_transpose:\n",
    "            if kernel_size is None:\n",
    "                kernel_size = 4\n",
    "            conv = nn.ConvTranspose2d(\n",
    "                channels,\n",
    "                self.out_channels,\n",
    "                kernel_size=kernel_size,\n",
    "                stride=2,\n",
    "                padding=padding,\n",
    "                bias=bias,\n",
    "            )\n",
    "        elif use_conv:\n",
    "            if kernel_size is None:\n",
    "                kernel_size = 3\n",
    "            conv = nn.Conv2d(\n",
    "                self.channels,\n",
    "                self.out_channels,\n",
    "                kernel_size=kernel_size,\n",
    "                padding=padding,\n",
    "                bias=bias,\n",
    "            )\n",
    "\n",
    "        # TODO(Suraj, Patrick) - clean up after weight dicts are correctly renamed\n",
    "        if name == \"conv\":\n",
    "            self.conv = conv\n",
    "        else:\n",
    "            self.Conv2d_0 = conv\n",
    "\n",
    "    def forward(\n",
    "        self,\n",
    "        hidden_states: torch.Tensor,\n",
    "        output_size: Optional[int] = None,\n",
    "        *args,\n",
    "        **kwargs,\n",
    "    ) -> torch.Tensor:\n",
    "        if len(args) > 0 or kwargs.get(\"scale\", None) is not None:\n",
    "            deprecation_message = \"The `scale` argument is deprecated and will be ignored. Please remove it, as passing it will raise an error in the future. `scale` should directly be passed while calling the underlying pipeline component i.e., via `cross_attention_kwargs`.\"\n",
    "            deprecate(\"scale\", \"1.0.0\", deprecation_message)\n",
    "\n",
    "        assert hidden_states.shape[1] == self.channels\n",
    "\n",
    "        if self.norm is not None:\n",
    "            hidden_states = self.norm(hidden_states.permute(0, 2, 3, 1)).permute(\n",
    "                0, 3, 1, 2\n",
    "            )\n",
    "\n",
    "        if self.use_conv_transpose:\n",
    "            return self.conv(hidden_states)\n",
    "\n",
    "        # Cast to float32 to as 'upsample_nearest2d_out_frame' op does not support bfloat16 until PyTorch 2.1\n",
    "        # https://github.com/pytorch/pytorch/issues/86679#issuecomment-1783978767\n",
    "        dtype = hidden_states.dtype\n",
    "        if dtype == torch.bfloat16 and is_torch_version(\"<\", \"2.1\"):\n",
    "            hidden_states = hidden_states.to(torch.float32)\n",
    "\n",
    "        # upsample_nearest_nhwc fails with large batch sizes. see https://github.com/huggingface/diffusers/issues/984\n",
    "        if hidden_states.shape[0] >= 64:\n",
    "            hidden_states = hidden_states.contiguous()\n",
    "\n",
    "        # if `output_size` is passed we force the interpolation output\n",
    "        # size and do not make use of `scale_factor=2`\n",
    "        if self.interpolate:\n",
    "            # upsample_nearest_nhwc also fails when the number of output elements is large\n",
    "            # https://github.com/pytorch/pytorch/issues/141831\n",
    "            scale_factor = (\n",
    "                2\n",
    "                if output_size is None\n",
    "                else max([f / s for f, s in zip(output_size, hidden_states.shape[-2:])])\n",
    "            )\n",
    "            if hidden_states.numel() * scale_factor > pow(2, 31):\n",
    "                hidden_states = hidden_states.contiguous()\n",
    "\n",
    "            if output_size is None:\n",
    "                hidden_states = F.interpolate(\n",
    "                    hidden_states, scale_factor=2.0, mode=\"nearest\"\n",
    "                )\n",
    "            else:\n",
    "                hidden_states = F.interpolate(\n",
    "                    hidden_states, size=output_size, mode=\"nearest\"\n",
    "                )\n",
    "\n",
    "        # Cast back to original dtype\n",
    "        if dtype == torch.bfloat16 and is_torch_version(\"<\", \"2.1\"):\n",
    "            hidden_states = hidden_states.to(dtype)\n",
    "\n",
    "        # TODO(Suraj, Patrick) - clean up after weight dicts are correctly renamed\n",
    "        if self.use_conv:\n",
    "            if self.name == \"conv\":\n",
    "                hidden_states = self.conv(hidden_states)\n",
    "            else:\n",
    "                hidden_states = self.Conv2d_0(hidden_states)\n",
    "\n",
    "        return hidden_states\n",
    "\n",
    "\n",
    "class ResnetBlock2D(nn.Module):\n",
    "    r\"\"\"ok\n",
    "    A Resnet block.\n",
    "\n",
    "    Parameters:\n",
    "        in_channels (`int`): The number of channels in the input.\n",
    "        out_channels (`int`, *optional*, default to be `None`):\n",
    "            The number of output channels for the first conv2d layer. If None, same as `in_channels`.\n",
    "        dropout (`float`, *optional*, defaults to `0.0`): The dropout probability to use.\n",
    "        temb_channels (`int`, *optional*, default to `512`): the number of channels in timestep embedding.\n",
    "        groups (`int`, *optional*, default to `32`): The number of groups to use for the first normalization layer.\n",
    "        groups_out (`int`, *optional*, default to None):\n",
    "            The number of groups to use for the second normalization layer. if set to None, same as `groups`.\n",
    "        eps (`float`, *optional*, defaults to `1e-6`): The epsilon to use for the normalization.\n",
    "        non_linearity (`str`, *optional*, default to `\"swish\"`): the activation function to use.\n",
    "        time_embedding_norm (`str`, *optional*, default to `\"default\"` ): Time scale shift config.\n",
    "            By default, apply timestep embedding conditioning with a simple shift mechanism. Choose \"scale_shift\" for a\n",
    "            stronger conditioning with scale and shift.\n",
    "        kernel (`torch.Tensor`, optional, default to None): FIR filter, see\n",
    "            [`~models.resnet.FirUpsample2D`] and [`~models.resnet.FirDownsample2D`].\n",
    "        output_scale_factor (`float`, *optional*, default to be `1.0`): the scale factor to use for the output.\n",
    "        use_in_shortcut (`bool`, *optional*, default to `True`):\n",
    "            If `True`, add a 1x1 nn.conv2d layer for skip-connection.\n",
    "        up (`bool`, *optional*, default to `False`): If `True`, add an upsample layer.\n",
    "        down (`bool`, *optional*, default to `False`): If `True`, add a downsample layer.\n",
    "        conv_shortcut_bias (`bool`, *optional*, default to `True`):  If `True`, adds a learnable bias to the\n",
    "            `conv_shortcut` output.\n",
    "        conv_2d_out_channels (`int`, *optional*, default to `None`): the number of channels in the output.\n",
    "            If None, same as `out_channels`.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(\n",
    "        self,\n",
    "        *,\n",
    "        in_channels: int,\n",
    "        out_channels: Optional[int] = None,\n",
    "        conv_shortcut: bool = False,\n",
    "        dropout: float = 0.0,\n",
    "        temb_channels: int = 512,\n",
    "        groups: int = 32,\n",
    "        groups_out: Optional[int] = None,\n",
    "        pre_norm: bool = True,\n",
    "        eps: float = 1e-6,\n",
    "        non_linearity: str = \"swish\",\n",
    "        skip_time_act: bool = False,\n",
    "        time_embedding_norm: str = \"default\",  # default, scale_shift,\n",
    "        kernel: Optional[torch.Tensor] = None,\n",
    "        output_scale_factor: float = 1.0,\n",
    "        use_in_shortcut: Optional[bool] = None,\n",
    "        up: bool = False,\n",
    "        down: bool = False,\n",
    "        conv_shortcut_bias: bool = True,\n",
    "        conv_2d_out_channels: Optional[int] = None,\n",
    "    ):\n",
    "        super().__init__()\n",
    "        if time_embedding_norm == \"ada_group\":\n",
    "            raise ValueError(\n",
    "                \"This class cannot be used with `time_embedding_norm==ada_group`, please use `ResnetBlockCondNorm2D` instead\",\n",
    "            )\n",
    "        if time_embedding_norm == \"spatial\":\n",
    "            raise ValueError(\n",
    "                \"This class cannot be used with `time_embedding_norm==spatial`, please use `ResnetBlockCondNorm2D` instead\",\n",
    "            )\n",
    "\n",
    "        self.pre_norm = True\n",
    "        self.in_channels = in_channels\n",
    "        out_channels = in_channels if out_channels is None else out_channels\n",
    "        self.out_channels = out_channels\n",
    "        self.use_conv_shortcut = conv_shortcut\n",
    "        self.up = up\n",
    "        self.down = down\n",
    "        self.output_scale_factor = output_scale_factor\n",
    "        self.time_embedding_norm = time_embedding_norm\n",
    "        self.skip_time_act = skip_time_act\n",
    "\n",
    "        if groups_out is None:\n",
    "            groups_out = groups\n",
    "\n",
    "        self.norm1 = torch.nn.GroupNorm(\n",
    "            num_groups=groups, num_channels=in_channels, eps=eps, affine=True\n",
    "        )\n",
    "\n",
    "        self.conv1 = nn.Conv2d(\n",
    "            in_channels, out_channels, kernel_size=3, stride=1, padding=1\n",
    "        )\n",
    "\n",
    "        if temb_channels is not None:\n",
    "            if self.time_embedding_norm == \"default\":\n",
    "                self.time_emb_proj = nn.Linear(temb_channels, out_channels)\n",
    "            elif self.time_embedding_norm == \"scale_shift\":\n",
    "                self.time_emb_proj = nn.Linear(temb_channels, 2 * out_channels)\n",
    "            else:\n",
    "                raise ValueError(\n",
    "                    f\"unknown time_embedding_norm : {self.time_embedding_norm} \"\n",
    "                )\n",
    "        else:\n",
    "            self.time_emb_proj = None\n",
    "\n",
    "        self.norm2 = torch.nn.GroupNorm(\n",
    "            num_groups=groups_out, num_channels=out_channels, eps=eps, affine=True\n",
    "        )\n",
    "\n",
    "        self.dropout = torch.nn.Dropout(dropout)\n",
    "        conv_2d_out_channels = conv_2d_out_channels or out_channels\n",
    "        self.conv2 = nn.Conv2d(\n",
    "            out_channels, conv_2d_out_channels, kernel_size=3, stride=1, padding=1\n",
    "        )\n",
    "\n",
    "        self.nonlinearity = get_activation(non_linearity)\n",
    "\n",
    "        self.upsample = self.downsample = None\n",
    "        if self.up:\n",
    "            if kernel == \"fir\":\n",
    "                fir_kernel = (1, 3, 3, 1)\n",
    "                self.upsample = lambda x: upsample_2d(x, kernel=fir_kernel)\n",
    "            elif kernel == \"sde_vp\":\n",
    "                self.upsample = partial(F.interpolate, scale_factor=2.0, mode=\"nearest\")\n",
    "            else:\n",
    "                self.upsample = Upsample2D(in_channels, use_conv=False)\n",
    "        elif self.down:\n",
    "            if kernel == \"fir\":\n",
    "                fir_kernel = (1, 3, 3, 1)\n",
    "                self.downsample = lambda x: downsample_2d(x, kernel=fir_kernel)\n",
    "            elif kernel == \"sde_vp\":\n",
    "                self.downsample = partial(F.avg_pool2d, kernel_size=2, stride=2)\n",
    "            else:\n",
    "                self.downsample = Downsample2D(\n",
    "                    in_channels, use_conv=False, padding=1, name=\"op\"\n",
    "                )\n",
    "\n",
    "        self.use_in_shortcut = (\n",
    "            self.in_channels != conv_2d_out_channels\n",
    "            if use_in_shortcut is None\n",
    "            else use_in_shortcut\n",
    "        )\n",
    "\n",
    "        self.conv_shortcut = None\n",
    "        if self.use_in_shortcut:\n",
    "            self.conv_shortcut = nn.Conv2d(\n",
    "                in_channels,\n",
    "                conv_2d_out_channels,\n",
    "                kernel_size=1,\n",
    "                stride=1,\n",
    "                padding=0,\n",
    "                bias=conv_shortcut_bias,\n",
    "            )\n",
    "\n",
    "    def forward(\n",
    "        self, input_tensor: torch.Tensor, temb: torch.Tensor, *args, **kwargs\n",
    "    ) -> torch.Tensor:\n",
    "        if len(args) > 0 or kwargs.get(\"scale\", None) is not None:\n",
    "            deprecation_message = \"The `scale` argument is deprecated and will be ignored. Please remove it, as passing it will raise an error in the future. `scale` should directly be passed while calling the underlying pipeline component i.e., via `cross_attention_kwargs`.\"\n",
    "            deprecate(\"scale\", \"1.0.0\", deprecation_message)\n",
    "\n",
    "        hidden_states = input_tensor\n",
    "\n",
    "        hidden_states = self.norm1(hidden_states)\n",
    "        hidden_states = self.nonlinearity(hidden_states)\n",
    "\n",
    "        if self.upsample is not None:\n",
    "            # upsample_nearest_nhwc fails with large batch sizes. see https://github.com/huggingface/diffusers/issues/984\n",
    "            if hidden_states.shape[0] >= 64:\n",
    "                input_tensor = input_tensor.contiguous()\n",
    "                hidden_states = hidden_states.contiguous()\n",
    "            input_tensor = self.upsample(input_tensor)\n",
    "            hidden_states = self.upsample(hidden_states)\n",
    "        elif self.downsample is not None:\n",
    "            input_tensor = self.downsample(input_tensor)\n",
    "            hidden_states = self.downsample(hidden_states)\n",
    "\n",
    "        hidden_states = self.conv1(hidden_states)\n",
    "\n",
    "        if self.time_emb_proj is not None:\n",
    "            if not self.skip_time_act:\n",
    "                temb = self.nonlinearity(temb)\n",
    "            temb = self.time_emb_proj(temb)[:, :, None, None]\n",
    "\n",
    "        if self.time_embedding_norm == \"default\":\n",
    "            if temb is not None:\n",
    "                hidden_states = hidden_states + temb\n",
    "            hidden_states = self.norm2(hidden_states)\n",
    "        elif self.time_embedding_norm == \"scale_shift\":\n",
    "            if temb is None:\n",
    "                raise ValueError(\n",
    "                    f\" `temb` should not be None when `time_embedding_norm` is {self.time_embedding_norm}\"\n",
    "                )\n",
    "            time_scale, time_shift = torch.chunk(temb, 2, dim=1)\n",
    "            hidden_states = self.norm2(hidden_states)\n",
    "            hidden_states = hidden_states * (1 + time_scale) + time_shift\n",
    "        else:\n",
    "            hidden_states = self.norm2(hidden_states)\n",
    "\n",
    "        hidden_states = self.nonlinearity(hidden_states)\n",
    "\n",
    "        hidden_states = self.dropout(hidden_states)\n",
    "        hidden_states = self.conv2(hidden_states)\n",
    "\n",
    "        if self.conv_shortcut is not None:\n",
    "            input_tensor = self.conv_shortcut(input_tensor.contiguous())\n",
    "\n",
    "        output_tensor = (input_tensor + hidden_states) / self.output_scale_factor\n",
    "\n",
    "        return output_tensor\n",
    "\n",
    "\n",
    "class Downsample2D(nn.Module):\n",
    "    \"\"\"ok A 2D downsampling layer with an optional convolution.\n",
    "\n",
    "    Parameters:\n",
    "        channels (`int`):\n",
    "            number of channels in the inputs and outputs.\n",
    "        use_conv (`bool`, default `False`):\n",
    "            option to use a convolution.\n",
    "        out_channels (`int`, optional):\n",
    "            number of output channels. Defaults to `channels`.\n",
    "        padding (`int`, default `1`):\n",
    "            padding for the convolution.\n",
    "        name (`str`, default `conv`):\n",
    "            name of the downsampling 2D layer.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(\n",
    "        self,\n",
    "        channels: int,\n",
    "        use_conv: bool = False,\n",
    "        out_channels: Optional[int] = None,\n",
    "        padding: int = 1,\n",
    "        name: str = \"conv\",\n",
    "        kernel_size=3,\n",
    "        norm_type=None,\n",
    "        eps=None,\n",
    "        elementwise_affine=None,\n",
    "        bias=True,\n",
    "    ):\n",
    "        super().__init__()\n",
    "        self.channels = channels\n",
    "        self.out_channels = out_channels or channels\n",
    "        self.use_conv = use_conv\n",
    "        self.padding = padding\n",
    "        stride = 2\n",
    "        self.name = name\n",
    "\n",
    "        if norm_type == \"ln_norm\":\n",
    "            self.norm = nn.LayerNorm(channels, eps, elementwise_affine)\n",
    "        elif norm_type == \"rms_norm\":\n",
    "            self.norm = RMSNorm(channels, eps, elementwise_affine)\n",
    "        elif norm_type is None:\n",
    "            self.norm = None\n",
    "        else:\n",
    "            raise ValueError(f\"unknown norm_type: {norm_type}\")\n",
    "\n",
    "        if use_conv:\n",
    "            conv = nn.Conv2d(\n",
    "                self.channels,\n",
    "                self.out_channels,\n",
    "                kernel_size=kernel_size,\n",
    "                stride=stride,\n",
    "                padding=padding,\n",
    "                bias=bias,\n",
    "            )\n",
    "        else:\n",
    "            assert self.channels == self.out_channels\n",
    "            conv = nn.AvgPool2d(kernel_size=stride, stride=stride)\n",
    "\n",
    "        # TODO(Suraj, Patrick) - clean up after weight dicts are correctly renamed\n",
    "        if name == \"conv\":\n",
    "            self.Conv2d_0 = conv\n",
    "            self.conv = conv\n",
    "        elif name == \"Conv2d_0\":\n",
    "            self.conv = conv\n",
    "        else:\n",
    "            self.conv = conv\n",
    "\n",
    "    def forward(self, hidden_states: torch.Tensor, *args, **kwargs) -> torch.Tensor:\n",
    "        if len(args) > 0 or kwargs.get(\"scale\", None) is not None:\n",
    "            deprecation_message = \"The `scale` argument is deprecated and will be ignored. Please remove it, as passing it will raise an error in the future. `scale` should directly be passed while calling the underlying pipeline component i.e., via `cross_attention_kwargs`.\"\n",
    "            deprecate(\"scale\", \"1.0.0\", deprecation_message)\n",
    "        assert hidden_states.shape[1] == self.channels\n",
    "\n",
    "        if self.norm is not None:\n",
    "            hidden_states = self.norm(hidden_states.permute(0, 2, 3, 1)).permute(\n",
    "                0, 3, 1, 2\n",
    "            )\n",
    "\n",
    "        if self.use_conv and self.padding == 0:\n",
    "            pad = (0, 1, 0, 1)\n",
    "            hidden_states = F.pad(hidden_states, pad, mode=\"constant\", value=0)\n",
    "\n",
    "        assert hidden_states.shape[1] == self.channels\n",
    "\n",
    "        hidden_states = self.conv(hidden_states)\n",
    "\n",
    "        return hidden_states\n",
    "\n",
    "\n",
    "class UNetMidBlock2D(nn.Module):\n",
    "    \"\"\"\n",
    "    ok/attention A 2D UNet mid-block [`UNetMidBlock2D`] with multiple residual blocks and optional attention blocks.\n",
    "\n",
    "    Args:\n",
    "        in_channels (`int`): The number of input channels.\n",
    "        temb_channels (`int`): The number of temporal embedding channels.\n",
    "        dropout (`float`, *optional*, defaults to 0.0): The dropout rate.\n",
    "        num_layers (`int`, *optional*, defaults to 1): The number of residual blocks.\n",
    "        resnet_eps (`float`, *optional*, 1e-6 ): The epsilon value for the resnet blocks.\n",
    "        resnet_time_scale_shift (`str`, *optional*, defaults to `default`):\n",
    "            The type of normalization to apply to the time embeddings. This can help to improve the performance of the\n",
    "            model on tasks with long-range temporal dependencies.\n",
    "        resnet_act_fn (`str`, *optional*, defaults to `swish`): The activation function for the resnet blocks.\n",
    "        resnet_groups (`int`, *optional*, defaults to 32):\n",
    "            The number of groups to use in the group normalization layers of the resnet blocks.\n",
    "        attn_groups (`Optional[int]`, *optional*, defaults to None): The number of groups for the attention blocks.\n",
    "        resnet_pre_norm (`bool`, *optional*, defaults to `True`):\n",
    "            Whether to use pre-normalization for the resnet blocks.\n",
    "        add_attention (`bool`, *optional*, defaults to `True`): Whether to add attention blocks.\n",
    "        attention_head_dim (`int`, *optional*, defaults to 1):\n",
    "            Dimension of a single attention head. The number of attention heads is determined based on this value and\n",
    "            the number of input channels.\n",
    "        output_scale_factor (`float`, *optional*, defaults to 1.0): The output scale factor.\n",
    "\n",
    "    Returns:\n",
    "        `torch.Tensor`: The output of the last residual block, which is a tensor of shape `(batch_size, in_channels,\n",
    "        height, width)`.\n",
    "\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(\n",
    "        self,\n",
    "        in_channels: int,\n",
    "        temb_channels: int,\n",
    "        dropout: float = 0.0,\n",
    "        num_layers: int = 1,\n",
    "        resnet_eps: float = 1e-6,\n",
    "        resnet_time_scale_shift: str = \"default\",  # default, spatial\n",
    "        resnet_act_fn: str = \"swish\",\n",
    "        resnet_groups: int = 32,\n",
    "        attn_groups: Optional[int] = None,\n",
    "        resnet_pre_norm: bool = True,\n",
    "        add_attention: bool = True,\n",
    "        attention_head_dim: int = 1,\n",
    "        output_scale_factor: float = 1.0,\n",
    "    ):\n",
    "        super().__init__()\n",
    "        resnet_groups = (\n",
    "            resnet_groups if resnet_groups is not None else min(in_channels // 4, 32)\n",
    "        )\n",
    "        self.add_attention = add_attention\n",
    "\n",
    "        if attn_groups is None:\n",
    "            attn_groups = (\n",
    "                resnet_groups if resnet_time_scale_shift == \"default\" else None\n",
    "            )\n",
    "\n",
    "        # there is always at least one resnet\n",
    "        if resnet_time_scale_shift == \"spatial\":\n",
    "            resnets = [\n",
    "                ResnetBlockCondNorm2D(\n",
    "                    in_channels=in_channels,\n",
    "                    out_channels=in_channels,\n",
    "                    temb_channels=temb_channels,\n",
    "                    eps=resnet_eps,\n",
    "                    groups=resnet_groups,\n",
    "                    dropout=dropout,\n",
    "                    time_embedding_norm=\"spatial\",\n",
    "                    non_linearity=resnet_act_fn,\n",
    "                    output_scale_factor=output_scale_factor,\n",
    "                )\n",
    "            ]\n",
    "        else:\n",
    "            resnets = [\n",
    "                ResnetBlock2D(\n",
    "                    in_channels=in_channels,\n",
    "                    out_channels=in_channels,\n",
    "                    temb_channels=temb_channels,\n",
    "                    eps=resnet_eps,\n",
    "                    groups=resnet_groups,\n",
    "                    dropout=dropout,\n",
    "                    time_embedding_norm=resnet_time_scale_shift,\n",
    "                    non_linearity=resnet_act_fn,\n",
    "                    output_scale_factor=output_scale_factor,\n",
    "                    pre_norm=resnet_pre_norm,\n",
    "                )\n",
    "            ]\n",
    "        attentions = []\n",
    "\n",
    "        if attention_head_dim is None:\n",
    "            logger.warning(\n",
    "                f\"It is not recommend to pass `attention_head_dim=None`. Defaulting `attention_head_dim` to `in_channels`: {in_channels}.\"\n",
    "            )\n",
    "            attention_head_dim = in_channels\n",
    "\n",
    "        for _ in range(num_layers):\n",
    "            if self.add_attention:\n",
    "                attentions.append(\n",
    "                    Attention(\n",
    "                        in_channels,\n",
    "                        heads=in_channels // attention_head_dim,\n",
    "                        dim_head=attention_head_dim,\n",
    "                        rescale_output_factor=output_scale_factor,\n",
    "                        eps=resnet_eps,\n",
    "                        norm_num_groups=attn_groups,\n",
    "                        spatial_norm_dim=(\n",
    "                            temb_channels\n",
    "                            if resnet_time_scale_shift == \"spatial\"\n",
    "                            else None\n",
    "                        ),\n",
    "                        residual_connection=True,\n",
    "                        bias=True,\n",
    "                        upcast_softmax=True,\n",
    "                        _from_deprecated_attn_block=True,\n",
    "                    )\n",
    "                )\n",
    "            else:\n",
    "                attentions.append(None)\n",
    "\n",
    "            if resnet_time_scale_shift == \"spatial\":\n",
    "                resnets.append(\n",
    "                    ResnetBlockCondNorm2D(\n",
    "                        in_channels=in_channels,\n",
    "                        out_channels=in_channels,\n",
    "                        temb_channels=temb_channels,\n",
    "                        eps=resnet_eps,\n",
    "                        groups=resnet_groups,\n",
    "                        dropout=dropout,\n",
    "                        time_embedding_norm=\"spatial\",\n",
    "                        non_linearity=resnet_act_fn,\n",
    "                        output_scale_factor=output_scale_factor,\n",
    "                    )\n",
    "                )\n",
    "            else:\n",
    "                resnets.append(\n",
    "                    ResnetBlock2D(\n",
    "                        in_channels=in_channels,\n",
    "                        out_channels=in_channels,\n",
    "                        temb_channels=temb_channels,\n",
    "                        eps=resnet_eps,\n",
    "                        groups=resnet_groups,\n",
    "                        dropout=dropout,\n",
    "                        time_embedding_norm=resnet_time_scale_shift,\n",
    "                        non_linearity=resnet_act_fn,\n",
    "                        output_scale_factor=output_scale_factor,\n",
    "                        pre_norm=resnet_pre_norm,\n",
    "                    )\n",
    "                )\n",
    "\n",
    "        self.attentions = nn.ModuleList(attentions)\n",
    "        self.resnets = nn.ModuleList(resnets)\n",
    "\n",
    "        self.gradient_checkpointing = False\n",
    "\n",
    "    def forward(\n",
    "        self, hidden_states: torch.Tensor, temb: Optional[torch.Tensor] = None\n",
    "    ) -> torch.Tensor:\n",
    "        hidden_states = self.resnets[0](hidden_states, temb)\n",
    "        for attn, resnet in zip(self.attentions, self.resnets[1:]):\n",
    "            if torch.is_grad_enabled() and self.gradient_checkpointing:\n",
    "                if attn is not None:\n",
    "                    hidden_states = attn(hidden_states, temb=temb)\n",
    "                hidden_states = self._gradient_checkpointing_func(\n",
    "                    resnet, hidden_states, temb\n",
    "                )\n",
    "            else:\n",
    "                if attn is not None:\n",
    "                    hidden_states = attn(hidden_states, temb=temb)\n",
    "                hidden_states = resnet(hidden_states, temb)\n",
    "\n",
    "        return hidden_states\n",
    "\n",
    "\n",
    "def get_timestep_embedding(\n",
    "    timesteps: torch.Tensor,\n",
    "    embedding_dim: int,\n",
    "    flip_sin_to_cos: bool = False,\n",
    "    downscale_freq_shift: float = 1,\n",
    "    scale: float = 1,\n",
    "    max_period: int = 10000,\n",
    "):\n",
    "    \"\"\"\n",
    "    ok This matches the implementation in Denoising Diffusion Probabilistic Models: Create sinusoidal timestep embeddings.\n",
    "\n",
    "    Args\n",
    "        timesteps (torch.Tensor):\n",
    "            a 1-D Tensor of N indices, one per batch element. These may be fractional.\n",
    "        embedding_dim (int):\n",
    "            the dimension of the output.\n",
    "        flip_sin_to_cos (bool):\n",
    "            Whether the embedding order should be `cos, sin` (if True) or `sin, cos` (if False)\n",
    "        downscale_freq_shift (float):\n",
    "            Controls the delta between frequencies between dimensions\n",
    "        scale (float):\n",
    "            Scaling factor applied to the embeddings.\n",
    "        max_period (int):\n",
    "            Controls the maximum frequency of the embeddings\n",
    "    Returns\n",
    "        torch.Tensor: an [N x dim] Tensor of positional embeddings.\n",
    "    \"\"\"\n",
    "    assert len(timesteps.shape) == 1, \"Timesteps should be a 1d-array\"\n",
    "\n",
    "    half_dim = embedding_dim // 2\n",
    "    exponent = -math.log(max_period) * torch.arange(\n",
    "        start=0, end=half_dim, dtype=torch.float32, device=timesteps.device\n",
    "    )\n",
    "    exponent = exponent / (half_dim - downscale_freq_shift)\n",
    "\n",
    "    emb = torch.exp(exponent)\n",
    "    emb = timesteps[:, None].float() * emb[None, :]\n",
    "\n",
    "    # scale embeddings\n",
    "    emb = scale * emb\n",
    "\n",
    "    # concat sine and cosine embeddings\n",
    "    emb = torch.cat([torch.sin(emb), torch.cos(emb)], dim=-1)\n",
    "\n",
    "    # flip sine and cosine embeddings\n",
    "    if flip_sin_to_cos:\n",
    "        emb = torch.cat([emb[:, half_dim:], emb[:, :half_dim]], dim=-1)\n",
    "\n",
    "    # zero pad\n",
    "    if embedding_dim % 2 == 1:\n",
    "        emb = torch.nn.functional.pad(emb, (0, 1, 0, 0))\n",
    "    return emb\n",
    "\n",
    "\n",
    "class TimestepEmbedding(nn.Module):\n",
    "    \"\"\"\n",
    "    ok\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(\n",
    "        self,\n",
    "        in_channels: int,\n",
    "        time_embed_dim: int,\n",
    "        act_fn: str = \"silu\",\n",
    "        out_dim: int = None,\n",
    "        post_act_fn: Optional[str] = None,\n",
    "        cond_proj_dim=None,\n",
    "        sample_proj_bias=True,\n",
    "    ):\n",
    "        super().__init__()\n",
    "\n",
    "        self.linear_1 = nn.Linear(in_channels, time_embed_dim, sample_proj_bias)\n",
    "\n",
    "        if cond_proj_dim is not None:\n",
    "            self.cond_proj = nn.Linear(cond_proj_dim, in_channels, bias=False)\n",
    "        else:\n",
    "            self.cond_proj = None\n",
    "\n",
    "        self.act = get_activation(act_fn)\n",
    "\n",
    "        if out_dim is not None:\n",
    "            time_embed_dim_out = out_dim\n",
    "        else:\n",
    "            time_embed_dim_out = time_embed_dim\n",
    "        self.linear_2 = nn.Linear(time_embed_dim, time_embed_dim_out, sample_proj_bias)\n",
    "\n",
    "        if post_act_fn is None:\n",
    "            self.post_act = None\n",
    "        else:\n",
    "            self.post_act = get_activation(post_act_fn)\n",
    "\n",
    "    def forward(self, sample, condition=None):\n",
    "        if condition is not None:\n",
    "            sample = sample + self.cond_proj(condition)\n",
    "        sample = self.linear_1(sample)\n",
    "\n",
    "        if self.act is not None:\n",
    "            sample = self.act(sample)\n",
    "\n",
    "        sample = self.linear_2(sample)\n",
    "\n",
    "        if self.post_act is not None:\n",
    "            sample = self.post_act(sample)\n",
    "        return sample\n",
    "\n",
    "\n",
    "class Timesteps(nn.Module):\n",
    "    \"\"\"\n",
    "    ok\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(\n",
    "        self,\n",
    "        num_channels: int,\n",
    "        flip_sin_to_cos: bool,\n",
    "        downscale_freq_shift: float,\n",
    "        scale: int = 1,\n",
    "    ):\n",
    "        super().__init__()\n",
    "        self.num_channels = num_channels\n",
    "        self.flip_sin_to_cos = flip_sin_to_cos\n",
    "        self.downscale_freq_shift = downscale_freq_shift\n",
    "        self.scale = scale\n",
    "\n",
    "    def forward(self, timesteps):\n",
    "        t_emb = get_timestep_embedding(\n",
    "            timesteps,\n",
    "            self.num_channels,\n",
    "            flip_sin_to_cos=self.flip_sin_to_cos,\n",
    "            downscale_freq_shift=self.downscale_freq_shift,\n",
    "            scale=self.scale,\n",
    "        )\n",
    "        return t_emb\n",
    "\n",
    "\n",
    "class GaussianFourierProjection(nn.Module):\n",
    "    \"\"\"ok Gaussian Fourier embeddings for noise levels.\"\"\"\n",
    "\n",
    "    def __init__(\n",
    "        self,\n",
    "        embedding_size: int = 256,\n",
    "        scale: float = 1.0,\n",
    "        set_W_to_weight=True,\n",
    "        log=True,\n",
    "        flip_sin_to_cos=False,\n",
    "    ):\n",
    "        super().__init__()\n",
    "        self.weight = nn.Parameter(\n",
    "            torch.randn(embedding_size) * scale, requires_grad=False\n",
    "        )\n",
    "        self.log = log\n",
    "        self.flip_sin_to_cos = flip_sin_to_cos\n",
    "\n",
    "        if set_W_to_weight:\n",
    "            # to delete later\n",
    "            del self.weight\n",
    "            self.W = nn.Parameter(\n",
    "                torch.randn(embedding_size) * scale, requires_grad=False\n",
    "            )\n",
    "            self.weight = self.W\n",
    "            del self.W\n",
    "\n",
    "    def forward(self, x):\n",
    "        if self.log:\n",
    "            x = torch.log(x)\n",
    "\n",
    "        x_proj = x[:, None] * self.weight[None, :] * 2 * np.pi\n",
    "\n",
    "        if self.flip_sin_to_cos:\n",
    "            out = torch.cat([torch.cos(x_proj), torch.sin(x_proj)], dim=-1)\n",
    "        else:\n",
    "            out = torch.cat([torch.sin(x_proj), torch.cos(x_proj)], dim=-1)\n",
    "        return out\n",
    "\n",
    "\n",
    "class UNet2DModel(ModelMixin, ConfigMixin):\n",
    "    r\"\"\"\n",
    "    A 2D UNet model that takes a noisy sample and a timestep and returns a sample shaped output.\n",
    "\n",
    "    This model inherits from [`ModelMixin`]. Check the superclass documentation for it's generic methods implemented\n",
    "    for all models (such as downloading or saving).\n",
    "\n",
    "    Parameters:\n",
    "        sample_size (`int` or `Tuple[int, int]`, *optional*, defaults to `None`):\n",
    "            Height and width of input/output sample. Dimensions must be a multiple of `2 ** (len(block_out_channels) -\n",
    "            1)`.\n",
    "        in_channels (`int`, *optional*, defaults to 3): Number of channels in the input sample.\n",
    "        out_channels (`int`, *optional*, defaults to 3): Number of channels in the output.\n",
    "        center_input_sample (`bool`, *optional*, defaults to `False`): Whether to center the input sample.\n",
    "        time_embedding_type (`str`, *optional*, defaults to `\"positional\"`): Type of time embedding to use.\n",
    "        freq_shift (`int`, *optional*, defaults to 0): Frequency shift for Fourier time embedding.\n",
    "        flip_sin_to_cos (`bool`, *optional*, defaults to `True`):\n",
    "            Whether to flip sin to cos for Fourier time embedding.\n",
    "        down_block_types (`Tuple[str]`, *optional*, defaults to `(\"DownBlock2D\", \"AttnDownBlock2D\", \"AttnDownBlock2D\", \"AttnDownBlock2D\")`):\n",
    "            Tuple of downsample block types.\n",
    "        mid_block_type (`str`, *optional*, defaults to `\"UNetMidBlock2D\"`):\n",
    "            Block type for middle of UNet, it can be either `UNetMidBlock2D` or `None`.\n",
    "        up_block_types (`Tuple[str]`, *optional*, defaults to `(\"AttnUpBlock2D\", \"AttnUpBlock2D\", \"AttnUpBlock2D\", \"UpBlock2D\")`):\n",
    "            Tuple of upsample block types.\n",
    "        block_out_channels (`Tuple[int]`, *optional*, defaults to `(224, 448, 672, 896)`):\n",
    "            Tuple of block output channels.\n",
    "        layers_per_block (`int`, *optional*, defaults to `2`): The number of layers per block.\n",
    "        mid_block_scale_factor (`float`, *optional*, defaults to `1`): The scale factor for the mid block.\n",
    "        downsample_padding (`int`, *optional*, defaults to `1`): The padding for the downsample convolution.\n",
    "        downsample_type (`str`, *optional*, defaults to `conv`):\n",
    "            The downsample type for downsampling layers. Choose between \"conv\" and \"resnet\"\n",
    "        upsample_type (`str`, *optional*, defaults to `conv`):\n",
    "            The upsample type for upsampling layers. Choose between \"conv\" and \"resnet\"\n",
    "        dropout (`float`, *optional*, defaults to 0.0): The dropout probability to use.\n",
    "        act_fn (`str`, *optional*, defaults to `\"silu\"`): The activation function to use.\n",
    "        attention_head_dim (`int`, *optional*, defaults to `8`): The attention head dimension.\n",
    "        norm_num_groups (`int`, *optional*, defaults to `32`): The number of groups for normalization.\n",
    "        attn_norm_num_groups (`int`, *optional*, defaults to `None`):\n",
    "            If set to an integer, a group norm layer will be created in the mid block's [`Attention`] layer with the\n",
    "            given number of groups. If left as `None`, the group norm layer will only be created if\n",
    "            `resnet_time_scale_shift` is set to `default`, and if created will have `norm_num_groups` groups.\n",
    "        norm_eps (`float`, *optional*, defaults to `1e-5`): The epsilon for normalization.\n",
    "        resnet_time_scale_shift (`str`, *optional*, defaults to `\"default\"`): Time scale shift config\n",
    "            for ResNet blocks (see [`~models.resnet.ResnetBlock2D`]). Choose from `default` or `scale_shift`.\n",
    "        class_embed_type (`str`, *optional*, defaults to `None`):\n",
    "            The type of class embedding to use which is ultimately summed with the time embeddings. Choose from `None`,\n",
    "            `\"timestep\"`, or `\"identity\"`.\n",
    "        num_class_embeds (`int`, *optional*, defaults to `None`):\n",
    "            Input dimension of the learnable embedding matrix to be projected to `time_embed_dim` when performing class\n",
    "            conditioning with `class_embed_type` equal to `None`.\n",
    "    \"\"\"\n",
    "\n",
    "    _supports_gradient_checkpointing = True\n",
    "    _skip_layerwise_casting_patterns = [\"norm\"]\n",
    "\n",
    "    @register_to_config\n",
    "    def __init__(\n",
    "        self,\n",
    "        sample_size: Optional[Union[int, Tuple[int, int]]] = None,\n",
    "        in_channels: int = 3,\n",
    "        out_channels: int = 3,\n",
    "        center_input_sample: bool = False,\n",
    "        time_embedding_type: str = \"positional\",\n",
    "        time_embedding_dim: Optional[int] = None,\n",
    "        freq_shift: int = 0,\n",
    "        flip_sin_to_cos: bool = True,\n",
    "        down_block_types: Tuple[str, ...] = (\n",
    "            \"DownBlock2D\",\n",
    "            \"AttnDownBlock2D\",\n",
    "            \"AttnDownBlock2D\",\n",
    "            \"AttnDownBlock2D\",\n",
    "        ),\n",
    "        mid_block_type: Optional[str] = \"UNetMidBlock2D\",\n",
    "        up_block_types: Tuple[str, ...] = (\n",
    "            \"AttnUpBlock2D\",\n",
    "            \"AttnUpBlock2D\",\n",
    "            \"AttnUpBlock2D\",\n",
    "            \"UpBlock2D\",\n",
    "        ),\n",
    "        block_out_channels: Tuple[int, ...] = (224, 448, 672, 896),\n",
    "        layers_per_block: int = 2,\n",
    "        mid_block_scale_factor: float = 1,\n",
    "        downsample_padding: int = 1,\n",
    "        downsample_type: str = \"conv\",\n",
    "        upsample_type: str = \"conv\",\n",
    "        dropout: float = 0.0,\n",
    "        act_fn: str = \"silu\",\n",
    "        attention_head_dim: Optional[int] = 8,\n",
    "        norm_num_groups: int = 32,\n",
    "        attn_norm_num_groups: Optional[int] = None,\n",
    "        norm_eps: float = 1e-5,\n",
    "        resnet_time_scale_shift: str = \"default\",\n",
    "        add_attention: bool = True,\n",
    "        class_embed_type: Optional[str] = None,\n",
    "        num_class_embeds: Optional[int] = None,\n",
    "        num_train_timesteps: Optional[int] = None,\n",
    "    ):\n",
    "        super().__init__()\n",
    "\n",
    "        self.sample_size = sample_size\n",
    "        time_embed_dim = time_embedding_dim or block_out_channels[0] * 4\n",
    "\n",
    "        # Check inputs\n",
    "        if len(down_block_types) != len(up_block_types):\n",
    "            raise ValueError(\n",
    "                f\"Must provide the same number of `down_block_types` as `up_block_types`. `down_block_types`: {down_block_types}. `up_block_types`: {up_block_types}.\"\n",
    "            )\n",
    "\n",
    "        if len(block_out_channels) != len(down_block_types):\n",
    "            raise ValueError(\n",
    "                f\"Must provide the same number of `block_out_channels` as `down_block_types`. `block_out_channels`: {block_out_channels}. `down_block_types`: {down_block_types}.\"\n",
    "            )\n",
    "\n",
    "        # input\n",
    "        self.conv_in = nn.Conv2d(\n",
    "            in_channels, block_out_channels[0], kernel_size=3, padding=(1, 1)\n",
    "        )\n",
    "\n",
    "        # time\n",
    "        if time_embedding_type == \"fourier\":\n",
    "            self.time_proj = GaussianFourierProjection(\n",
    "                embedding_size=block_out_channels[0], scale=16\n",
    "            )\n",
    "            timestep_input_dim = 2 * block_out_channels[0]\n",
    "        elif time_embedding_type == \"positional\":\n",
    "            self.time_proj = Timesteps(\n",
    "                block_out_channels[0], flip_sin_to_cos, freq_shift\n",
    "            )\n",
    "            timestep_input_dim = block_out_channels[0]\n",
    "        elif time_embedding_type == \"learned\":\n",
    "            self.time_proj = nn.Embedding(num_train_timesteps, block_out_channels[0])\n",
    "            timestep_input_dim = block_out_channels[0]\n",
    "\n",
    "        self.time_embedding = TimestepEmbedding(timestep_input_dim, time_embed_dim)\n",
    "\n",
    "        # class embedding\n",
    "        if class_embed_type is None and num_class_embeds is not None:\n",
    "            self.class_embedding = nn.Embedding(num_class_embeds, time_embed_dim)\n",
    "        elif class_embed_type == \"timestep\":\n",
    "            self.class_embedding = TimestepEmbedding(timestep_input_dim, time_embed_dim)\n",
    "        elif class_embed_type == \"identity\":\n",
    "            self.class_embedding = nn.Identity(time_embed_dim, time_embed_dim)\n",
    "        else:\n",
    "            self.class_embedding = None\n",
    "\n",
    "        self.down_blocks = nn.ModuleList([])\n",
    "        self.mid_block = None\n",
    "        self.up_blocks = nn.ModuleList([])\n",
    "\n",
    "        # down\n",
    "        output_channel = block_out_channels[0]\n",
    "        for i, down_block_type in enumerate(down_block_types):\n",
    "            input_channel = output_channel\n",
    "            output_channel = block_out_channels[i]\n",
    "            is_final_block = i == len(block_out_channels) - 1\n",
    "\n",
    "            down_block = get_down_block(\n",
    "                down_block_type,\n",
    "                num_layers=layers_per_block,\n",
    "                in_channels=input_channel,\n",
    "                out_channels=output_channel,\n",
    "                temb_channels=time_embed_dim,\n",
    "                add_downsample=not is_final_block,\n",
    "                resnet_eps=norm_eps,\n",
    "                resnet_act_fn=act_fn,\n",
    "                resnet_groups=norm_num_groups,\n",
    "                attention_head_dim=(\n",
    "                    attention_head_dim\n",
    "                    if attention_head_dim is not None\n",
    "                    else output_channel\n",
    "                ),\n",
    "                downsample_padding=downsample_padding,\n",
    "                resnet_time_scale_shift=resnet_time_scale_shift,\n",
    "                downsample_type=downsample_type,\n",
    "                dropout=dropout,\n",
    "            )\n",
    "            self.down_blocks.append(down_block)\n",
    "\n",
    "        # mid\n",
    "        if mid_block_type is None:\n",
    "            self.mid_block = None\n",
    "        else:\n",
    "            self.mid_block = UNetMidBlock2D(\n",
    "                in_channels=block_out_channels[-1],\n",
    "                temb_channels=time_embed_dim,\n",
    "                dropout=dropout,\n",
    "                resnet_eps=norm_eps,\n",
    "                resnet_act_fn=act_fn,\n",
    "                output_scale_factor=mid_block_scale_factor,\n",
    "                resnet_time_scale_shift=resnet_time_scale_shift,\n",
    "                attention_head_dim=(\n",
    "                    attention_head_dim\n",
    "                    if attention_head_dim is not None\n",
    "                    else block_out_channels[-1]\n",
    "                ),\n",
    "                resnet_groups=norm_num_groups,\n",
    "                attn_groups=attn_norm_num_groups,\n",
    "                add_attention=add_attention,\n",
    "            )\n",
    "\n",
    "        # up\n",
    "        reversed_block_out_channels = list(reversed(block_out_channels))\n",
    "        output_channel = reversed_block_out_channels[0]\n",
    "        for i, up_block_type in enumerate(up_block_types):\n",
    "            prev_output_channel = output_channel\n",
    "            output_channel = reversed_block_out_channels[i]\n",
    "            input_channel = reversed_block_out_channels[\n",
    "                min(i + 1, len(block_out_channels) - 1)\n",
    "            ]\n",
    "\n",
    "            is_final_block = i == len(block_out_channels) - 1\n",
    "\n",
    "            up_block = get_up_block(\n",
    "                up_block_type,\n",
    "                num_layers=layers_per_block + 1,\n",
    "                in_channels=input_channel,\n",
    "                out_channels=output_channel,\n",
    "                prev_output_channel=prev_output_channel,\n",
    "                temb_channels=time_embed_dim,\n",
    "                add_upsample=not is_final_block,\n",
    "                resnet_eps=norm_eps,\n",
    "                resnet_act_fn=act_fn,\n",
    "                resnet_groups=norm_num_groups,\n",
    "                attention_head_dim=(\n",
    "                    attention_head_dim\n",
    "                    if attention_head_dim is not None\n",
    "                    else output_channel\n",
    "                ),\n",
    "                resnet_time_scale_shift=resnet_time_scale_shift,\n",
    "                upsample_type=upsample_type,\n",
    "                dropout=dropout,\n",
    "            )\n",
    "            self.up_blocks.append(up_block)\n",
    "\n",
    "        # out\n",
    "        num_groups_out = (\n",
    "            norm_num_groups\n",
    "            if norm_num_groups is not None\n",
    "            else min(block_out_channels[0] // 4, 32)\n",
    "        )\n",
    "        self.conv_norm_out = nn.GroupNorm(\n",
    "            num_channels=block_out_channels[0], num_groups=num_groups_out, eps=norm_eps\n",
    "        )\n",
    "        self.conv_act = nn.SiLU()\n",
    "        self.conv_out = nn.Conv2d(\n",
    "            block_out_channels[0], out_channels, kernel_size=3, padding=1\n",
    "        )\n",
    "\n",
    "    def forward(\n",
    "        self,\n",
    "        sample: torch.Tensor,\n",
    "        timestep: Union[torch.Tensor, float, int],\n",
    "        class_labels: Optional[torch.Tensor] = None,\n",
    "        return_dict: bool = True,\n",
    "    ) -> Union[UNet2DOutput, Tuple]:\n",
    "        r\"\"\"\n",
    "        The [`UNet2DModel`] forward method.\n",
    "\n",
    "        Args:\n",
    "            sample (`torch.Tensor`):\n",
    "                The noisy input tensor with the following shape `(batch, channel, height, width)`.\n",
    "            timestep (`torch.Tensor` or `float` or `int`): The number of timesteps to denoise an input.\n",
    "            class_labels (`torch.Tensor`, *optional*, defaults to `None`):\n",
    "                Optional class labels for conditioning. Their embeddings will be summed with the timestep embeddings.\n",
    "            return_dict (`bool`, *optional*, defaults to `True`):\n",
    "                Whether or not to return a [`~models.unets.unet_2d.UNet2DOutput`] instead of a plain tuple.\n",
    "\n",
    "        Returns:\n",
    "            [`~models.unets.unet_2d.UNet2DOutput`] or `tuple`:\n",
    "                If `return_dict` is True, an [`~models.unets.unet_2d.UNet2DOutput`] is returned, otherwise a `tuple` is\n",
    "                returned where the first element is the sample tensor.\n",
    "        \"\"\"\n",
    "        # 0. center input if necessary\n",
    "        if self.config.center_input_sample:\n",
    "            sample = 2 * sample - 1.0\n",
    "\n",
    "        # 1. time\n",
    "        timesteps = timestep\n",
    "        if not torch.is_tensor(timesteps):\n",
    "            timesteps = torch.tensor(\n",
    "                [timesteps], dtype=torch.long, device=sample.device\n",
    "            )\n",
    "        elif torch.is_tensor(timesteps) and len(timesteps.shape) == 0:\n",
    "            timesteps = timesteps[None].to(sample.device)\n",
    "\n",
    "        # broadcast to batch dimension in a way that's compatible with ONNX/Core ML\n",
    "        timesteps = timesteps * torch.ones(\n",
    "            sample.shape[0], dtype=timesteps.dtype, device=timesteps.device\n",
    "        )\n",
    "\n",
    "        t_emb = self.time_proj(timesteps)\n",
    "\n",
    "        # timesteps does not contain any weights and will always return f32 tensors\n",
    "        # but time_embedding might actually be running in fp16. so we need to cast here.\n",
    "        # there might be better ways to encapsulate this.\n",
    "        t_emb = t_emb.to(dtype=self.dtype)\n",
    "        emb = self.time_embedding(t_emb)\n",
    "\n",
    "        if self.class_embedding is not None:\n",
    "            if class_labels is None:\n",
    "                raise ValueError(\n",
    "                    \"class_labels should be provided when doing class conditioning\"\n",
    "                )\n",
    "\n",
    "            if self.config.class_embed_type == \"timestep\":\n",
    "                class_labels = self.time_proj(class_labels)\n",
    "\n",
    "            class_emb = self.class_embedding(class_labels).to(dtype=self.dtype)\n",
    "            emb = emb + class_emb\n",
    "        elif self.class_embedding is None and class_labels is not None:\n",
    "            raise ValueError(\n",
    "                \"class_embedding needs to be initialized in order to use class conditioning\"\n",
    "            )\n",
    "\n",
    "        # 2. pre-process\n",
    "        skip_sample = sample\n",
    "        sample = self.conv_in(sample)\n",
    "\n",
    "        # 3. down\n",
    "        down_block_res_samples = (sample,)\n",
    "        for downsample_block in self.down_blocks:\n",
    "            if hasattr(downsample_block, \"skip_conv\"):\n",
    "                sample, res_samples, skip_sample = downsample_block(\n",
    "                    hidden_states=sample, temb=emb, skip_sample=skip_sample\n",
    "                )\n",
    "            else:\n",
    "                sample, res_samples = downsample_block(hidden_states=sample, temb=emb)\n",
    "\n",
    "            down_block_res_samples += res_samples\n",
    "\n",
    "        # 4. mid\n",
    "        if self.mid_block is not None:\n",
    "            sample = self.mid_block(sample, emb)\n",
    "\n",
    "        # 5. up\n",
    "        skip_sample = None\n",
    "        for upsample_block in self.up_blocks:\n",
    "            res_samples = down_block_res_samples[-len(upsample_block.resnets) :]\n",
    "            down_block_res_samples = down_block_res_samples[\n",
    "                : -len(upsample_block.resnets)\n",
    "            ]\n",
    "\n",
    "            if hasattr(upsample_block, \"skip_conv\"):\n",
    "                sample, skip_sample = upsample_block(\n",
    "                    sample, res_samples, emb, skip_sample\n",
    "                )\n",
    "            else:\n",
    "                sample = upsample_block(sample, res_samples, emb)\n",
    "\n",
    "        # 6. post-process\n",
    "        sample = self.conv_norm_out(sample)\n",
    "        sample = self.conv_act(sample)\n",
    "        sample = self.conv_out(sample)\n",
    "\n",
    "        if skip_sample is not None:\n",
    "            sample += skip_sample\n",
    "\n",
    "        if self.config.time_embedding_type == \"fourier\":\n",
    "            timesteps = timesteps.reshape(\n",
    "                (sample.shape[0], *([1] * len(sample.shape[1:])))\n",
    "            )\n",
    "            sample = sample / timesteps\n",
    "\n",
    "        if not return_dict:\n",
    "            return (sample,)\n",
    "\n",
    "        return UNet2DOutput(sample=sample)\n",
    "\n",
    "\n",
    "@dataclass\n",
    "class Args:\n",
    "    resolution: int = 64\n",
    "    ddpm_num_steps: int = 1000\n",
    "    ddpm_beta_schedule: str = \"linear\"\n",
    "    learning_rate: float = 1e-4\n",
    "    adam_beta1: float = 0.95\n",
    "    adam_beta2: float = 0.999\n",
    "    adam_weight_decay: float = 1e-6\n",
    "    adam_epsilon: float = 1e-08\n",
    "    dataset_name: str = \"huggan/flowers-102-categories\"\n",
    "    dataset_config_name: str = None\n",
    "    cache_dir: str = None\n",
    "    ema_max_decay: float = 0.9999\n",
    "    ema_inv_gamma: float = 1.0\n",
    "    ema_power: float = 3 / 4\n",
    "    train_batch_size: int = 16\n",
    "    eval_batch_size: int = 16\n",
    "    gradient_accumulation_steps: int = 1\n",
    "    lr_scheduler: str = \"cosine\"\n",
    "    lr_warmup_steps: int = 500\n",
    "    num_epochs: int = 1\n",
    "    center_crop: bool = False\n",
    "    random_flip: bool = False\n",
    "    dataloader_num_workers: int = 0\n",
    "    use_ema: bool = True\n",
    "    checkpointing_steps: int = 500\n",
    "    checkpoints_total_limit: int = None\n",
    "    output_dir: str = \"minimal_diffusion/unconditional_diffusion/ddpm-model-64\"\n",
    "    save_images_epochs: int = 10\n",
    "    save_model_epochs: int = 1\n",
    "    prediction_type: str = \"epsilon\"\n",
    "    logging_dir: str = \"logs\"\n",
    "    mixed_precision: str = \"no\"\n",
    "    logger: str = \"wandb\"\n",
    "    ddpm_num_inference_steps: int = 1000\n",
    "    project_name: str = \"train_unconditional\"\n",
    "\n",
    "\n",
    "args = Args()\n",
    "\n",
    "model = UNet2DModel(\n",
    "    sample_size=args.resolution,\n",
    "    in_channels=3,\n",
    "    out_channels=3,\n",
    "    layers_per_block=2,\n",
    "    block_out_channels=(128, 128, 256, 256, 512, 512),\n",
    "    down_block_types=(\n",
    "        \"DownBlock2D\",\n",
    "        \"DownBlock2D\",\n",
    "        \"DownBlock2D\",\n",
    "        \"DownBlock2D\",\n",
    "        \"AttnDownBlock2D\",\n",
    "        \"DownBlock2D\",\n",
    "    ),\n",
    "    up_block_types=(\n",
    "        \"UpBlock2D\",\n",
    "        \"AttnUpBlock2D\",\n",
    "        \"UpBlock2D\",\n",
    "        \"UpBlock2D\",\n",
    "        \"UpBlock2D\",\n",
    "        \"UpBlock2D\",\n",
    "    ),\n",
    ")\n",
    "model = model.cuda()\n",
    "ema_model = EMAModel(\n",
    "    model.parameters(),\n",
    "    decay=args.ema_max_decay,\n",
    "    use_ema_warmup=True,\n",
    "    inv_gamma=args.ema_inv_gamma,\n",
    "    power=args.ema_power,\n",
    "    model_cls=UNet2DModel,\n",
    "    model_config=model.config,\n",
    ")\n",
    "\n",
    "noise_scheduler = DDPMScheduler(\n",
    "    num_train_timesteps=args.ddpm_num_steps,\n",
    "    beta_schedule=args.ddpm_beta_schedule,\n",
    ")\n",
    "\n",
    "optimizer = torch.optim.AdamW(\n",
    "    model.parameters(),\n",
    "    lr=args.learning_rate,\n",
    "    betas=(args.adam_beta1, args.adam_beta2),\n",
    "    weight_decay=args.adam_weight_decay,\n",
    "    eps=args.adam_epsilon,\n",
    ")\n",
    "\n",
    "dataset = load_dataset(\n",
    "    args.dataset_name,\n",
    "    args.dataset_config_name,\n",
    "    cache_dir=args.cache_dir,\n",
    "    split=\"train\",\n",
    ")\n",
    "\n",
    "augmentations = transforms.Compose(\n",
    "    [\n",
    "        transforms.Resize(\n",
    "            args.resolution, interpolation=transforms.InterpolationMode.BILINEAR\n",
    "        ),\n",
    "        (\n",
    "            transforms.CenterCrop(args.resolution)\n",
    "            if args.center_crop\n",
    "            else transforms.RandomCrop(args.resolution)\n",
    "        ),\n",
    "        (\n",
    "            transforms.RandomHorizontalFlip()\n",
    "            if args.random_flip\n",
    "            else transforms.Lambda(lambda x: x)\n",
    "        ),\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize(\n",
    "            [0.5],\n",
    "            [0.5],\n",
    "        ),\n",
    "    ]\n",
    ")\n",
    "\n",
    "\n",
    "def transform_images(examples):\n",
    "    images = [augmentations(image.convert(\"RGB\")) for image in examples[\"image\"]]\n",
    "    return {\"input\": images}\n",
    "\n",
    "\n",
    "dataset.set_transform(transform_images)\n",
    "train_dataloader = torch.utils.data.DataLoader(\n",
    "    dataset,\n",
    "    batch_size=args.train_batch_size,\n",
    "    shuffle=True,\n",
    "    num_workers=args.dataloader_num_workers,\n",
    ")\n",
    "\n",
    "# Initialize the learning rate scheduler\n",
    "lr_scheduler = get_scheduler(\n",
    "    args.lr_scheduler,\n",
    "    optimizer=optimizer,\n",
    "    num_warmup_steps=args.lr_warmup_steps * args.gradient_accumulation_steps,\n",
    "    num_training_steps=(len(train_dataloader) * args.num_epochs),\n",
    ")\n",
    "\n",
    "\n",
    "total_batch_size = args.train_batch_size * args.gradient_accumulation_steps\n",
    "num_update_steps_per_epoch = math.ceil(\n",
    "    len(train_dataloader) / args.gradient_accumulation_steps\n",
    ")\n",
    "max_train_steps = args.num_epochs * num_update_steps_per_epoch\n",
    "\n",
    "\n",
    "print(\"***** Running training *****\")\n",
    "print(f\"  Num examples = {len(dataset)}\")\n",
    "print(f\"  Num Epochs = {args.num_epochs}\")\n",
    "print(f\"  Instantaneous batch size per device = {args.train_batch_size}\")\n",
    "print(\n",
    "    f\"  Total train batch size (w. parallel, distributed & accumulation) = {total_batch_size}\"\n",
    ")\n",
    "print(f\"  Gradient Accumulation steps = {args.gradient_accumulation_steps}\")\n",
    "print(f\"  Total optimization steps = {max_train_steps}\")\n",
    "\n",
    "global_step = 0\n",
    "first_epoch = 0\n",
    "\n",
    "weight_dtype = torch.float32\n",
    "# model.enable_xformers_memory_efficient_attention()\n",
    "logging_dir = os.path.join(args.output_dir, args.logging_dir)\n",
    "accelerator_project_config = ProjectConfiguration(\n",
    "    project_dir=args.output_dir, logging_dir=logging_dir\n",
    ")\n",
    "accelerator = Accelerator(\n",
    "    gradient_accumulation_steps=args.gradient_accumulation_steps,\n",
    "    mixed_precision=args.mixed_precision,\n",
    "    log_with=args.logger,\n",
    "    project_config=accelerator_project_config,\n",
    "    # kwargs_handlers=[kwargs],\n",
    ")\n",
    "model, optimizer, train_dataloader, lr_scheduler = accelerator.prepare(\n",
    "    model, optimizer, train_dataloader, lr_scheduler\n",
    ")\n",
    "\n",
    "if accelerator.is_main_process:\n",
    "    run = os.path.split(args.project_name)[-1].split(\".\")[0]\n",
    "    accelerator.init_trackers(run)\n",
    "\n",
    "# Train!\n",
    "for epoch in range(first_epoch, args.num_epochs):\n",
    "    model.train()\n",
    "    progress_bar = tqdm(\n",
    "        total=num_update_steps_per_epoch,\n",
    "        disable=not accelerator.is_local_main_process,\n",
    "    )\n",
    "    progress_bar.set_description(f\"Epoch {epoch}\")\n",
    "    for step, batch in enumerate(train_dataloader):\n",
    "        # Skip steps until we reach the resumed step\n",
    "\n",
    "        clean_images = batch[\"input\"].to(weight_dtype)\n",
    "        # Sample noise that we'll add to the images\n",
    "        noise = torch.randn(\n",
    "            clean_images.shape, dtype=weight_dtype, device=clean_images.device\n",
    "        )\n",
    "        bsz = clean_images.shape[0]\n",
    "        # Sample a random timestep for each image\n",
    "        timesteps = torch.randint(\n",
    "            0,\n",
    "            noise_scheduler.config.num_train_timesteps,\n",
    "            (bsz,),\n",
    "            device=clean_images.device,\n",
    "        ).long()\n",
    "\n",
    "        # Add noise to the clean images according to the noise magnitude at each timestep\n",
    "        # (this is the forward diffusion process)\n",
    "        noisy_images = noise_scheduler.add_noise(clean_images, noise, timesteps)\n",
    "\n",
    "        with accelerator.accumulate(model):\n",
    "            # Predict the noise residual\n",
    "            model_output = model(noisy_images, timesteps).sample\n",
    "\n",
    "            if args.prediction_type == \"epsilon\":\n",
    "                loss = F.mse_loss(\n",
    "                    model_output.float(),\n",
    "                    noise.float(),\n",
    "                )\n",
    "\n",
    "            accelerator.backward(loss)\n",
    "\n",
    "            if accelerator.sync_gradients:\n",
    "                accelerator.clip_grad_norm_(model.parameters(), 1.0)\n",
    "            optimizer.step()\n",
    "            lr_scheduler.step()\n",
    "            optimizer.zero_grad()\n",
    "\n",
    "        # Checks if the accelerator has performed an optimization step behind the scenes\n",
    "        if accelerator.sync_gradients:\n",
    "            if args.use_ema:\n",
    "                ema_model.step(model.parameters())\n",
    "            progress_bar.update(1)\n",
    "            global_step += 1\n",
    "\n",
    "            if accelerator.is_main_process:\n",
    "                if global_step % args.checkpointing_steps == 0:\n",
    "                    # _before_ saving state, check if this save would set us over the `checkpoints_total_limit`\n",
    "\n",
    "                    save_path = os.path.join(\n",
    "                        args.output_dir, f\"checkpoint-{global_step}\"\n",
    "                    )\n",
    "                    accelerator.save_state(save_path)\n",
    "                    print(f\"Saved state to {save_path}\")\n",
    "\n",
    "        logs = {\n",
    "            \"loss\": loss.detach().item(),\n",
    "            \"lr\": lr_scheduler.get_last_lr()[0],\n",
    "            \"step\": global_step,\n",
    "        }\n",
    "        if args.use_ema:\n",
    "            logs[\"ema_decay\"] = ema_model.cur_decay_value\n",
    "        progress_bar.set_postfix(**logs)\n",
    "        accelerator.log(logs, step=global_step)\n",
    "    progress_bar.close()\n",
    "\n",
    "    accelerator.wait_for_everyone()\n",
    "\n",
    "    # Generate sample images for visual inspection\n",
    "    if accelerator.is_main_process:\n",
    "        if epoch % args.save_images_epochs == 0 or epoch == args.num_epochs - 1:\n",
    "            unet = accelerator.unwrap_model(model)\n",
    "\n",
    "            if args.use_ema:\n",
    "                ema_model.store(unet.parameters())\n",
    "                ema_model.copy_to(unet.parameters())\n",
    "\n",
    "            pipeline = DDPMPipeline(\n",
    "                unet=unet,\n",
    "                scheduler=noise_scheduler,\n",
    "            )\n",
    "\n",
    "            generator = torch.Generator(device=pipeline.device).manual_seed(0)\n",
    "            # run pipeline in inference (sample random noise and denoise)\n",
    "            images = pipeline(\n",
    "                generator=generator,\n",
    "                batch_size=args.eval_batch_size,\n",
    "                num_inference_steps=args.ddpm_num_inference_steps,\n",
    "                output_type=\"np\",\n",
    "            ).images\n",
    "\n",
    "            if args.use_ema:\n",
    "                ema_model.restore(unet.parameters())\n",
    "\n",
    "            # denormalize the images and save to tensorboard\n",
    "            images_processed = (images * 255).round().astype(\"uint8\")\n",
    "\n",
    "            if args.logger == \"wandb\":\n",
    "                # Upcoming `log_images` helper coming in https://github.com/huggingface/accelerate/pull/962/files\n",
    "                accelerator.get_tracker(\"wandb\").log(\n",
    "                    {\n",
    "                        \"test_samples\": [wandb.Image(img) for img in images_processed],\n",
    "                        \"epoch\": epoch,\n",
    "                    },\n",
    "                    step=global_step,\n",
    "                )\n",
    "\n",
    "        if epoch % args.save_model_epochs == 0 or epoch == args.num_epochs - 1:\n",
    "            # save the model\n",
    "            unet = accelerator.unwrap_model(model)\n",
    "\n",
    "            if args.use_ema:\n",
    "                ema_model.store(unet.parameters())\n",
    "                ema_model.copy_to(unet.parameters())\n",
    "\n",
    "            pipeline = DDPMPipeline(\n",
    "                unet=unet,\n",
    "                scheduler=noise_scheduler,\n",
    "            )\n",
    "\n",
    "            pipeline.save_pretrained(args.output_dir)\n",
    "\n",
    "            if args.use_ema:\n",
    "                ema_model.restore(unet.parameters())\n",
    "\n",
    "\n",
    "accelerator.end_training()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "aeeaab98",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "UNet2DModel(\n",
       "  (conv_in): Conv2d(3, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "  (time_proj): Timesteps()\n",
       "  (time_embedding): TimestepEmbedding(\n",
       "    (linear_1): Linear(in_features=128, out_features=512, bias=True)\n",
       "    (act): SiLU()\n",
       "    (linear_2): Linear(in_features=512, out_features=512, bias=True)\n",
       "  )\n",
       "  (down_blocks): ModuleList(\n",
       "    (0-1): 2 x DownBlock2D(\n",
       "      (resnets): ModuleList(\n",
       "        (0-1): 2 x ResnetBlock2D(\n",
       "          (norm1): GroupNorm(32, 128, eps=1e-05, affine=True)\n",
       "          (conv1): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "          (time_emb_proj): Linear(in_features=512, out_features=128, bias=True)\n",
       "          (norm2): GroupNorm(32, 128, eps=1e-05, affine=True)\n",
       "          (dropout): Dropout(p=0.0, inplace=False)\n",
       "          (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "          (nonlinearity): SiLU()\n",
       "        )\n",
       "      )\n",
       "      (downsamplers): ModuleList(\n",
       "        (0): Downsample2D(\n",
       "          (conv): Conv2d(128, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1))\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (2): DownBlock2D(\n",
       "      (resnets): ModuleList(\n",
       "        (0): ResnetBlock2D(\n",
       "          (norm1): GroupNorm(32, 128, eps=1e-05, affine=True)\n",
       "          (conv1): Conv2d(128, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "          (time_emb_proj): Linear(in_features=512, out_features=256, bias=True)\n",
       "          (norm2): GroupNorm(32, 256, eps=1e-05, affine=True)\n",
       "          (dropout): Dropout(p=0.0, inplace=False)\n",
       "          (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "          (nonlinearity): SiLU()\n",
       "          (conv_shortcut): Conv2d(128, 256, kernel_size=(1, 1), stride=(1, 1))\n",
       "        )\n",
       "        (1): ResnetBlock2D(\n",
       "          (norm1): GroupNorm(32, 256, eps=1e-05, affine=True)\n",
       "          (conv1): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "          (time_emb_proj): Linear(in_features=512, out_features=256, bias=True)\n",
       "          (norm2): GroupNorm(32, 256, eps=1e-05, affine=True)\n",
       "          (dropout): Dropout(p=0.0, inplace=False)\n",
       "          (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "          (nonlinearity): SiLU()\n",
       "        )\n",
       "      )\n",
       "      (downsamplers): ModuleList(\n",
       "        (0): Downsample2D(\n",
       "          (conv): Conv2d(256, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1))\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (3): DownBlock2D(\n",
       "      (resnets): ModuleList(\n",
       "        (0-1): 2 x ResnetBlock2D(\n",
       "          (norm1): GroupNorm(32, 256, eps=1e-05, affine=True)\n",
       "          (conv1): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "          (time_emb_proj): Linear(in_features=512, out_features=256, bias=True)\n",
       "          (norm2): GroupNorm(32, 256, eps=1e-05, affine=True)\n",
       "          (dropout): Dropout(p=0.0, inplace=False)\n",
       "          (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "          (nonlinearity): SiLU()\n",
       "        )\n",
       "      )\n",
       "      (downsamplers): ModuleList(\n",
       "        (0): Downsample2D(\n",
       "          (conv): Conv2d(256, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1))\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (4): AttnDownBlock2D(\n",
       "      (attentions): ModuleList(\n",
       "        (0-1): 2 x Attention(\n",
       "          (group_norm): GroupNorm(32, 512, eps=1e-05, affine=True)\n",
       "          (to_q): Linear(in_features=512, out_features=512, bias=True)\n",
       "          (to_k): Linear(in_features=512, out_features=512, bias=True)\n",
       "          (to_v): Linear(in_features=512, out_features=512, bias=True)\n",
       "          (to_out): ModuleList(\n",
       "            (0): Linear(in_features=512, out_features=512, bias=True)\n",
       "            (1): Dropout(p=0.0, inplace=False)\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "      (resnets): ModuleList(\n",
       "        (0): ResnetBlock2D(\n",
       "          (norm1): GroupNorm(32, 256, eps=1e-05, affine=True)\n",
       "          (conv1): Conv2d(256, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "          (time_emb_proj): Linear(in_features=512, out_features=512, bias=True)\n",
       "          (norm2): GroupNorm(32, 512, eps=1e-05, affine=True)\n",
       "          (dropout): Dropout(p=0.0, inplace=False)\n",
       "          (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "          (nonlinearity): SiLU()\n",
       "          (conv_shortcut): Conv2d(256, 512, kernel_size=(1, 1), stride=(1, 1))\n",
       "        )\n",
       "        (1): ResnetBlock2D(\n",
       "          (norm1): GroupNorm(32, 512, eps=1e-05, affine=True)\n",
       "          (conv1): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "          (time_emb_proj): Linear(in_features=512, out_features=512, bias=True)\n",
       "          (norm2): GroupNorm(32, 512, eps=1e-05, affine=True)\n",
       "          (dropout): Dropout(p=0.0, inplace=False)\n",
       "          (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "          (nonlinearity): SiLU()\n",
       "        )\n",
       "      )\n",
       "      (downsamplers): ModuleList(\n",
       "        (0): Downsample2D(\n",
       "          (conv): Conv2d(512, 512, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1))\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (5): DownBlock2D(\n",
       "      (resnets): ModuleList(\n",
       "        (0-1): 2 x ResnetBlock2D(\n",
       "          (norm1): GroupNorm(32, 512, eps=1e-05, affine=True)\n",
       "          (conv1): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "          (time_emb_proj): Linear(in_features=512, out_features=512, bias=True)\n",
       "          (norm2): GroupNorm(32, 512, eps=1e-05, affine=True)\n",
       "          (dropout): Dropout(p=0.0, inplace=False)\n",
       "          (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "          (nonlinearity): SiLU()\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "  )\n",
       "  (up_blocks): ModuleList(\n",
       "    (0): UpBlock2D(\n",
       "      (resnets): ModuleList(\n",
       "        (0-2): 3 x ResnetBlock2D(\n",
       "          (norm1): GroupNorm(32, 1024, eps=1e-05, affine=True)\n",
       "          (conv1): Conv2d(1024, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "          (time_emb_proj): Linear(in_features=512, out_features=512, bias=True)\n",
       "          (norm2): GroupNorm(32, 512, eps=1e-05, affine=True)\n",
       "          (dropout): Dropout(p=0.0, inplace=False)\n",
       "          (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "          (nonlinearity): SiLU()\n",
       "          (conv_shortcut): Conv2d(1024, 512, kernel_size=(1, 1), stride=(1, 1))\n",
       "        )\n",
       "      )\n",
       "      (upsamplers): ModuleList(\n",
       "        (0): Upsample2D(\n",
       "          (conv): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (1): AttnUpBlock2D(\n",
       "      (attentions): ModuleList(\n",
       "        (0-2): 3 x Attention(\n",
       "          (group_norm): GroupNorm(32, 512, eps=1e-05, affine=True)\n",
       "          (to_q): Linear(in_features=512, out_features=512, bias=True)\n",
       "          (to_k): Linear(in_features=512, out_features=512, bias=True)\n",
       "          (to_v): Linear(in_features=512, out_features=512, bias=True)\n",
       "          (to_out): ModuleList(\n",
       "            (0): Linear(in_features=512, out_features=512, bias=True)\n",
       "            (1): Dropout(p=0.0, inplace=False)\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "      (resnets): ModuleList(\n",
       "        (0-1): 2 x ResnetBlock2D(\n",
       "          (norm1): GroupNorm(32, 1024, eps=1e-05, affine=True)\n",
       "          (conv1): Conv2d(1024, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "          (time_emb_proj): Linear(in_features=512, out_features=512, bias=True)\n",
       "          (norm2): GroupNorm(32, 512, eps=1e-05, affine=True)\n",
       "          (dropout): Dropout(p=0.0, inplace=False)\n",
       "          (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "          (nonlinearity): SiLU()\n",
       "          (conv_shortcut): Conv2d(1024, 512, kernel_size=(1, 1), stride=(1, 1))\n",
       "        )\n",
       "        (2): ResnetBlock2D(\n",
       "          (norm1): GroupNorm(32, 768, eps=1e-05, affine=True)\n",
       "          (conv1): Conv2d(768, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "          (time_emb_proj): Linear(in_features=512, out_features=512, bias=True)\n",
       "          (norm2): GroupNorm(32, 512, eps=1e-05, affine=True)\n",
       "          (dropout): Dropout(p=0.0, inplace=False)\n",
       "          (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "          (nonlinearity): SiLU()\n",
       "          (conv_shortcut): Conv2d(768, 512, kernel_size=(1, 1), stride=(1, 1))\n",
       "        )\n",
       "      )\n",
       "      (upsamplers): ModuleList(\n",
       "        (0): Upsample2D(\n",
       "          (conv): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (2): UpBlock2D(\n",
       "      (resnets): ModuleList(\n",
       "        (0): ResnetBlock2D(\n",
       "          (norm1): GroupNorm(32, 768, eps=1e-05, affine=True)\n",
       "          (conv1): Conv2d(768, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "          (time_emb_proj): Linear(in_features=512, out_features=256, bias=True)\n",
       "          (norm2): GroupNorm(32, 256, eps=1e-05, affine=True)\n",
       "          (dropout): Dropout(p=0.0, inplace=False)\n",
       "          (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "          (nonlinearity): SiLU()\n",
       "          (conv_shortcut): Conv2d(768, 256, kernel_size=(1, 1), stride=(1, 1))\n",
       "        )\n",
       "        (1-2): 2 x ResnetBlock2D(\n",
       "          (norm1): GroupNorm(32, 512, eps=1e-05, affine=True)\n",
       "          (conv1): Conv2d(512, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "          (time_emb_proj): Linear(in_features=512, out_features=256, bias=True)\n",
       "          (norm2): GroupNorm(32, 256, eps=1e-05, affine=True)\n",
       "          (dropout): Dropout(p=0.0, inplace=False)\n",
       "          (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "          (nonlinearity): SiLU()\n",
       "          (conv_shortcut): Conv2d(512, 256, kernel_size=(1, 1), stride=(1, 1))\n",
       "        )\n",
       "      )\n",
       "      (upsamplers): ModuleList(\n",
       "        (0): Upsample2D(\n",
       "          (conv): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (3): UpBlock2D(\n",
       "      (resnets): ModuleList(\n",
       "        (0-1): 2 x ResnetBlock2D(\n",
       "          (norm1): GroupNorm(32, 512, eps=1e-05, affine=True)\n",
       "          (conv1): Conv2d(512, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "          (time_emb_proj): Linear(in_features=512, out_features=256, bias=True)\n",
       "          (norm2): GroupNorm(32, 256, eps=1e-05, affine=True)\n",
       "          (dropout): Dropout(p=0.0, inplace=False)\n",
       "          (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "          (nonlinearity): SiLU()\n",
       "          (conv_shortcut): Conv2d(512, 256, kernel_size=(1, 1), stride=(1, 1))\n",
       "        )\n",
       "        (2): ResnetBlock2D(\n",
       "          (norm1): GroupNorm(32, 384, eps=1e-05, affine=True)\n",
       "          (conv1): Conv2d(384, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "          (time_emb_proj): Linear(in_features=512, out_features=256, bias=True)\n",
       "          (norm2): GroupNorm(32, 256, eps=1e-05, affine=True)\n",
       "          (dropout): Dropout(p=0.0, inplace=False)\n",
       "          (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "          (nonlinearity): SiLU()\n",
       "          (conv_shortcut): Conv2d(384, 256, kernel_size=(1, 1), stride=(1, 1))\n",
       "        )\n",
       "      )\n",
       "      (upsamplers): ModuleList(\n",
       "        (0): Upsample2D(\n",
       "          (conv): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (4): UpBlock2D(\n",
       "      (resnets): ModuleList(\n",
       "        (0): ResnetBlock2D(\n",
       "          (norm1): GroupNorm(32, 384, eps=1e-05, affine=True)\n",
       "          (conv1): Conv2d(384, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "          (time_emb_proj): Linear(in_features=512, out_features=128, bias=True)\n",
       "          (norm2): GroupNorm(32, 128, eps=1e-05, affine=True)\n",
       "          (dropout): Dropout(p=0.0, inplace=False)\n",
       "          (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "          (nonlinearity): SiLU()\n",
       "          (conv_shortcut): Conv2d(384, 128, kernel_size=(1, 1), stride=(1, 1))\n",
       "        )\n",
       "        (1-2): 2 x ResnetBlock2D(\n",
       "          (norm1): GroupNorm(32, 256, eps=1e-05, affine=True)\n",
       "          (conv1): Conv2d(256, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "          (time_emb_proj): Linear(in_features=512, out_features=128, bias=True)\n",
       "          (norm2): GroupNorm(32, 128, eps=1e-05, affine=True)\n",
       "          (dropout): Dropout(p=0.0, inplace=False)\n",
       "          (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "          (nonlinearity): SiLU()\n",
       "          (conv_shortcut): Conv2d(256, 128, kernel_size=(1, 1), stride=(1, 1))\n",
       "        )\n",
       "      )\n",
       "      (upsamplers): ModuleList(\n",
       "        (0): Upsample2D(\n",
       "          (conv): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (5): UpBlock2D(\n",
       "      (resnets): ModuleList(\n",
       "        (0-2): 3 x ResnetBlock2D(\n",
       "          (norm1): GroupNorm(32, 256, eps=1e-05, affine=True)\n",
       "          (conv1): Conv2d(256, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "          (time_emb_proj): Linear(in_features=512, out_features=128, bias=True)\n",
       "          (norm2): GroupNorm(32, 128, eps=1e-05, affine=True)\n",
       "          (dropout): Dropout(p=0.0, inplace=False)\n",
       "          (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "          (nonlinearity): SiLU()\n",
       "          (conv_shortcut): Conv2d(256, 128, kernel_size=(1, 1), stride=(1, 1))\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "  )\n",
       "  (mid_block): UNetMidBlock2D(\n",
       "    (attentions): ModuleList(\n",
       "      (0): Attention(\n",
       "        (group_norm): GroupNorm(32, 512, eps=1e-05, affine=True)\n",
       "        (to_q): Linear(in_features=512, out_features=512, bias=True)\n",
       "        (to_k): Linear(in_features=512, out_features=512, bias=True)\n",
       "        (to_v): Linear(in_features=512, out_features=512, bias=True)\n",
       "        (to_out): ModuleList(\n",
       "          (0): Linear(in_features=512, out_features=512, bias=True)\n",
       "          (1): Dropout(p=0.0, inplace=False)\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (resnets): ModuleList(\n",
       "      (0-1): 2 x ResnetBlock2D(\n",
       "        (norm1): GroupNorm(32, 512, eps=1e-05, affine=True)\n",
       "        (conv1): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "        (time_emb_proj): Linear(in_features=512, out_features=512, bias=True)\n",
       "        (norm2): GroupNorm(32, 512, eps=1e-05, affine=True)\n",
       "        (dropout): Dropout(p=0.0, inplace=False)\n",
       "        (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "        (nonlinearity): SiLU()\n",
       "      )\n",
       "    )\n",
       "  )\n",
       "  (conv_norm_out): GroupNorm(32, 128, eps=1e-05, affine=True)\n",
       "  (conv_act): SiLU()\n",
       "  (conv_out): Conv2d(128, 3, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       ")"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
