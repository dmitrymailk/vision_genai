{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "c5aaba2e",
   "metadata": {},
   "source": [
    "https://docs.pytorch.org/docs/stable/fx.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "fd0bf366",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "graph():\n",
      "    %x : [num_users=1] = placeholder[target=x]\n",
      "    %param : [num_users=1] = get_attr[target=param]\n",
      "    %add : [num_users=1] = call_function[target=operator.add](args = (%x, %param), kwargs = {})\n",
      "    %linear : [num_users=1] = call_module[target=linear](args = (%add,), kwargs = {})\n",
      "    %clamp : [num_users=1] = call_method[target=clamp](args = (%linear,), kwargs = {min: 0.0, max: 1.0})\n",
      "    return clamp\n",
      "\n",
      "\n",
      "\n",
      "def forward(self, x):\n",
      "    param = self.param\n",
      "    add = x + param;  x = param = None\n",
      "    linear = self.linear(add);  add = None\n",
      "    clamp = linear.clamp(min = 0.0, max = 1.0);  linear = None\n",
      "    return clamp\n",
      "    \n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'\\ndef forward(self, x):\\n    param = self.param\\n    add = x + param;  x = param = None\\n    linear = self.linear(add);  add = None\\n    clamp = linear.clamp(min = 0.0, max = 1.0);  linear = None\\n    return clamp\\n'"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "\n",
    "# Simple module for demonstration\n",
    "class MyModule(torch.nn.Module):\n",
    "    def __init__(self) -> None:\n",
    "        super().__init__()\n",
    "        self.param = torch.nn.Parameter(torch.rand(3, 4))\n",
    "        self.linear = torch.nn.Linear(4, 5)\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.linear(x + self.param).clamp(min=0.0, max=1.0)\n",
    "\n",
    "\n",
    "module = MyModule()\n",
    "\n",
    "from torch.fx import symbolic_trace\n",
    "\n",
    "# Symbolic tracing frontend - captures the semantics of the module\n",
    "symbolic_traced: torch.fx.GraphModule = symbolic_trace(module)\n",
    "\n",
    "# High-level intermediate representation (IR) - Graph representation\n",
    "print(symbolic_traced.graph)\n",
    "\"\"\"\n",
    "graph():\n",
    "    %x : [num_users=1] = placeholder[target=x]\n",
    "    %param : [num_users=1] = get_attr[target=param]\n",
    "    %add : [num_users=1] = call_function[target=operator.add](args = (%x, %param), kwargs = {})\n",
    "    %linear : [num_users=1] = call_module[target=linear](args = (%add,), kwargs = {})\n",
    "    %clamp : [num_users=1] = call_method[target=clamp](args = (%linear,), kwargs = {min: 0.0, max: 1.0})\n",
    "    return clamp\n",
    "\"\"\"\n",
    "\n",
    "# Code generation - valid Python code\n",
    "print(symbolic_traced.code)\n",
    "\"\"\"\n",
    "def forward(self, x):\n",
    "    param = self.param\n",
    "    add = x + param;  x = param = None\n",
    "    linear = self.linear(add);  add = None\n",
    "    clamp = linear.clamp(min = 0.0, max = 1.0);  linear = None\n",
    "    return clamp\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3b4ffd72",
   "metadata": {},
   "outputs": [],
   "source": [
    "def forward(self, x):\n",
    "    param = self.param\n",
    "    add = x + param\n",
    "    x = param = None\n",
    "    linear = self.linear(add)\n",
    "    add = None\n",
    "    clamp = linear.clamp(min=0.0, max=1.0)\n",
    "    linear = None\n",
    "    return clamp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "98b68fd5",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install tabulate -q"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "c8148a3f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "opcode         name           target                                                   args                kwargs\n",
      "-------------  -------------  -------------------------------------------------------  ------------------  -----------\n",
      "placeholder    x              x                                                        ()                  {}\n",
      "get_attr       linear_weight  linear.weight                                            ()                  {}\n",
      "call_function  add            <built-in function add>                                  (x, linear_weight)  {}\n",
      "call_module    linear         linear                                                   (add,)              {}\n",
      "call_method    relu           relu                                                     (linear,)           {}\n",
      "call_function  sum_1          <built-in method sum of type object at 0x7c28a9ef6f80>   (relu,)             {'dim': -1}\n",
      "call_function  topk           <built-in method topk of type object at 0x7c28a9ef6f80>  (sum_1, 3)          {}\n",
      "output         output         output                                                   (topk,)             {}\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.fx\n",
    "\n",
    "\n",
    "class MyModule(torch.nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.param = torch.nn.Parameter(torch.rand(3, 4))\n",
    "        self.linear = torch.nn.Linear(4, 5)\n",
    "\n",
    "    def forward(self, x):\n",
    "        return torch.topk(\n",
    "            torch.sum(self.linear(x + self.linear.weight).relu(), dim=-1), 3\n",
    "        )\n",
    "\n",
    "\n",
    "m = MyModule()\n",
    "gm = torch.fx.symbolic_trace(m)\n",
    "\n",
    "gm.graph.print_tabular()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "170d514e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch import fx\n",
    "\n",
    "\n",
    "# Sample module\n",
    "class M(torch.nn.Module):\n",
    "    def forward(self, x, y):\n",
    "        return torch.add(x, y)\n",
    "\n",
    "\n",
    "def transform(m: torch.nn.Module, tracer_class: type = fx.Tracer) -> torch.nn.Module:\n",
    "    graph: fx.Graph = tracer_class().trace(m)\n",
    "    # FX represents its Graph as an ordered list of\n",
    "    # nodes, so we can iterate through them.\n",
    "    for node in graph.nodes:\n",
    "        # Checks if we're calling a function (i.e:\n",
    "        # torch.add)\n",
    "        if node.op == \"call_function\":\n",
    "            # The target attribute is the function\n",
    "            # that call_function calls.\n",
    "            if node.target == torch.add:\n",
    "                node.target = torch.mul\n",
    "\n",
    "    graph.lint()  # Does some checks to make sure the\n",
    "    # Graph is well-formed.\n",
    "\n",
    "    return fx.GraphModule(m, graph)  #"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
