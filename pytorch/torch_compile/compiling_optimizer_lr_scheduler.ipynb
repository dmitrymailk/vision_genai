{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "https://docs.pytorch.org/tutorials/recipes/compiling_optimizer_lr_scheduler.html"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "UDVB3wVWzQSL"
      },
      "outputs": [],
      "source": [
        "# For tips on running notebooks in Google Colab, see\n",
        "# https://pytorch.org/tutorials/beginner/colab\n",
        "# %matplotlib inline"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oUuAEsSazQSO"
      },
      "source": [
        "(beta) Running the compiled optimizer with an LR Scheduler\n",
        "==========================================================\n",
        "\n",
        "**Author:** [Michael Lazos](https://github.com/mlazos)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ap34AAjLzQSP"
      },
      "source": [
        "The optimizer is a key algorithm for training any deep learning model.\n",
        "In this example, we will show how to pair the optimizer, which has been\n",
        "compiled using `torch.compile`, with the LR schedulers to accelerate\n",
        "training convergence.\n",
        "\n",
        "<div style=\"background-color: #54c7ec; color: #fff; font-weight: 700; padding-left: 10px; padding-top: 5px; padding-bottom: 5px\"><strong>NOTE:</strong></div>\n",
        "\n",
        "<div style=\"background-color: #f3f4f7; padding-left: 10px; padding-top: 10px; padding-bottom: 10px; padding-right: 10px\">\n",
        "\n",
        "<p>This tutorial requires PyTorch 2.3.0 or later.</p>\n",
        "\n",
        "</div>\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "k3aIDk0szQSP"
      },
      "source": [
        "Model Setup\n",
        "===========\n",
        "\n",
        "For this example, we\\'ll use a simple sequence of linear layers.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "UWAcXzBEzQSP"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "\n",
        "# Create simple model\n",
        "model = torch.nn.Sequential(\n",
        "    *[torch.nn.Linear(1024, 1024, False, device=\"cuda\") for _ in range(10)]\n",
        ")\n",
        "input = torch.rand(1024, device=\"cuda\")\n",
        "\n",
        "# run forward pass\n",
        "output = model(input)\n",
        "\n",
        "# run backward to populate the grads for our optimizer below\n",
        "output.sum().backward()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "v7fZkLoRzQSQ"
      },
      "source": [
        "Setting up and running the compiled optimizer with LR Scheduler\n",
        "===============================================================\n",
        "\n",
        "In this section, we\\'ll use the Adam optimizer with LinearLR Scheduler\n",
        "and create a helper function to wrap the `step()` call for each of them\n",
        "in `torch.compile()`.\n",
        "\n",
        "<div style=\"background-color: #54c7ec; color: #fff; font-weight: 700; padding-left: 10px; padding-top: 5px; padding-bottom: 5px\"><strong>NOTE:</strong></div>\n",
        "\n",
        "<div style=\"background-color: #f3f4f7; padding-left: 10px; padding-top: 10px; padding-bottom: 10px; padding-right: 10px\">\n",
        "\n",
        "<p><code>torch.compile</code> is only supported on CUDA devices that have a compute capability of 7.0 or higher.</p>\n",
        "\n",
        "</div>\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "LiMYJkMXzQSQ"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "W0614 23:38:43.202000 486588 site-packages/torch/_logging/_internal.py:1130] [1/0] Profiler function <class 'torch.autograd.profiler.record_function'> will be ignored\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "tensor(0.0047)\n",
            "tensor(0.0060)\n",
            "tensor(0.0073)\n",
            "tensor(0.0087)\n",
            "tensor(0.0100)\n"
          ]
        }
      ],
      "source": [
        "# exit cleanly if we are on a device that doesn't support ``torch.compile``\n",
        "# if torch.cuda.get_device_capability() < (7, 0):\n",
        "#     print(\"Exiting because torch.compile is not supported on this device.\")\n",
        "#     import sys\n",
        "#     sys.exit(0)\n",
        "\n",
        "# !!! IMPORTANT !!! Wrap the lr in a Tensor if we are pairing the\n",
        "# the optimizer with an LR Scheduler.\n",
        "# Without this, torch.compile will recompile as the value of the LR\n",
        "# changes.\n",
        "opt = torch.optim.Adam(model.parameters(), lr=torch.tensor(0.01))\n",
        "sched = torch.optim.lr_scheduler.LinearLR(opt, total_iters=5)\n",
        "\n",
        "@torch.compile(fullgraph=False)\n",
        "def fn():\n",
        "    opt.step()\n",
        "    sched.step()\n",
        "\n",
        "\n",
        "# Warmup runs to compile the function\n",
        "for _ in range(5):\n",
        "    fn()\n",
        "    print(opt.param_groups[0][\"lr\"])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ba3ntKLIzQSQ"
      },
      "source": [
        "Extension: What happens with a non-tensor LR?\n",
        "=============================================\n",
        "\n",
        "For the curious, we will show how to peek into what happens with\n",
        "`torch.compile` when we don\\'t wrap the LR in a tensor.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "bTd3GMFqzQSR"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "V0614 23:39:21.699000 486588 site-packages/torch/_dynamo/guards.py:2997] [1/1] [__recompiles] Recompiling function wrapper in /opt/conda/lib/python3.11/site-packages/torch/optim/optimizer.py:465\n",
            "V0614 23:39:21.699000 486588 site-packages/torch/_dynamo/guards.py:2997] [1/1] [__recompiles]     triggered by the following guard failure(s):\n",
            "V0614 23:39:21.699000 486588 site-packages/torch/_dynamo/guards.py:2997] [1/1] [__recompiles]     - 1/0: Cache line invalidated because L['args'][0] got deallocated\n",
            "V0614 23:39:21.706000 486588 site-packages/torch/_dynamo/guards.py:2997] [2/1] [__recompiles] Recompiling function step in /opt/conda/lib/python3.11/site-packages/torch/optim/adam.py:212\n",
            "V0614 23:39:21.706000 486588 site-packages/torch/_dynamo/guards.py:2997] [2/1] [__recompiles]     triggered by the following guard failure(s):\n",
            "V0614 23:39:21.706000 486588 site-packages/torch/_dynamo/guards.py:2997] [2/1] [__recompiles]     - 2/0: Cache line invalidated because L['self'] got deallocated\n",
            "V0614 23:39:23.196000 486588 site-packages/torch/_dynamo/guards.py:2997] [2/2] [__recompiles] Recompiling function step in /opt/conda/lib/python3.11/site-packages/torch/optim/adam.py:212\n",
            "V0614 23:39:23.196000 486588 site-packages/torch/_dynamo/guards.py:2997] [2/2] [__recompiles]     triggered by the following guard failure(s):\n",
            "V0614 23:39:23.196000 486588 site-packages/torch/_dynamo/guards.py:2997] [2/2] [__recompiles]     - 2/1: ___as_tensor(self.param_groups[0]['lr']).item() == 0.003333333333333333  # (unknown source ___as_tensor(self.param_groups[0]['lr']).item(), please file a bug)\n",
            "V0614 23:39:23.196000 486588 site-packages/torch/_dynamo/guards.py:2997] [2/2] [__recompiles]     - 2/0: Cache line invalidated because L['self'] got deallocated\n",
            "V0614 23:39:24.211000 486588 site-packages/torch/_dynamo/guards.py:2997] [2/3] [__recompiles] Recompiling function step in /opt/conda/lib/python3.11/site-packages/torch/optim/adam.py:212\n",
            "V0614 23:39:24.211000 486588 site-packages/torch/_dynamo/guards.py:2997] [2/3] [__recompiles]     triggered by the following guard failure(s):\n",
            "V0614 23:39:24.211000 486588 site-packages/torch/_dynamo/guards.py:2997] [2/3] [__recompiles]     - 2/2: ___as_tensor(self.param_groups[0]['lr']).item() == 0.004666666666666667  # (unknown source ___as_tensor(self.param_groups[0]['lr']).item(), please file a bug)\n",
            "V0614 23:39:24.211000 486588 site-packages/torch/_dynamo/guards.py:2997] [2/3] [__recompiles]     - 2/1: ___as_tensor(self.param_groups[0]['lr']).item() == 0.003333333333333333  # (unknown source ___as_tensor(self.param_groups[0]['lr']).item(), please file a bug)\n",
            "V0614 23:39:24.211000 486588 site-packages/torch/_dynamo/guards.py:2997] [2/3] [__recompiles]     - 2/0: Cache line invalidated because L['self'] got deallocated\n",
            "V0614 23:39:25.238000 486588 site-packages/torch/_dynamo/guards.py:2997] [2/4] [__recompiles] Recompiling function step in /opt/conda/lib/python3.11/site-packages/torch/optim/adam.py:212\n",
            "V0614 23:39:25.238000 486588 site-packages/torch/_dynamo/guards.py:2997] [2/4] [__recompiles]     triggered by the following guard failure(s):\n",
            "V0614 23:39:25.238000 486588 site-packages/torch/_dynamo/guards.py:2997] [2/4] [__recompiles]     - 2/3: ___as_tensor(self.param_groups[0]['lr']).item() == 0.006000000000000001  # (unknown source ___as_tensor(self.param_groups[0]['lr']).item(), please file a bug)\n",
            "V0614 23:39:25.238000 486588 site-packages/torch/_dynamo/guards.py:2997] [2/4] [__recompiles]     - 2/2: ___as_tensor(self.param_groups[0]['lr']).item() == 0.004666666666666667  # (unknown source ___as_tensor(self.param_groups[0]['lr']).item(), please file a bug)\n",
            "V0614 23:39:25.238000 486588 site-packages/torch/_dynamo/guards.py:2997] [2/4] [__recompiles]     - 2/1: ___as_tensor(self.param_groups[0]['lr']).item() == 0.003333333333333333  # (unknown source ___as_tensor(self.param_groups[0]['lr']).item(), please file a bug)\n",
            "V0614 23:39:25.238000 486588 site-packages/torch/_dynamo/guards.py:2997] [2/4] [__recompiles]     - 2/0: Cache line invalidated because L['self'] got deallocated\n",
            "V0614 23:39:26.387000 486588 site-packages/torch/_dynamo/guards.py:2997] [2/5] [__recompiles] Recompiling function step in /opt/conda/lib/python3.11/site-packages/torch/optim/adam.py:212\n",
            "V0614 23:39:26.387000 486588 site-packages/torch/_dynamo/guards.py:2997] [2/5] [__recompiles]     triggered by the following guard failure(s):\n",
            "V0614 23:39:26.387000 486588 site-packages/torch/_dynamo/guards.py:2997] [2/5] [__recompiles]     - 2/4: ___as_tensor(self.param_groups[0]['lr']).item() == 0.007333333333333335  # (unknown source ___as_tensor(self.param_groups[0]['lr']).item(), please file a bug)\n",
            "V0614 23:39:26.387000 486588 site-packages/torch/_dynamo/guards.py:2997] [2/5] [__recompiles]     - 2/3: ___as_tensor(self.param_groups[0]['lr']).item() == 0.006000000000000001  # (unknown source ___as_tensor(self.param_groups[0]['lr']).item(), please file a bug)\n",
            "V0614 23:39:26.387000 486588 site-packages/torch/_dynamo/guards.py:2997] [2/5] [__recompiles]     - 2/2: ___as_tensor(self.param_groups[0]['lr']).item() == 0.004666666666666667  # (unknown source ___as_tensor(self.param_groups[0]['lr']).item(), please file a bug)\n",
            "V0614 23:39:26.387000 486588 site-packages/torch/_dynamo/guards.py:2997] [2/5] [__recompiles]     - 2/1: ___as_tensor(self.param_groups[0]['lr']).item() == 0.003333333333333333  # (unknown source ___as_tensor(self.param_groups[0]['lr']).item(), please file a bug)\n",
            "V0614 23:39:26.387000 486588 site-packages/torch/_dynamo/guards.py:2997] [2/5] [__recompiles]     - 2/0: Cache line invalidated because L['self'] got deallocated\n"
          ]
        }
      ],
      "source": [
        "# No longer wrap the LR in a tensor here\n",
        "opt = torch.optim.Adam(model.parameters(), lr=0.01)\n",
        "sched = torch.optim.lr_scheduler.LinearLR(opt, total_iters=5)\n",
        "\n",
        "@torch.compile(fullgraph=False)\n",
        "def fn():\n",
        "    opt.step()\n",
        "    sched.step()\n",
        "\n",
        "# Setup logging to view recompiles\n",
        "torch._logging.set_logs(recompiles=True)\n",
        "\n",
        "# Warmup runs to compile the function\n",
        "# We will now recompile on each iteration\n",
        "# as the value of the lr is mutated.\n",
        "for _ in range(5):\n",
        "    fn()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hELwbVk3zQSR"
      },
      "source": [
        "With this example, we can see that we recompile the optimizer a few\n",
        "times due to the guard failure on the `lr` in `param_groups[0]`.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gXi8m6-rzQSR"
      },
      "source": [
        "Conclusion\n",
        "==========\n",
        "\n",
        "In this tutorial we showed how to pair the optimizer compiled with\n",
        "`torch.compile` with an LR Scheduler to accelerate training convergence.\n",
        "We used a model consisting of a simple sequence of linear layers with\n",
        "the Adam optimizer paired with a LinearLR scheduler to demonstrate the\n",
        "LR changing across iterations.\n",
        "\n",
        "See also:\n",
        "\n",
        "-   [Compiled optimizer\n",
        "    tutorial](https://pytorch.org/tutorials/recipes/compiling_optimizer.html) -\n",
        "    an intro into the compiled optimizer.\n",
        "-   [Compiling the optimizer with\n",
        "    PT2](https://dev-discuss.pytorch.org/t/compiling-the-optimizer-with-pt2/1669) -\n",
        "    deeper technical details on the compiled optimizer.\n"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "base",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.12"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
