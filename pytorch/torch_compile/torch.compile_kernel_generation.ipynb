{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "9126e495",
   "metadata": {},
   "source": [
    "https://shashankprasanna.com/workshops/a-tour-of-pytorch2/4_inspecting_torch_compile/inspecting_torch_compile/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "f4381551",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install matplotlib -q"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "0b71cee6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(True, device='cuda:0')"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n",
    "import math\n",
    "import os\n",
    "import matplotlib.pyplot as plt\n",
    "from torch import optim\n",
    "import torch._dynamo\n",
    "from torchvision import models\n",
    "from torch.profiler import profile, record_function, ProfilerActivity\n",
    "\n",
    "pi = math.pi\n",
    "device = torch.device(\"cuda\") if torch.cuda.is_available() else \"cpu\"\n",
    "\n",
    "\n",
    "def fn(x):\n",
    "    return torch.sin(x) ** 2 + torch.cos(x) ** 2\n",
    "\n",
    "\n",
    "torch.manual_seed(0)\n",
    "x = torch.rand(1000000, requires_grad=True).to(device)\n",
    "\n",
    "out = fn(x)\n",
    "torch.linalg.norm(out - 1) <= 1e-4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "8af5d14c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "class GraphModule(torch.nn.Module):\n",
      "    def forward(self, L_x_: \"f32[1000000]\"):\n",
      "        l_x_ = L_x_\n",
      "        \n",
      "         # File: /tmp/ipykernel_745/1194271619.py:15 in fn, code: return torch.sin(x) ** 2 + torch.cos(x) ** 2\n",
      "        sin: \"f32[1000000]\" = torch.sin(l_x_)\n",
      "        pow_1: \"f32[1000000]\" = sin ** 2;  sin = None\n",
      "        cos: \"f32[1000000]\" = torch.cos(l_x_);  l_x_ = None\n",
      "        pow_2: \"f32[1000000]\" = cos ** 2;  cos = None\n",
      "        add: \"f32[1000000]\" = pow_1 + pow_2;  pow_1 = pow_2 = None\n",
      "        return (add,)\n",
      "        \n"
     ]
    }
   ],
   "source": [
    "torch.manual_seed(0)\n",
    "x = torch.rand(1000000, requires_grad=True).to(device)\n",
    "\n",
    "\n",
    "def inspect_backend(gm, sample_inputs):\n",
    "    gm.print_readable()\n",
    "    return gm.forward\n",
    "\n",
    "\n",
    "torch._dynamo.reset()\n",
    "compiled_model = torch.compile(fn, backend=inspect_backend)\n",
    "\n",
    "out = compiled_model(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "d21ea0a7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "class GraphModule(torch.nn.Module):\n",
      "    def forward(self, primals_1: \"f32[1000000]\"):\n",
      "         # File: /tmp/ipykernel_745/1194271619.py:15 in fn, code: return torch.sin(x) ** 2 + torch.cos(x) ** 2\n",
      "        sin: \"f32[1000000]\" = torch.ops.aten.sin.default(primals_1)\n",
      "        pow_1: \"f32[1000000]\" = torch.ops.aten.pow.Tensor_Scalar(sin, 2)\n",
      "        cos: \"f32[1000000]\" = torch.ops.aten.cos.default(primals_1)\n",
      "        pow_2: \"f32[1000000]\" = torch.ops.aten.pow.Tensor_Scalar(cos, 2)\n",
      "        add: \"f32[1000000]\" = torch.ops.aten.add.Tensor(pow_1, pow_2);  pow_1 = pow_2 = None\n",
      "        return (add, primals_1, sin, cos)\n",
      "        \n",
      "class GraphModule(torch.nn.Module):\n",
      "    def forward(self, primals_1: \"f32[1000000]\", sin: \"f32[1000000]\", cos: \"f32[1000000]\", tangents_1: \"f32[1000000]\"):\n",
      "         # File: /tmp/ipykernel_745/1194271619.py:15 in fn, code: return torch.sin(x) ** 2 + torch.cos(x) ** 2\n",
      "        pow_3: \"f32[1000000]\" = torch.ops.aten.pow.Tensor_Scalar(cos, 1.0);  cos = None\n",
      "        mul: \"f32[1000000]\" = torch.ops.aten.mul.Scalar(pow_3, 2.0);  pow_3 = None\n",
      "        mul_1: \"f32[1000000]\" = torch.ops.aten.mul.Tensor(tangents_1, mul);  mul = None\n",
      "        sin_1: \"f32[1000000]\" = torch.ops.aten.sin.default(primals_1)\n",
      "        neg: \"f32[1000000]\" = torch.ops.aten.neg.default(sin_1);  sin_1 = None\n",
      "        mul_2: \"f32[1000000]\" = torch.ops.aten.mul.Tensor(mul_1, neg);  mul_1 = neg = None\n",
      "        pow_4: \"f32[1000000]\" = torch.ops.aten.pow.Tensor_Scalar(sin, 1.0);  sin = None\n",
      "        mul_3: \"f32[1000000]\" = torch.ops.aten.mul.Scalar(pow_4, 2.0);  pow_4 = None\n",
      "        mul_4: \"f32[1000000]\" = torch.ops.aten.mul.Tensor(tangents_1, mul_3);  tangents_1 = mul_3 = None\n",
      "        cos_1: \"f32[1000000]\" = torch.ops.aten.cos.default(primals_1);  primals_1 = None\n",
      "        mul_5: \"f32[1000000]\" = torch.ops.aten.mul.Tensor(mul_4, cos_1);  mul_4 = cos_1 = None\n",
      "        \n",
      "         # File: /tmp/ipykernel_745/1194271619.py:15 in fn, code: return torch.sin(x) ** 2 + torch.cos(x) ** 2\n",
      "        add_1: \"f32[1000000]\" = torch.ops.aten.add.Tensor(mul_2, mul_5);  mul_2 = mul_5 = None\n",
      "        return (add_1,)\n",
      "        \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.11/site-packages/torch/_functorch/_aot_autograd/utils.py:130: UserWarning: Your compiler for AOTAutograd is returning a function that doesn't take boxed arguments. Please wrap it with functorch.compile.make_boxed_func or handle the boxed arguments yourself. See https://github.com/pytorch/pytorch/pull/83137#issuecomment-1211320670 for rationale.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "import torch._dynamo\n",
    "from torch.fx.passes.graph_drawer import FxGraphDrawer\n",
    "from torch._functorch.aot_autograd import aot_module_simplified\n",
    "\n",
    "\n",
    "def inspect_backend(gm, sample_inputs):\n",
    "    def fw(gm, sample_inputs):\n",
    "        gm.print_readable()\n",
    "        # g = FxGraphDrawer(gm, \"fn\")\n",
    "        # with open(\"forward.svg\", \"wb\") as f:\n",
    "        #     f.write(g.get_dot_graph().create_svg())\n",
    "        return gm.forward\n",
    "\n",
    "    def bw(gm, sample_inputs):\n",
    "        gm.print_readable()\n",
    "        # g = FxGraphDrawer(gm, \"fn\")\n",
    "        # with open(\"backward.svg\", \"wb\") as f:\n",
    "        #     f.write(g.get_dot_graph().create_svg())\n",
    "        return gm.forward\n",
    "\n",
    "    # Invoke AOTAutograd\n",
    "    return aot_module_simplified(gm, sample_inputs, fw_compiler=fw, bw_compiler=bw)\n",
    "\n",
    "\n",
    "torch._dynamo.reset()\n",
    "compiled_model = torch.compile(fn, backend=inspect_backend)\n",
    "\n",
    "out = compiled_model(x).sum().backward()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "2cfd2844",
   "metadata": {},
   "outputs": [],
   "source": [
    "def forward(\n",
    "    self,\n",
    "    primals_1: \"f32[1000000]\",\n",
    "    sin: \"f32[1000000]\",\n",
    "    cos: \"f32[1000000]\",\n",
    "    tangents_1: \"f32[1000000]\",\n",
    "):\n",
    "    # File: /tmp/ipykernel_704917/1194271619.py:15 in fn, code: return torch.sin(x) ** 2 + torch.cos(x) ** 2\n",
    "    pow_3: \"f32[1000000]\" = torch.ops.aten.pow.Tensor_Scalar(cos, 1.0)\n",
    "    cos = None\n",
    "    mul: \"f32[1000000]\" = torch.ops.aten.mul.Scalar(pow_3, 2.0)\n",
    "    pow_3 = None\n",
    "    mul_1: \"f32[1000000]\" = torch.ops.aten.mul.Tensor(tangents_1, mul)\n",
    "    mul = None\n",
    "    sin_1: \"f32[1000000]\" = torch.ops.aten.sin.default(primals_1)\n",
    "    neg: \"f32[1000000]\" = torch.ops.aten.neg.default(sin_1)\n",
    "    sin_1 = None\n",
    "    mul_2: \"f32[1000000]\" = torch.ops.aten.mul.Tensor(mul_1, neg)\n",
    "    mul_1 = neg = None\n",
    "    pow_4: \"f32[1000000]\" = torch.ops.aten.pow.Tensor_Scalar(sin, 1.0)\n",
    "    sin = None\n",
    "    mul_3: \"f32[1000000]\" = torch.ops.aten.mul.Scalar(pow_4, 2.0)\n",
    "    pow_4 = None\n",
    "    mul_4: \"f32[1000000]\" = torch.ops.aten.mul.Tensor(tangents_1, mul_3)\n",
    "    tangents_1 = mul_3 = None\n",
    "    cos_1: \"f32[1000000]\" = torch.ops.aten.cos.default(primals_1)\n",
    "    primals_1 = None\n",
    "    mul_5: \"f32[1000000]\" = torch.ops.aten.mul.Tensor(mul_4, cos_1)\n",
    "    mul_4 = cos_1 = None\n",
    "\n",
    "    # File: /tmp/ipykernel_704917/1194271619.py:15 in fn, code: return torch.sin(x) ** 2 + torch.cos(x) ** 2\n",
    "    add_1: \"f32[1000000]\" = torch.ops.aten.add.Tensor(mul_2, mul_5)\n",
    "    mul_2 = mul_5 = None\n",
    "    return (add_1,)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "e6df8454",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "W0615 23:35:22.046000 745 site-packages/torch/_inductor/debug.py:72] [0/0] draw_buffers() requires `graphviz` package\n",
      "W0615 23:35:22.261000 745 site-packages/torch/_inductor/debug.py:454] [0/0] model__6_forward_11 debug trace: /code/torch_compile_debug/run_2025_06_15_23_35_22_005250-pid_745/torchinductor/model__6_forward_11.0\n",
      "W0615 23:35:22.316000 745 site-packages/torch/_inductor/debug.py:72] [0/0] draw_buffers() requires `graphviz` package\n",
      "W0615 23:35:22.513000 745 site-packages/torch/_inductor/debug.py:454] [0/0] model__6_backward_13 debug trace: /code/torch_compile_debug/run_2025_06_15_23_35_22_005250-pid_745/torchinductor/model__6_backward_13.1\n"
     ]
    }
   ],
   "source": [
    "torch._dynamo.reset()\n",
    "x = x.to(device)\n",
    "compiled_model = torch.compile(\n",
    "    fn,\n",
    "    # backend=\"inductor\",\n",
    "    options={\"trace.enabled\": True, \"trace.graph_diagram\": True},\n",
    ")\n",
    "\n",
    "out = compiled_model(x).sum().backward()\n",
    "#"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "9d0b48d8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "device(type='cuda')"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "436725af",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([0.4963, 0.7682, 0.0885,  ..., 0.6331, 0.9980, 0.5057], device='cuda:0',\n",
       "       grad_fn=<ToCopyBackward0>)"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
