- [whisper.cpp](https://github.com/ggerganov/whisper.cpp)
- [EAGLE](https://github.com/SafeAILab/EAGLE)
- [koboldcpp](https://github.com/LostRuins/koboldcpp)
- [speculative-decoding](https://github.com/lucidrains/speculative-decoding)
- [ts_server](https://bellard.org/ts_server/)
- [fastT5](https://github.com/Ki6an/fastT5) - —Ä–∞–±–æ—Ç–∞–µ—Ç —Å –Ω–µ–∫–æ—Ç–æ—Ä—ã–º–∏ –∏—Å–ø—Ä–∞–≤–ª–µ–Ω–∏—è–º–∏ —Å—Ç–∞—Ä–æ–≥–æ –∞–ø–∏. –ù–µ —Ä–∞–±–æ—Ç–∞–µ—Ç —Å GPU.
- [Neural Network Compression: Techniques for Reducing Size and ImprovingLatency](https://youtu.be/-QCbDjpIM2I?si=1Zv6-RxcuilKWXIl)
- [https://vgel.me/posts/faster-inference/](https://vgel.me/posts/faster-inference/)
- [UMIA-Group/FourierTransformer: The official Pytorch implementation of the paper "Fourier Transformer: Fast Long Range Modeling by Removing Sequence Redundancy with FFT Operator" (ACL 2023 Findings)](https://github.com/LUMIA-Group/FourierTransformer)
- [Neural Network Distiller by Intel AI Lab: a Python package for neural network compression research.](https://github.com/IntelLabs/distiller/)
- [Exploring Extreme Parameter Compression for Pre-trained Language Models](https://arxiv.org/pdf/2205.10036.pdf)
- [EdgeBERT: Sentence-Level Energy Optimizations for Latency-Aware Multi-Task NLP Inference](https://arxiv.org/pdf/2011.14203.pdf)
- [https://github.com/ELS-RD/kernl/tree/main](https://github.com/ELS-RD/kernl/tree/main) - –∑–∞–≤–∏—Å–∞–µ—Ç –Ω–∞ –±–æ–ª—å—à–æ–π –º–æ–¥–µ–ª–∏, –Ω–µ –∑–∞—Ä–∞–±–æ—Ç–∞–ª–æ.
- [Fast & Simple repository for pre-training and fine-tuning T5-style models](https://github.com/PiotrNawrot/nanoT5)
- [Fused Softmax triton](https://triton-lang.org/main/getting-started/tutorials/02-fused-softmax.html)
- [PyTorch compile to speed up inference on Llama 2](https://pytorch.org/blog/pytorch-compile-to-speed-up-inference/)
- [Fuse Modules Recipe](https://pytorch.org/tutorials/recipes/fuse.html)
- [Dynamic Quantization pytorch](https://pytorch.org/tutorials/recipes/recipes/dynamic_quantization.html) - —Ö–æ—Ä–æ—à–æ —Ä–∞–±–æ—Ç–∞–µ—Ç
- [Introduction to Quantization on PyTorch](https://pytorch.org/blog/introduction-to-quantization-on-pytorch/)
- [Static Quantization with Eager Mode in PyTorch](https://pytorch.org/tutorials/advanced/static_quantization_tutorial.html)
- [QUANTIZATION pytorch](https://pytorch.org/docs/stable/quantization.html)
- [Quantized Transfer Learning for Computer Vision Tutorial](https://pytorch.org/tutorials/intermediate/quantized_transfer_learning_tutorial.html)
- [Pytorch Mobile Performance Recipes](https://pytorch.org/tutorials/recipes/mobile_perf.html)
- [A BetterTransformer for Fast Transformer Inference](https://pytorch.org/blog/a-better-transformer-for-fast-transformer-encoder-inference/)
- [fastseq An efficient implementation of the popular sequence models for text generation, translation tasks.](https://github.com/microsoft/fastseq) - –Ω–µ –ø–æ–ª—É—á–∏–ª–æ—Å—å –ø–æ–ª—É—á–∏—Ç—å –Ω–∏–∫–∞–∫–æ–≥–æ –ø—Ä–∏—Ä–æ—Å—Ç–∞ –Ω–∞ —Ü–µ–ª–µ–≤–æ–π –∑–∞–¥–∞—á–µ. —Ç—Ä–µ–±—É–µ—Ç –¥–æ–ø–æ–ª–Ω–∏—Ç–µ–ª—å–Ω–æ–≥–æ –∏–∑—É—á–µ–Ω–∏—è.
- [Accelerating HuggingFace T5 Inference with TensorRT](https://github.com/NVIDIA/TensorRT/blob/release/8.2/demo/HuggingFace/notebooks/t5.ipynb)
- [TensorRT-LLM provides users with an easy-to-use Python API to define Large Language Models (LLMs)](https://github.com/NVIDIA/TensorRT-LLM/tree/main)
- [https://github.com/NVIDIA/FasterTransformer](https://github.com/NVIDIA/FasterTransformer)
- [AOTInductor: Ahead-Of-Time Compilation for Torch.Export-ed Models](https://pytorch.org/docs/main/torch.compiler_aot_inductor.html)
- [PYTORCH MOBILE End-to-end workflow from Training to Deployment for iOS and Android mobile devices](https://pytorch.org/mobile/home/)
- [An easy to use PyTorch to TensorRT converter](https://github.com/NVIDIA-AI-IOT/torch2trt)
- [Lite Transformer with Long-Short Range Attention](https://github.com/mit-han-lab/lite-transformer)
- [EdgeTran: Co-designing Transformers for Efficient Inference on Mobile Edge Platforms](https://arxiv.org/pdf/2303.13745.pdf)
- [MOBILEVIT: LIGHT-WEIGHT, GENERAL-PURPOSE, AND MOBILE-FRIENDLY VISION TRANSFORMER](https://arxiv.org/pdf/2110.02178.pdf)
- [EfficientViT: Multi-Scale Linear Attention for High-Resolution Dense Prediction](https://hanlab.mit.edu/projects/efficientvit)
- [Efficient AI Inference & Serving](https://github.com/hpcaitech/SwiftInfer)
- [[ICLR 2024] Efficient Streaming Language Models with Attention Sinks](https://github.com/mit-han-lab/streaming-llm)
- [TinyChat: Efficient and Lightweight Chatbot with AWQ](https://github.com/mit-han-lab/llm-awq/tree/main/tinychat) 
- [PockEngine: Sparse and Efficient Fine-tuning in a Pocket](https://hanlab.mit.edu/projects/pockengine)
- [Tiny Machine Learning: Progress and Futures](https://hanlab.mit.edu/projects/tiny-machine-learning-progress-and-futures)
- [Offsite-Tuning: Transfer Learning without Full Model](https://github.com/mit-han-lab/offsite-tuning)
- [[CVPR'23] FlatFormer: Flattened Window Attention for Efficient Point Cloud Transformer](https://github.com/mit-han-lab/flatformer)
- [On-Device Training Under 256KB Memory [NeurIPS'22]](https://github.com/mit-han-lab/tiny-training)
- [AMC: AutoML for Model Compression and Acceleration on Mobile Devices](https://arxiv.org/abs/1802.03494)
- [parallel-computing-tutorial](https://github.com/mit-han-lab/parallel-computing-tutorial)
- [Lab 5 project of MIT-6.5940, deploying LLaMA2-7B-chat on one's laptop with TinyChatEngine.](https://github.com/yifanlu0227/LLaMA2-7B-on-laptop)
- [Optimum is an extension of Transformers](https://huggingface.co/docs/optimum/index)
- [LLM-QAT: Data-Free Quantization Aware Training for Large Language Models](https://arxiv.org/pdf/2305.17888.pdf)
- [[ACL'20] HAT: Hardware-Aware Transformers for Efficient Natural Language Processing](https://github.com/mit-han-lab/hardware-aware-transformers)
- [TorchSparse: Efficient Training and Inference Framework for Sparse Convolution on GPUs.](https://github.com/mit-han-lab/torchsparse)
- [NVIDIA¬Æ TensorRT‚Ñ¢](https://github.com/NVIDIA/TensorRT)
- [[CVPR 2019, Oral] HAQ: Hardware-Aware Automated Quantization with Mixed Precision](https://github.com/mit-han-lab/haq)
- [tinyML Asia - Jungwook Choi: Quantization Techniques for Efficient Large Language Model Inference](https://youtu.be/YalbvQxOr_0?si=sIrMnMAvEP3JZDiY)
- [[NeurIPS 2023] Token-Scaled Logit Distillation for Ternary Weight Generative Language Models](https://github.com/aiha-lab/TSLD)
- [https://github.com/aiha-lab](Model Quantization lab)
- [[EACL 2023 main] This Repository provides a Pytorch implementation of Teacher Intervention: Improving Convergence of Quantization Aware Training for Ultra-Low Precision Transformers](https://github.com/MarsJacobs/ti-kd-qat)
- [NEW - YOLOv8 üöÄ in PyTorch > ONNX > OpenVINO > CoreML > TFLite](https://github.com/ultralytics/ultralytics)
- [How to make LLMs go fast inference](https://vgel.me/posts/faster-inference/)
- [QuIP#: Even Better LLM Quantization with Hadamard Incoherence and Lattice Codebooks](https://arxiv.org/pdf/2402.04396.pdf)
- [Extreme Compression of Large Language Models via Additive Quantization](https://arxiv.org/pdf/2401.06118.pdf)
- [Sparse Fine-tuning for Inference Acceleration of Large Language Models](https://arxiv.org/pdf/2310.06927.pdf)
- [Exponentially Faster Language Modeling](https://arxiv.org/pdf/2311.10770.pdf)
- [TensorRT C++ API Tutorial](https://github.com/cyrusbehr/tensorrt-cpp-api)
- [ncnn is a high-performance neural network inference framework optimized for the mobile platform](https://github.com/Tencent/ncnn/tree/master)
- [Holistic Trace Analysis (HTA), is a performance analysis tool to identify performance bottlenecks in distributed training workloads.](https://github.com/facebookresearch/HolisticTraceAnalysis/tree/main)
- [Tensor rt profiler](https://docs.nvidia.com/deeplearning/tensorrt/api/python_api/infer/Core/Profiler.html)
- [torchao: PyTorch Architecture Optimization (AO). A repository to host AO techniques and performant kernels that work with PyTorch.](https://github.com/pytorch-labs/ao)
- [A library for accelerating Transformer models on NVIDIA GPUs, including using 8-bit floating point (FP8) precision on Hopper and Ada GPUs, to provide better performance with lower memory utilization in both training and inference.](https://github.com/NVIDIA/TransformerEngine)
- [LLaMA Now Goes Faster on CPUs](https://justine.lol/matmul/)
- [The Triton Inference Server provides an optimized cloud and edge inferencing solution.](https://github.com/triton-inference-server/server)
- [Aboutü§ñ The free, Open Source OpenAI alternative. Self-hosted, community-driven and local-first. Drop-in replacement for OpenAI running on consumer-grade hardware. No GPU required. Runs gguf, transformers, diffusers and many more models architectures. It allows to generate Text, Audio, Video, Images. Also with voice cloning capabilities.](https://github.com/mudler/LocalAI)
- [TensorRT 10. Working with Loops](https://docs.nvidia.com/deeplearning/tensorrt/developer-guide/index.html#work-with-loops)
- [TensorRT 11.5.1. Simple If-Conditional](https://docs.nvidia.com/deeplearning/tensorrt/developer-guide/index.html#work-with-loops)
- [neural-compressor Intel](https://github.com/intel/neural-compressor)
- [AIMET is a library that provides advanced quantization and compression techniques for trained neural network models.](https://github.com/quic/aimet/tree/develop)
- [Neural Network Compression Framework for enhanced OpenVINO‚Ñ¢ inference](https://github.com/openvinotoolkit/nncf)
- [OpenVINO‚Ñ¢ is an open-source toolkit for optimizing and deploying AI inference](https://github.com/openvinotoolkit/openvino)
- [GitHub - xuyuzhuang11/OneBit: The homepage of OneBit model quantization framework.](https://github.com/xuyuzhuang11/OneBit)
- [PygmalionAI's large-scale inference engine](https://github.com/PygmalionAI/aphrodite-engine)
- [[ICML 2023] SmoothQuant: Accurate and Efficient Post-Training Quantization for Large Language Models](https://github.com/mit-han-lab/smoothquant)
- [A simple high performance CUDA GEMM implementation.](https://github.com/Cjkkkk/CUDA_gemm)
- [Meta Lingua: a lean, efficient, and easy-to-hack codebase to research LLMs.](https://github.com/facebookresearch/lingua?tab=readme-ov-file)
- [NanoGPT (124M) in 3 minutes](https://github.com/KellerJordan/modded-nanogpt)
- https://github.com/pytorch-labs/applied-ai
- [torch compile whisper](https://github.com/zhxchen17/torchnative/tree/main/whisper_aoti)
- https://github.com/pytorch-labs/attention-gym
- https://github.com/pytorch-labs/LeanRL
- [torchchat Run PyTorch LLMs locally on servers, desktop and mobile](https://github.com/pytorch/torchchat)
- [üöÄ Collection of components for development, training, tuning, and inference of foundation models leveraging PyTorch native components.](https://github.com/foundation-model-stack/foundation-model-stack)
- üöÄ [Efficiently (pre)training foundation models with native PyTorch features, including FSDP for training and SDPA implementation of Flash attention v2.](https://github.com/foundation-model-stack/fms-fsdp)
- [Modeling, training, eval, and inference code for OLMo](https://github.com/allenai/OLMo)
- [torchtune PyTorch native post-training library](https://github.com/pytorch/torchtune)
- [torchtitan A PyTorch native platform for training generative AI models](https://github.com/pytorch/torchtitan)
- [Native PyTorch library for quantization and sparsity](https://github.com/pytorch/ao)
- [diffusion-fast Faster generation with text-to-image diffusion models.](https://github.com/huggingface/diffusion-fast)

### courses
- [TinyML and Efficient Deep Learning Computing](https://hanlab.mit.edu/courses/2023-fall-65940)
- [tinyML youtube channel](https://www.youtube.com/@tinyML)
- [Stanford CS149, Fall 2021 PARALLEL COMPUTING](https://gfxcourses.stanford.edu/cs149/fall23)
- [pytorch developer syncs](https://www.youtube.com/@edwardzyang/videos)
- [Efficient Deep Learning Systems(Yandex School of Data Analysis)](https://github.com/mryab/efficient-dl-systems/tree/main)
- [Quantization in Depth (huggingface, deeplearning ai)](https://www.deeplearning.ai/short-courses/quantization-in-depth/)
- [Intro to GPUs (diffusion inference optimization)](https://www.vrushankdes.ai/diffusion-policy-inference-optimization/part-i---intro-to-gpus)
- [Custom Pytorch CUDA Kernel (using StyleGAN2 as an example)](https://ppeetteerrsx.com/post/cuda/stylegan_cuda_kernels/)
- [awesome-cuda-and-hpc](https://github.com/codingonion/awesome-cuda-and-hpc)
- [Diffusion Policy Accelerated is a library that showcases the use of custom CUDA extensions and CUDA graphs to accelerate the inference of DiffusionPolicy-C.](https://github.com/vdesai2014/diffusion-policy-accelerated.git)
- [A set of examples around pytorch in Vision, Text, Reinforcement Learning, etc.](https://github.com/pytorch/examples/tree/main/cpp)
- https://docs.pytorch.org/functorch/nightly/generated/functorch.vmap.html

### UPenn CIS 5650 - GPU Programming and Architecture - Fall 2023
- https://www.bilibili.com/video/BV1mQ4y177tj/
- https://cis565-fall-2023.github.io/syllabus/


### Lists
- [Awesome LLM compression research papers and tools.](https://github.com/HuangOwen/Awesome-LLM-Compression)

### Docs

- [Nvidia Compute Capabilities](https://docs.nvidia.com/cuda/cuda-c-programming-guide/index.html#compute-capabilities)
- [NVIDIA Deep Learning TensorRT Documentation Sample Support Guide](https://docs.nvidia.com/deeplearning/tensorrt/sample-support-guide/index.html#samples)

### T5 modeling
- https://github.com/huggingface/transformers/blob/main/src/transformers/models/t5/modeling_t5.py

### Cuda programming
- Programming Massively Parallel Processors: A Hands-on Approach 4nd Edition
- [CUDA C++ Programming Guide](https://docs.nvidia.com/cuda/cuda-c-programming-guide/index.html)
- [cuda-samples](https://github.com/NVIDIA/cuda-samples)
- [cuda-training-series](https://youtube.com/playlist?list=PL6RdenZrxrw-zNX7uuGppWETdxt_JxdMj&si=uPw8lt5skigLditP) - [github](https://github.com/olcf/cuda-training-series)
- [CUDA Library Samples](https://github.com/NVIDIA/CUDALibrarySamples)
a
### CUDA, main themes
- [warp shuffle, reduction, Using CUDA Warp-Level Primitives](https://developer.nvidia.com/blog/using-cuda-warp-level-primitives/)
- [debug kernels, Using Nsight Compute to Inspect your Kernels](https://developer.nvidia.com/blog/using-nsight-compute-to-inspect-your-kernels/)
- [shared memory, matrix transpose, An Efficient Matrix Transpose in CUDA C/C++](https://developer.nvidia.com/blog/efficient-matrix-transpose-cuda-cc/)
- [Unified Memory for CUDA Beginners](https://developer.nvidia.com/blog/unified-memory-cuda-beginners/)
- [streams, GPU Pro Tip: CUDA 7 Streams Simplify Concurrency](https://developer.nvidia.com/blog/gpu-pro-tip-cuda-7-streams-simplify-concurrency/)
- [pinned memory, How to Optimize Data Transfers in CUDA C/C++](https://developer.nvidia.com/blog/how-optimize-data-transfers-cuda-cc/)
- [nsys profiler documentation](https://docs.nvidia.com/nsight-systems/UserGuide/index.html)
- [Become Faster in Writing Performant CUDA Kernels using the Source Page in Nsight Compute](https://www.nvidia.com/en-us/on-demand/session/gtcspring23-s51882/)
- [Cooperative Groups: Flexible CUDA Thread Programming](https://developer.nvidia.com/blog/cooperative-groups/) - –Ω—É–∂–Ω–æ –¥–ª—è —Ä–∞–∑–±–∏–µ–Ω–∏—è warps, –Ω–∞ –±–æ–ª–µ–µ –º–µ–ª–∫–∏–µ –µ–¥–∏–Ω–∏—Ü—ã.
- [Accelerating PyTorch with CUDA Graphs](https://pytorch.org/blog/accelerating-pytorch-with-cuda-graphs/)
- [CUDA Shared Memory Bank](https://leimao.github.io/blog/CUDA-Shared-Memory-Bank/) 
- [hardware-effects-gpu/bank-conflicts](https://github.com/Kobzol/hardware-effects-gpu/blob/master/bank-conflicts/README.md)
- [loop unrolling](https://www.vrushankdes.ai/diffusion-policy-inference-optimization/part-ii---cuda-kernel-optimization-tips)
- [CUDA Zero Copy Mapped Memory](https://leimao.github.io/blog/CUDA-Zero-Copy-Mapped-Memory/)
- [How to Overlap Data Transfers in CUDA C/C++](https://developer.nvidia.com/blog/how-overlap-data-transfers-cuda-cc/)
- [Atomic Operations in CUDA](https://huangzhiyuan.github.io/2020/05/18/atomic-operations/index.html)
- [Mutual Exclusion and Atomic Functions](https://turing.une.edu.au/~cosc330/lectures/display_notes.php?lecture=20)
- [Understanding and Using Atomic Memory Operations](https://on-demand.gputechconf.com/gtc/2013/presentations/S3101-Atomic-Memory-Operations.pdf)
- [Optimizing Parallel Reduction in CUDA](https://developer.download.nvidia.com/assets/cuda/files/reduction.pdf)
- [Accelerating Transformers with NVIDIA cuDNN 9](https://developer.nvidia.com/blog/accelerating-transformers-with-nvidia-cudnn-9/)
- [memory padding | CUDA Memory Management & Use cases](https://medium.com/distributed-knowledge/cuda-memory-management-use-cases-f9d340f7c704)
- [How to Optimize a CUDA Matmul Kernel for cuBLAS-like Performance: a Worklog](https://siboehm.com/articles/22/CUDA-MMM)
- [CUDA: Shared memory](https://medium.com/@fatlip/cuda-shared-memory-23cd1a0d4e39)
- [Getting Started with CUDA Graphs](https://developer.nvidia.com/blog/cuda-graphs)

- [EVERYTHING YOU NEED TO KNOW ABOUT UNIFIED MEMORY](https://on-demand.gputechconf.com/gtc/2018/presentation/s8430-everything-you-need-to-know-about-unified-memory.pdf)
- [Maximizing Unified Memory Performance in CUDA](https://developer.nvidia.com/blog/maximizing-unified-memory-performance-cuda/)

### CUDA Tutorial Projects
- https://github.com/pwlnk/cuda-neural-network

### Blog Posts

- [Making Deep Learning Go Brrrr From First Principles](https://horace.io/brrr_intro.html)

### –ö–∞–∫ –¥–µ–±–∞–∂–∏—Ç –ø–∏—Ç–æ–Ω –≤–º–µ—Å—Ç–µ —Å++
- [–ö–∞–∫ —Å–¥–µ–ª–∞—Ç—å pybind –∏—Å–ø–æ–ª–Ω—è–µ–º—ã–º –∏ –æ–±–µ—Å–ø–µ—á–∏—Ç—å –µ–≥–æ –æ—Ç–ª–∞–¥–∫—É](https://stackoverflow.com/questions/78552821/simple-example-of-c-calling-python-using-pybind11-and-cmake)
- https://marketplace.visualstudio.com/items?itemName=benjamin-simmonds.pythoncpp-debug
- https://github.com/microsoft/MIEngine/blob/9470e98d946772ed1ace9ab68e9fbd4d88090b42/src/MICore/UnixUtilities.cs#L140C44-L140C59
- https://github.com/microsoft/MIEngine/issues/1089
- https://web-standards.ru/articles/docker-unboxing-3/
- https://stackoverflow.com/questions/46053672/set-secomp-to-unconfined-in-docker-compose
- https://stackoverflow.com/questions/19215177/how-to-solve-ptrace-operation-not-permitted-when-trying-to-attach-gdb-to-a-pro
- https://alexreinking.com/blog/how-to-use-cmake-without-the-agonizing-pain-part-2.html
- https://www.youtube.com/watch?v=KhuMRDY4BeU


### –°–æ–∑–¥–∞–Ω–∏–µ –±–∏–±–ª–∏–æ—Ç–µ–∫
- [Packaging Python Projects](https://packaging.python.org/en/latest/tutorials/packaging-projects/)
- https://pytorch.org/tutorials/advanced/cpp_custom_ops.html#cpp-custom-ops-tutorial
- https://github.com/pybind/pybind11



### Pytorch Compile

- https://docs.google.com/document/d/1y5CRfMLdwEoF1nTk9q8qEu1mgMUuUtvhklPKJ2emLU8/edit
- https://www.youtube.com/live/rew5CSUaIXg?si=84kYR6vV8MqA4xCd
- [Identifying the cause of a graph break](https://docs.pytorch.org/docs/stable/torch.compiler_faq.html#why-am-i-not-seeing-speedups)
- https://ui.perfetto.dev/
- chrome://tracing/
- [Torch Compile and External Kernels](https://docs.nvidia.com/deeplearning/physicsnemo/physicsnemo-core/tutorials/performance_docs/torch_compile_support.html)
- https://github.com/MDK8888/GPTFast
- [gpt-fast](https://github.com/pytorch-labs/gpt-fast)
- [The dynamic shapes manual](https://docs.google.com/document/d/1GgvOe7C8_NVOMLOCwDaYV1mXXyHMXY7ExoewHqooxrs/edit?tab=t.0#heading=h.fh8zzonyw8ng)
- [torch.compile Troubleshooting](https://docs.pytorch.org/docs/stable/torch.compiler_troubleshooting.html#torch-compile-troubleshooting)
- https://blog.ezyang.com/2024/11/ways-to-use-torch-compile/
- [–ø—Ä–µ–≤—Ä–∞—â–µ–Ω–∏–µ –ø—Ä–æ–≥—Ä–∞–º –æ—Ç torch compile –≤ –±–∏–Ω–∞—Ä–Ω–∏–∫–∏ –±–µ–∑ libtorch](https://github.com/lianakoleva/no-libtorch-compile)
- [torch.compile, the missing manual](https://docs.google.com/document/d/1y5CRfMLdwEoF1nTk9q8qEu1mgMUuUtvhklPKJ2emLU8/edit?tab=t.0#heading=h.ivdr7fmrbeab)
- https://docs.pytorch.org/docs/stable/notes/cuda.html#whole-network-capture
- [Benchmark Individual Triton Kernel](https://docs.pytorch.org/docs/stable/torch.compiler_inductor_profiling.html#benchmark-individual-triton-kernel)
- [(Beta) Implementing High-Performance Transformers with Scaled Dot Product Attention (SDPA)](https://docs.pytorch.org/tutorials/intermediate/scaled_dot_product_attention_tutorial.html)
- [Speeding Up Graph Learning Models with PyG and torch.compile](https://kumo.ai/research/speeding-up-graph-learning-models-with-pyg-and-torch-compile/)
- [tensorrt Torch Compile Advanced Usage](https://docs.pytorch.org/TensorRT/tutorials/_rendered_examples/dynamo/torch_compile_advanced_usage.html)
- [A Walk Through Example of torch.compile](https://depyf.readthedocs.io/en/latest/walk_through.html)