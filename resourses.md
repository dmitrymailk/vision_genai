- [whisper.cpp](https://github.com/ggerganov/whisper.cpp)
- [EAGLE](https://github.com/SafeAILab/EAGLE)
- [koboldcpp](https://github.com/LostRuins/koboldcpp)
- [speculative-decoding](https://github.com/lucidrains/speculative-decoding)
- [ts_server](https://bellard.org/ts_server/)
- [fastT5](https://github.com/Ki6an/fastT5) - работает с некоторыми исправлениями старого апи. Не работает с GPU.
- [Neural Network Compression: Techniques for Reducing Size and ImprovingLatency](https://youtu.be/-QCbDjpIM2I?si=1Zv6-RxcuilKWXIl)
- [https://vgel.me/posts/faster-inference/](https://vgel.me/posts/faster-inference/)
- [UMIA-Group/FourierTransformer: The official Pytorch implementation of the paper "Fourier Transformer: Fast Long Range Modeling by Removing Sequence Redundancy with FFT Operator" (ACL 2023 Findings)](https://github.com/LUMIA-Group/FourierTransformer)
- [Neural Network Distiller by Intel AI Lab: a Python package for neural network compression research.](https://github.com/IntelLabs/distiller/)
- [Exploring Extreme Parameter Compression for Pre-trained Language Models](https://arxiv.org/pdf/2205.10036.pdf)
- [EdgeBERT: Sentence-Level Energy Optimizations for Latency-Aware Multi-Task NLP Inference](https://arxiv.org/pdf/2011.14203.pdf)
- [https://github.com/ELS-RD/kernl/tree/main](https://github.com/ELS-RD/kernl/tree/main) - зависает на большой модели, не заработало.
- [Fast & Simple repository for pre-training and fine-tuning T5-style models](https://github.com/PiotrNawrot/nanoT5)

- [PyTorch compile to speed up inference on Llama 2](https://pytorch.org/blog/pytorch-compile-to-speed-up-inference/)
- [Fuse Modules Recipe](https://pytorch.org/tutorials/recipes/fuse.html)
- [Dynamic Quantization pytorch](https://pytorch.org/tutorials/recipes/recipes/dynamic_quantization.html) - хорошо работает
- [Introduction to Quantization on PyTorch](https://pytorch.org/blog/introduction-to-quantization-on-pytorch/)
- [Static Quantization with Eager Mode in PyTorch](https://pytorch.org/tutorials/advanced/static_quantization_tutorial.html)
- [QUANTIZATION pytorch](https://pytorch.org/docs/stable/quantization.html)
- [Quantized Transfer Learning for Computer Vision Tutorial](https://pytorch.org/tutorials/intermediate/quantized_transfer_learning_tutorial.html)
- [Pytorch Mobile Performance Recipes](https://pytorch.org/tutorials/recipes/mobile_perf.html)
- [A BetterTransformer for Fast Transformer Inference](https://pytorch.org/blog/a-better-transformer-for-fast-transformer-encoder-inference/)
- [fastseq An efficient implementation of the popular sequence models for text generation, translation tasks.](https://github.com/microsoft/fastseq)
- [Accelerating HuggingFace T5 Inference with TensorRT](https://github.com/NVIDIA/TensorRT/blob/release/8.2/demo/HuggingFace/notebooks/t5.ipynb)
- [TensorRT-LLM provides users with an easy-to-use Python API to define Large Language Models (LLMs)](https://github.com/NVIDIA/TensorRT-LLM/tree/main)
- [https://github.com/NVIDIA/FasterTransformer](https://github.com/NVIDIA/FasterTransformer)
- [AOTInductor: Ahead-Of-Time Compilation for Torch.Export-ed Models](https://pytorch.org/docs/main/torch.compiler_aot_inductor.html)
- [PYTORCH MOBILE End-to-end workflow from Training to Deployment for iOS and Android mobile devices](https://pytorch.org/mobile/home/)
- [An easy to use PyTorch to TensorRT converter](https://github.com/NVIDIA-AI-IOT/torch2trt)
- [Lite Transformer with Long-Short Range Attention](https://github.com/mit-han-lab/lite-transformer)
- [EdgeTran: Co-designing Transformers for Efficient Inference on Mobile Edge Platforms](https://arxiv.org/pdf/2303.13745.pdf)

- [Efficient AI Inference & Serving](https://github.com/hpcaitech/SwiftInfer)
- [[ICLR 2024] Efficient Streaming Language Models with Attention Sinks](https://github.com/mit-han-lab/streaming-llm)
- [TinyChat: Efficient and Lightweight Chatbot with AWQ](https://github.com/mit-han-lab/llm-awq/tree/main/tinychat) 
- [PockEngine: Sparse and Efficient Fine-tuning in a Pocket](https://hanlab.mit.edu/projects/pockengine)
- [Tiny Machine Learning: Progress and Futures](https://hanlab.mit.edu/projects/tiny-machine-learning-progress-and-futures)
- [Offsite-Tuning: Transfer Learning without Full Model](https://github.com/mit-han-lab/offsite-tuning)
- [[CVPR'23] FlatFormer: Flattened Window Attention for Efficient Point Cloud Transformer](https://github.com/mit-han-lab/flatformer)
- [On-Device Training Under 256KB Memory [NeurIPS'22]](https://github.com/mit-han-lab/tiny-training)
- [AMC: AutoML for Model Compression and Acceleration on Mobile Devices](https://arxiv.org/abs/1802.03494)
- [parallel-computing-tutorial](https://github.com/mit-han-lab/parallel-computing-tutorial)
- [Lab 5 project of MIT-6.5940, deploying LLaMA2-7B-chat on one's laptop with TinyChatEngine.](https://github.com/yifanlu0227/LLaMA2-7B-on-laptop)
- [Optimum is an extension of Transformers](https://huggingface.co/docs/optimum/index)
- [LLM-QAT: Data-Free Quantization Aware Training for Large Language Models](https://arxiv.org/pdf/2305.17888.pdf)
- [[ACL'20] HAT: Hardware-Aware Transformers for Efficient Natural Language Processing](https://github.com/mit-han-lab/hardware-aware-transformers)
- [TorchSparse: Efficient Training and Inference Framework for Sparse Convolution on GPUs.](https://github.com/mit-han-lab/torchsparse)
- [NVIDIA® TensorRT™](https://github.com/NVIDIA/TensorRT)
- [[CVPR 2019, Oral] HAQ: Hardware-Aware Automated Quantization with Mixed Precision](https://github.com/mit-han-lab/haq)
- [tinyML Asia - Jungwook Choi: Quantization Techniques for Efficient Large Language Model Inference](https://youtu.be/YalbvQxOr_0?si=sIrMnMAvEP3JZDiY)
- [[NeurIPS 2023] Token-Scaled Logit Distillation for Ternary Weight Generative Language Models](https://github.com/aiha-lab/TSLD)
- [https://github.com/aiha-lab](Model Quantization lab)
- [[EACL 2023 main] This Repository provides a Pytorch implementation of Teacher Intervention: Improving Convergence of Quantization Aware Training for Ultra-Low Precision Transformers](https://github.com/MarsJacobs/ti-kd-qat)
- [NEW - YOLOv8 🚀 in PyTorch > ONNX > OpenVINO > CoreML > TFLite](https://github.com/ultralytics/ultralytics)
- [How to make LLMs go fast inference](https://vgel.me/posts/faster-inference/)
- [QuIP#: Even Better LLM Quantization with Hadamard Incoherence and Lattice Codebooks](https://arxiv.org/pdf/2402.04396.pdf)
- [Extreme Compression of Large Language Models via Additive Quantization](https://arxiv.org/pdf/2401.06118.pdf)
- [Sparse Fine-tuning for Inference Acceleration of Large Language Models](https://arxiv.org/pdf/2310.06927.pdf)
- [Exponentially Faster Language Modeling](https://arxiv.org/pdf/2311.10770.pdf)
- [TensorRT C++ API Tutorial](https://github.com/cyrusbehr/tensorrt-cpp-api)
- [ncnn is a high-performance neural network inference framework optimized for the mobile platform](https://github.com/Tencent/ncnn/tree/master)
- [Holistic Trace Analysis (HTA), is a performance analysis tool to identify performance bottlenecks in distributed training workloads.](https://github.com/facebookresearch/HolisticTraceAnalysis/tree/main)
- [Tensor rt profiler](https://docs.nvidia.com/deeplearning/tensorrt/api/python_api/infer/Core/Profiler.html)
- [torchao: PyTorch Architecture Optimization (AO). A repository to host AO techniques and performant kernels that work with PyTorch.](https://github.com/pytorch-labs/ao)
- [A library for accelerating Transformer models on NVIDIA GPUs, including using 8-bit floating point (FP8) precision on Hopper and Ada GPUs, to provide better performance with lower memory utilization in both training and inference.](https://github.com/NVIDIA/TransformerEngine)
- [LLaMA Now Goes Faster on CPUs](https://justine.lol/matmul/)
- [The Triton Inference Server provides an optimized cloud and edge inferencing solution.](https://github.com/triton-inference-server/server)
- [About🤖 The free, Open Source OpenAI alternative. Self-hosted, community-driven and local-first. Drop-in replacement for OpenAI running on consumer-grade hardware. No GPU required. Runs gguf, transformers, diffusers and many more models architectures. It allows to generate Text, Audio, Video, Images. Also with voice cloning capabilities.](https://github.com/mudler/LocalAI)
- [TensorRT 10. Working with Loops](https://docs.nvidia.com/deeplearning/tensorrt/developer-guide/index.html#work-with-loops)
- [TensorRT 11.5.1. Simple If-Conditional](https://docs.nvidia.com/deeplearning/tensorrt/developer-guide/index.html#work-with-loops)
- [neural-compressor Intel](https://github.com/intel/neural-compressor)
- [AIMET is a library that provides advanced quantization and compression techniques for trained neural network models.](https://github.com/quic/aimet/tree/develop)
- [Neural Network Compression Framework for enhanced OpenVINO™ inference](https://github.com/openvinotoolkit/nncf)
- [OpenVINO™ is an open-source toolkit for optimizing and deploying AI inference](https://github.com/openvinotoolkit/openvino)
- [GitHub - xuyuzhuang11/OneBit: The homepage of OneBit model quantization framework.](https://github.com/xuyuzhuang11/OneBit)
- [PygmalionAI's large-scale inference engine](https://github.com/PygmalionAI/aphrodite-engine)
- [[ICML 2023] SmoothQuant: Accurate and Efficient Post-Training Quantization for Large Language Models](https://github.com/mit-han-lab/smoothquant)
- [A simple high performance CUDA GEMM implementation.](https://github.com/Cjkkkk/CUDA_gemm)
- [Meta Lingua: a lean, efficient, and easy-to-hack codebase to research LLMs.](https://github.com/facebookresearch/lingua?tab=readme-ov-file)
- [NanoGPT (124M) in 3 minutes](https://github.com/KellerJordan/modded-nanogpt)
- https://github.com/pytorch-labs/applied-ai
- [torch compile whisper](https://github.com/zhxchen17/torchnative/tree/main/whisper_aoti)
- https://github.com/pytorch-labs/LeanRL
- [torchchat Run PyTorch LLMs locally on servers, desktop and mobile](https://github.com/pytorch/torchchat)
- [🚀 Collection of components for development, training, tuning, and inference of foundation models leveraging PyTorch native components.](https://github.com/foundation-model-stack/foundation-model-stack)
- 🚀 [Efficiently (pre)training foundation models with native PyTorch features, including FSDP for training and SDPA implementation of Flash attention v2.](https://github.com/foundation-model-stack/fms-fsdp)
- [Modeling, training, eval, and inference code for OLMo](https://github.com/allenai/OLMo)
- [torchtune PyTorch native post-training library](https://github.com/pytorch/torchtune)
- [torchtitan A PyTorch native platform for training generative AI models](https://github.com/pytorch/torchtitan)
- [Native PyTorch library for quantization and sparsity](https://github.com/pytorch/ao)
- [diffusion-fast Faster generation with text-to-image diffusion models.](https://github.com/huggingface/diffusion-fast)
- [AITemplate is a Python framework which renders neural network into high performance CUDA/HIP C++ code. Specialized for FP16 TensorCore (NVIDIA GPU) and MatrixCore (AMD GPU) inference.](https://github.com/facebookincubator/AITemplate/tree/main)
- [The largest collection of PyTorch image encoders / backbones. Including train, eval, inference, export scripts, and pretrained weights -- ResNet, ResNeXT, EfficientNet, NFNet, Vision Transformer (ViT), MobileNetV4, MobileNet-V3 & V2, RegNet, DPN, CSPNet, Swin Transformer, MaxViT, CoAtNet, ConvNeXt, and more](https://github.com/huggingface/pytorch-image-models?tab=readme-ov-file#models)
- [A coding-free framework built on PyTorch for reproducible deep learning studies. PyTorch Ecosystem. 🏆25 knowledge distillation methods presented at CVPR, ICLR, ECCV, NeurIPS, ICCV, etc are implemented so far. 🎁 Trained models, training logs and configurations are available for ensuring the reproducibiliy and benchmark.](https://github.com/yoshitomo-matsubara/torchdistill)
- https://docs.pytorch.org/functorch/nightly/generated/functorch.vmap.html
- [stable-fast is an ultra lightweight inference optimization framework for HuggingFace Diffusers on NVIDIA GPUs](https://github.com/chengzeyi/stable-fast)
- [\[MLSys 2024 Best Paper Award\] AWQ: Activation-aware Weight Quantization for LLM Compression and Acceleration](https://github.com/mit-han-lab/llm-awq)
- [🏎️ Enhance Your Models in 5 Minutes with the Hugging Face Kernel Hub](https://huggingface.co/blog/hello-hf-kernels)
- https://github.com/huggingface/smollm
- https://github.com/huggingface/open-r1
- https://github.com/hkust-nlp/simpleRL-reason
- https://github.com/Jiayi-Pan/TinyZero
- https://github.com/pytorch/torchft
- https://github.com/GeeeekExplorer/nano-vllm
- https://github.com/BlinkDL/modded-nanogpt-rwkv
- [Efficient LLM Pretraining: Packed Sequences and Masked Attention](https://huggingface.co/blog/sirluk/llm-sequence-packing)
- [A multi-GPU training framework that combines Unsloth with multi-GPU support and sequence packing optimizations.](https://github.com/anhvth/opensloth)

- [unslothai Cut Your Losses in Large-Vocabulary Language Models](https://github.com/unslothai/cut-cross-entropy)
- [original Cut Your Losses in Large-Vocabulary Language Models](https://github.com/apple/ml-cross-entropy) 
- https://github.com/JonasGeiping/linear_cross_entropy_loss
- https://github.com/allenai/OLMo-core
- https://github.com/facebookresearch/xformers/tree/main/examples/llama_inference
- https://github.com/showlab/Show-o/tree/main/show-o2
- [FlashInfer: Kernel Library for LLM Serving](https://github.com/flashinfer-ai/flashinfer)
- https://github.com/pytorch-labs/attention-gym
- https://github.com/unslothai/unsloth
- https://github.com/Dao-AILab/flash-attention/tree/main/training
- https://github.com/facebookresearch/xformers/tree/main/examples
- https://github.com/facebookresearch/fairseq2
- https://github.com/apple/ml-dataset-decomposition

- https://github.com/zyushun/Adam-mini
- https://github.com/llm-efficiency-challenge/neurips_llm_efficiency_challenge
- https://huggingface.co/docs/bitsandbytes/en/explanations/optimizers#8-bit-optimizers
- [BitNet b1.58 training](https://github.com/pytorch/ao/pull/930)
- [Add INT8 mixed-precision training](https://github.com/pytorch/ao/pull/748)
- https://github.com/gau-nernst/quantized-training
- https://github.com/thu-ml/Jetfire-INT8Training
- https://github.com/JonasGeiping/cramming
- https://github.com/JonasGeiping/linear_cross_entropy_loss
- https://github.com/BorealisAI/flora-opt
- https://github.com/IST-DASLab/QuEST
- https://github.com/BlackSamorez/tensor_parallel
- https://github.com/Vahe1994/AQLM
- https://github.com/mosaicml/composer
- https://pytorch.org/blog/maximizing-training-throughput/
- [FlexFlow Serve: Low-Latency, High-Performance LLM Serving](https://github.com/flexflow/flexflow-serve)
- [Welcome to the Llama Cookbook! This is your go to guide for Building with Llama: Getting started with Inference, Fine-Tuning, RAG. We also show you how to solve end to end problems using Llama model family and using them on various provider services](https://github.com/meta-llama/llama-cookbook)
- [Look Ma, No Bubbles! Designing a Low-Latency Megakernel for Llama-1B](https://github.com/HazyResearch/Megakernels)
- [GaLore: Memory-Efficient LLM Training by Gradient Low-Rank Projection](https://github.com/jiaweizzhao/GaLore)
- [Code for Adam-mini: Use Fewer Learning Rates To Gain More](https://github.com/zyushun/Adam-mini)
- [Scaling RL on advanced reasoning models](https://github.com/ChenxinAn-fdu/POLARIS)
- [Reducing Model Checkpointing Times by Over 10x with PyTorch Distributed Asynchronous Checkpointing](https://pytorch.org/blog/reducing-checkpointing-times/)
- [Supercharging Training using float8 and FSDP2](https://pytorch.org/blog/training-using-float8-fsdp2/)
- [Efficient Pre-training of Llama 3-like model architectures using torchtitan on Amazon SageMaker](https://aws.amazon.com/blogs/machine-learning/efficient-pre-training-of-llama-3-like-model-architectures-using-torchtitan-on-amazon-sagemaker/)
- [Basic Example for Using PyTorch Fully Sharded Data Parallel mode with Transformer Engine, float8](https://github.com/NVIDIA/TransformerEngine/blob/main/examples/pytorch/fsdp/fsdp.py)
- [How We Trained Stable Diffusion for Less than $50k (Part 3)](https://www.databricks.com/blog/diffusion)
- [Benchmarking Large Language Models on NVIDIA H100 GPUs with CoreWeave (Part 1) float8 Transformer Engine](https://www.databricks.com/blog/coreweave-nvidia-h100-part-1)
- [accelerate FP8 training torchao](https://github.com/huggingface/accelerate/tree/main/benchmarks/fp8/torchao)
- [llm-foundry LLM training code for Databricks foundation models](https://github.com/mosaicml/llm-foundry)
- [MegaBlocks is a light-weight library for mixture-of-experts (MoE) training.](https://github.com/databricks/megablocks)
- [Multimodal example (LLaVA training pipeline) Megatron-LM](https://github.com/NVIDIA/Megatron-LM/tree/main/examples/multimodal)
- [Arctic Long Sequence Training (ALST) for HF Transformers integration](https://www.deepspeed.ai/tutorials/ulysses-alst-sequence-pallellism/)
- [A Scalable Inference Engine for Diffusion Transformers (DiTs) on Multiple Computing Devices](https://github.com/xdit-project/xDiT)
- [Thunder makes optimizing PyTorch models easy, augmenting them with custom kernels, fusions, quantization, distributed strategies, and more.](https://github.com/Lightning-AI/lightning-thunder/tree/main)

### courses 
- [TinyML and Efficient Deep Learning Computing](https://hanlab.mit.edu/courses/2023-fall-65940)
- [tinyML youtube channel](https://www.youtube.com/@tinyML)
- [Stanford CS149, Fall 2021 PARALLEL COMPUTING](https://gfxcourses.stanford.edu/cs149/fall23)
- [pytorch developer syncs](https://www.youtube.com/@edwardzyang/videos)
- [Efficient Deep Learning Systems(Yandex School of Data Analysis)](https://github.com/mryab/efficient-dl-systems/tree/main)
- [Quantization in Depth (huggingface, deeplearning ai)](https://www.deeplearning.ai/short-courses/quantization-in-depth/)
- [Intro to GPUs (diffusion inference optimization)](https://www.vrushankdes.ai/diffusion-policy-inference-optimization/part-i---intro-to-gpus)
- [Custom Pytorch CUDA Kernel (using StyleGAN2 as an example)](https://ppeetteerrsx.com/post/cuda/stylegan_cuda_kernels/)
- [awesome-cuda-and-hpc](https://github.com/codingonion/awesome-cuda-and-hpc)
- [Diffusion Policy Accelerated is a library that showcases the use of custom CUDA extensions and CUDA graphs to accelerate the inference of DiffusionPolicy-C.](https://github.com/vdesai2014/diffusion-policy-accelerated.git)
- [A set of examples around pytorch in Vision, Text, Reinforcement Learning, etc.](https://github.com/pytorch/examples/tree/main/cpp)
- [Andrej Karpathy Let's build GPT: from scratch, in code, spelled out.](https://youtu.be/kCc8FmEb1nY?si=Wtxzi-kWVvEZFlO-)
- https://github.com/lucidrains
- https://github.com/foundation-model-stack/fms-extras/blob/main/scripts/paged_speculative_inference.py
- [CS336: Language Modeling from Scratch](https://stanford-cs336.github.io/spring2025/)
- [Material for gpu-mode lectures](https://github.com/gpu-mode/lectures)
- [The Ultra-Scale Playbook: Training LLMs on GPU Clusters](https://huggingface.co/spaces/nanotron/ultrascale-playbook)
- https://huggingface.co/spaces/nanotron/predict_memory
- [Learn CUDA with PyTorch](https://github.com/gau-nernst/learn-cuda)
- https://www.youtube.com/@umarjamilai/videos
- https://www.youtube.com/@Tunadorable/videos
- https://github.com/yandexdataschool/Practical_RL
- https://github.com/yandexdataschool/sdc_course
- https://github.com/yandexdataschool/speech_course
- https://github.com/yandexdataschool/recsys_course
- https://erdem.pl/2023/11/step-by-step-visual-introduction-to-diffusion-models
- https://www.bilibili.com/video/BV1mQ4y177tj/
- https://cis565-fall-2023.github.io/syllabus/
- [ML Performance Reading Group Session 2: Flash Attention](https://youtu.be/Lys0TpsLIEc?si=wNoLKsKi8Kib_PXb)
- [nanoMoE: Mixture-of-Experts (MoE) LLMs from Scratch in PyTorch](https://cameronrwolfe.substack.com/p/nano-moe)
- [GPUs Go Brrr](https://hazyresearch.stanford.edu/blog/2024-05-12-tk)

### Triton 
- [Fused Softmax triton](https://triton-lang.org/main/getting-started/tutorials/02-fused-softmax.html)
- [Efficient Triton Kernels for LLM Training](https://github.com/linkedin/Liger-Kernel)
- [Triton GPU Kernels 101: Syllabus day (Lesson #1)](https://youtu.be/TUQAyCNxFe4?si=yZ_JywD1qTm-uVJO)
- https://triton-lang.org/main/getting-started/tutorials/index.html

### dataset Packing, no cross-contamination
- [Improving Hugging Face Training Efficiency Through Packing with Flash Attention](https://huggingface.co/blog/packing-with-FA2)
- [How to implement example packing with flash_attn v2](https://github.com/Dao-AILab/flash-attention/issues/654)
- [torchtune Sample packing](https://docs.pytorch.org/torchtune/stable/basics/packing.html)
- [Packing Inputs Without Cross-Contamination Attention](https://github.com/MeetKai/functionary/tree/main/functionary/train/packing)
- [Multipack (Sample Packing)](https://github.com/axolotl-ai-cloud/axolotl/blob/main/docs/multipack.qmd)
- [Sample packing for map datasets with correct RoPE encoding and no cross-contamination](https://github.com/pytorch/torchtune/pull/875)
- [INTRO‌DUC‍ING‌‍ PAC‌‍KED‍ BER‌‍T FO‌R‌ 2‌X TR‌‍A‌INING SP‌‍EED‍-U‌P‍ IN NATUR‌‍A‌L LANGU‍A‌G‍E P‌R‌O‌C‍ESSING](https://www.graphcore.ai/posts/introducing-packed-bert-for-2x-faster-training-in-natural-language-processing)
- [Packing without cross-contamination (huggingface) ](https://github.com/huggingface/transformers/issues/25452)
- [Enhancing Training Efficiency Using Packing with Flash Attention](https://arxiv.org/pdf/2407.09105v4) лучше всего делать это в батче, а не заранее
- https://huggingface.co/docs/transformers/en/main_classes/data_collator#transformers.DataCollatorWithFlattening
- [Optimizing Transformer Models for Variable-Length Input Sequences](https://medium.com/data-science/optimizing-transformer-models-for-variable-length-input-sequences-19fb88fddf71)
- [4D masks support in Transformers](https://huggingface.co/blog/poedator/4d-masks)

### Lists
- [Awesome LLM compression research papers and tools.](https://github.com/HuangOwen/Awesome-LLM-Compression)

### Docs

- [Nvidia Compute Capabilities](https://docs.nvidia.com/cuda/cuda-c-programming-guide/index.html#compute-capabilities)
- [NVIDIA Deep Learning TensorRT Documentation Sample Support Guide](https://docs.nvidia.com/deeplearning/tensorrt/sample-support-guide/index.html#samples)

### T5 modeling
- https://github.com/huggingface/transformers/blob/main/src/transformers/models/t5/modeling_t5.py

### Cuda programming
- Programming Massively Parallel Processors: A Hands-on Approach 4nd Edition
- [CUDA C++ Programming Guide](https://docs.nvidia.com/cuda/cuda-c-programming-guide/index.html)
- [cuda-samples](https://github.com/NVIDIA/cuda-samples)
- [cuda-training-series](https://youtube.com/playlist?list=PL6RdenZrxrw-zNX7uuGppWETdxt_JxdMj&si=uPw8lt5skigLditP) - [github](https://github.com/olcf/cuda-training-series)
- [CUDA Library Samples](https://github.com/NVIDIA/CUDALibrarySamples)
- [Unsloth.ai: Easily finetune & train LLMs](https://youtu.be/MQwryfkydc0?si=w3JAQJtDtkWaWgKS)

### CUDA, main themes
- [warp shuffle, reduction, Using CUDA Warp-Level Primitives](https://developer.nvidia.com/blog/using-cuda-warp-level-primitives/)
- [debug kernels, Using Nsight Compute to Inspect your Kernels](https://developer.nvidia.com/blog/using-nsight-compute-to-inspect-your-kernels/)
- [shared memory, matrix transpose, An Efficient Matrix Transpose in CUDA C/C++](https://developer.nvidia.com/blog/efficient-matrix-transpose-cuda-cc/)
- [Unified Memory for CUDA Beginners](https://developer.nvidia.com/blog/unified-memory-cuda-beginners/)
- [streams, GPU Pro Tip: CUDA 7 Streams Simplify Concurrency](https://developer.nvidia.com/blog/gpu-pro-tip-cuda-7-streams-simplify-concurrency/)
- [pinned memory, How to Optimize Data Transfers in CUDA C/C++](https://developer.nvidia.com/blog/how-optimize-data-transfers-cuda-cc/)
- [nsys profiler documentation](https://docs.nvidia.com/nsight-systems/UserGuide/index.html)
- [Become Faster in Writing Performant CUDA Kernels using the Source Page in Nsight Compute](https://www.nvidia.com/en-us/on-demand/session/gtcspring23-s51882/)
- [Cooperative Groups: Flexible CUDA Thread Programming](https://developer.nvidia.com/blog/cooperative-groups/) - нужно для разбиения warps, на более мелкие единицы.
- [Accelerating PyTorch with CUDA Graphs](https://pytorch.org/blog/accelerating-pytorch-with-cuda-graphs/)
- [CUDA Shared Memory Bank](https://leimao.github.io/blog/CUDA-Shared-Memory-Bank/) 
- [hardware-effects-gpu/bank-conflicts](https://github.com/Kobzol/hardware-effects-gpu/blob/master/bank-conflicts/README.md)
- [loop unrolling](https://www.vrushankdes.ai/diffusion-policy-inference-optimization/part-ii---cuda-kernel-optimization-tips)
- [CUDA Zero Copy Mapped Memory](https://leimao.github.io/blog/CUDA-Zero-Copy-Mapped-Memory/)
- [How to Overlap Data Transfers in CUDA C/C++](https://developer.nvidia.com/blog/how-overlap-data-transfers-cuda-cc/)
- [Atomic Operations in CUDA](https://huangzhiyuan.github.io/2020/05/18/atomic-operations/index.html)
- [Mutual Exclusion and Atomic Functions](https://turing.une.edu.au/~cosc330/lectures/display_notes.php?lecture=20)
- [Understanding and Using Atomic Memory Operations](https://on-demand.gputechconf.com/gtc/2013/presentations/S3101-Atomic-Memory-Operations.pdf)
- [Optimizing Parallel Reduction in CUDA](https://developer.download.nvidia.com/assets/cuda/files/reduction.pdf)
- [Accelerating Transformers with NVIDIA cuDNN 9](https://developer.nvidia.com/blog/accelerating-transformers-with-nvidia-cudnn-9/)
- [memory padding | CUDA Memory Management & Use cases](https://medium.com/distributed-knowledge/cuda-memory-management-use-cases-f9d340f7c704)
- [How to Optimize a CUDA Matmul Kernel for cuBLAS-like Performance: a Worklog](https://siboehm.com/articles/22/CUDA-MMM)
- [CUDA: Shared memory](https://medium.com/@fatlip/cuda-shared-memory-23cd1a0d4e39)
- [Getting Started with CUDA Graphs](https://developer.nvidia.com/blog/cuda-graphs)

- [EVERYTHING YOU NEED TO KNOW ABOUT UNIFIED MEMORY](https://on-demand.gputechconf.com/gtc/2018/presentation/s8430-everything-you-need-to-know-about-unified-memory.pdf)
- [Maximizing Unified Memory Performance in CUDA](https://developer.nvidia.com/blog/maximizing-unified-memory-performance-cuda/)

### CUDA Tutorial Projects
- https://github.com/pwlnk/cuda-neural-network

### Blog Posts

- [Making Deep Learning Go Brrrr From First Principles](https://horace.io/brrr_intro.html)

### Как дебажит питон вместе с++
- [Как сделать pybind исполняемым и обеспечить его отладку](https://stackoverflow.com/questions/78552821/simple-example-of-c-calling-python-using-pybind11-and-cmake)
- https://marketplace.visualstudio.com/items?itemName=benjamin-simmonds.pythoncpp-debug
- https://github.com/microsoft/MIEngine/blob/9470e98d946772ed1ace9ab68e9fbd4d88090b42/src/MICore/UnixUtilities.cs#L140C44-L140C59
- https://github.com/microsoft/MIEngine/issues/1089
- https://web-standards.ru/articles/docker-unboxing-3/
- https://stackoverflow.com/questions/46053672/set-secomp-to-unconfined-in-docker-compose
- https://stackoverflow.com/questions/19215177/how-to-solve-ptrace-operation-not-permitted-when-trying-to-attach-gdb-to-a-pro
- https://alexreinking.com/blog/how-to-use-cmake-without-the-agonizing-pain-part-2.html
- https://www.youtube.com/watch?v=KhuMRDY4BeU


### Создание библиотек
- [Packaging Python Projects](https://packaging.python.org/en/latest/tutorials/packaging-projects/)
- https://pytorch.org/tutorials/advanced/cpp_custom_ops.html#cpp-custom-ops-tutorial
- https://github.com/pybind/pybind11



### Pytorch Compile

- https://docs.google.com/document/d/1y5CRfMLdwEoF1nTk9q8qEu1mgMUuUtvhklPKJ2emLU8/edit
- https://www.youtube.com/live/rew5CSUaIXg?si=84kYR6vV8MqA4xCd
- [Identifying the cause of a graph break](https://docs.pytorch.org/docs/stable/torch.compiler_faq.html#why-am-i-not-seeing-speedups)
- https://ui.perfetto.dev/
- chrome://tracing/
- [Torch Compile and External Kernels](https://docs.nvidia.com/deeplearning/physicsnemo/physicsnemo-core/tutorials/performance_docs/torch_compile_support.html)
- https://github.com/MDK8888/GPTFast
- [gpt-fast](https://github.com/pytorch-labs/gpt-fast)
- [The dynamic shapes manual](https://docs.google.com/document/d/1GgvOe7C8_NVOMLOCwDaYV1mXXyHMXY7ExoewHqooxrs/edit?tab=t.0#heading=h.fh8zzonyw8ng)
- [torch.compile Troubleshooting](https://docs.pytorch.org/docs/stable/torch.compiler_troubleshooting.html#torch-compile-troubleshooting)
- https://blog.ezyang.com/2024/11/ways-to-use-torch-compile/
- [превращение програм от torch compile в бинарники без libtorch](https://github.com/lianakoleva/no-libtorch-compile)
- [torch.compile, the missing manual](https://docs.google.com/document/d/1y5CRfMLdwEoF1nTk9q8qEu1mgMUuUtvhklPKJ2emLU8/edit?tab=t.0#heading=h.ivdr7fmrbeab)
- https://docs.pytorch.org/docs/stable/notes/cuda.html#whole-network-capture
- [Benchmark Individual Triton Kernel](https://docs.pytorch.org/docs/stable/torch.compiler_inductor_profiling.html#benchmark-individual-triton-kernel)
- [(Beta) Implementing High-Performance Transformers with Scaled Dot Product Attention (SDPA)](https://docs.pytorch.org/tutorials/intermediate/scaled_dot_product_attention_tutorial.html)
- [Speeding Up Graph Learning Models with PyG and torch.compile](https://kumo.ai/research/speeding-up-graph-learning-models-with-pyg-and-torch-compile/)
- [tensorrt Torch Compile Advanced Usage](https://docs.pytorch.org/TensorRT/tutorials/_rendered_examples/dynamo/torch_compile_advanced_usage.html)
- [A Walk Through Example of torch.compile](https://depyf.readthedocs.io/en/latest/walk_through.html)
- [Introducing depyf: mastering torch.compile with ease](https://pytorch.org/blog/introducing-depyf/)
- [torch.compile Frequently Asked Questions](https://docs.pytorch.org/docs/stable/torch.compiler_faq.html)
- [PyTorch 2: Faster Machine Learning Through Dynamic Python Bytecode Transformation and Graph Compilation](https://assets-global.website-files.com/5eda9dd611c901bc56b01a91/65cbd869aff1b575ff2bc95e_pytorch_2.pdf)
- [Reducing torch.compile cold start compilation time with regional compilation](https://docs.pytorch.org/tutorials/recipes/regional_compilation.html)
- [py gnn compile examples](https://github.com/pyg-team/pytorch_geometric/tree/master/examples/compile)
- [workshop torch compile Using torch.compile()](https://shashankprasanna.com/workshops/a-tour-of-pytorch2/3_getting_started/)
- [Dynamo Deep-Dive](https://docs.pytorch.org/docs/stable/torch.compiler_dynamo_deepdive.html)


### Vision Models
- [⚡ Flash Diffusion ⚡: Accelerating Any Conditional Diffusion Model for Few Steps Image Generation (AAAI 2025 Oral)](https://github.com/gojasper/flash-diffusion)
- [LBM: Latent Bridge Matching for Fast Image-to-Image Translation ✨](https://github.com/gojasper/LBM)
- [Unifying Variational Autoencoder (VAE) implementations in Pytorch (NeurIPS 2022)](https://github.com/clementchadebec/benchmark_VAE)
- [MOBILEVIT: LIGHT-WEIGHT, GENERAL-PURPOSE, AND MOBILE-FRIENDLY VISION TRANSFORMER](https://arxiv.org/pdf/2110.02178.pdf)
- [EfficientViT: Multi-Scale Linear Attention for High-Resolution Dense Prediction](https://hanlab.mit.edu/projects/efficientvit)
- [Deep Compression Autoencoder for Efficient High-Resolution Diffusion Models](https://github.com/mit-han-lab/efficientvit/tree/master/applications/dc_ae)
- [SANA: Efficient High-Resolution Image Synthesis with Linear Diffusion Transformer](https://github.com/NVlabs/Sana)
- [🏃SANA-Sprint: One-Step Diffusion with Continuous-Time Consistency Distillation](https://github.com/NVlabs/Sana/blob/main/asset/docs/sana_sprint.md)
- [Tackling the Generative Learning Trilemma with Denoising Diffusion GANs](https://github.com/NVlabs/denoising-diffusion-gan)

### Torch compile benchmarks
- https://github.com/pytorch/torchtitan/blob/main/benchmarks/llama3_h100_202412_torchtitan.md#user-content-fn-2-cae6e35c41ee7ec0bdbbad5503a98dd1
- [Getting started with Pytorch 2.0 and Hugging Face Transformers](https://www.philschmid.de/getting-started-pytorch-2-0-transformers)- time is reduced by 52.5%
- https://sebastianraschka.com/blog/2023/pytorch-faster.html - наоборот увеличил 

### Audio models
- [SOTA Open Source TTS](https://github.com/fishaudio/fish-speech)
- [Moshi is a speech-text foundation model and full-duplex spoken dialogue framework. It uses Mimi, a state-of-the-art streaming neural audio codec.](https://github.com/kyutai-labs/moshi)
- [Voila is a new family of large voice-language foundation models aiming to lift human-AI interaction experiences to the next level.](https://github.com/maitrix-org/Voila?tab=readme-ov-file)
- [Text-to-Speech (TTS) Fine-tuning](https://docs.unsloth.ai/basics/text-to-speech-tts-fine-tuning)
